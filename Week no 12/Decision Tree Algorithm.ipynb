{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "743d6c88",
   "metadata": {},
   "source": [
    "# Decision Tree Algorithm\n",
    "\n",
    "### Learning Agenda of this Notebook\n",
    "\n",
    "\n",
    "- What are decision trees?\n",
    "- Basic concepts of decision trees, such as nodes, branches, root, leaves, etc.\n",
    "- Types of decision trees - classification and regression.\n",
    "- How decision trees work and their advantages and disadvantages.\n",
    "- How to build decision trees - algorithms and techniques.\n",
    "- Entropy and information gain.\n",
    "- Gini index and Gini impurity.\n",
    "- Pruning and overfitting.\n",
    "- Hyperparameters tuning and grid search.\n",
    "- Ensemble methods with decision trees - Random Forest and Gradient Boosting.\n",
    "- Decision tree libraries in Python - Scikit-learn and Pandas.\n",
    "- Decision tree visualization using Graphviz.\n",
    "- Real-world applications of decision trees.\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc6d21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f94866f",
   "metadata": {},
   "source": [
    "**Decision tree** is a type of supervised learning algorithm that is used for both classification and regression tasks. In this algorithm, a tree-like model of decisions is built by recursively splitting the data based on the values of features. The goal of the algorithm is to create a model that predicts the target variable by making a series of decisions based on the values of the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d1c58c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e4607c8",
   "metadata": {},
   "source": [
    "### Idea behind decision tree algorithm? \n",
    "\n",
    "The idea behind the decision tree algorithm is to create a model that predicts the value of a target variable by making a series of decisions based on the values of input features. The decision tree algorithm works by recursively partitioning the feature space into smaller regions, such that each region contains data points with similar values of the target variable.\n",
    "\n",
    "At each node of the tree, a decision is made based on the value of a single feature, and the data is split into two or more subsets based on the value of that feature. This process continues until the data in each leaf node is homogeneous with respect to the target variable, or until a stopping criterion is met (e.g., maximum depth of the tree, minimum number of samples required to split a node).\n",
    "\n",
    "The decision tree algorithm is useful for both classification and regression problems. In a classification problem, the goal is to predict the class label of a data point, while in a regression problem, the goal is to predict a continuous-valued target variable.\n",
    "\n",
    "The decision tree algorithm is intuitive, easy to interpret, and can handle both numerical and categorical data. However, it can be prone to overfitting if the tree is too deep or the data is noisy. To address these issues, techniques such as pruning and ensemble methods can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33167896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801f8b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4017946",
   "metadata": {},
   "source": [
    "### Step-by-Step workings of the decision tree algorithm\n",
    "Here are the step-by-step workings of the decision tree algorithm:\n",
    "\n",
    "**Splitting data:** First, the decision tree algorithm splits the dataset into two or more subsets based on the values of one of the input features.\n",
    "\n",
    "**Selecting the best feature:** The algorithm selects the feature that best splits the data, based on some criterion such as information gain or Gini impurity.\n",
    "\n",
    "**Recursive partitioning:** The process of splitting the data and selecting the best feature is repeated recursively for each subset until a stopping criterion is met, such as a maximum depth of the tree or a minimum number of samples required to split a node.\n",
    "\n",
    "**Building the tree:** The algorithm builds the tree by recursively splitting the data and selecting the best feature, until the stopping criterion is met.\n",
    "\n",
    "**Predicting the target variable:** Once the tree is built, it can be used to predict the value of the target variable for new data points. This is done by traversing the tree from the root node to a leaf node, based on the values of the input features for the new data point.\n",
    "\n",
    "**Handling missing values:** The decision tree algorithm can handle missing values by either imputing them or by ignoring them during the split.\n",
    "\n",
    "**Handling categorical data:** The decision tree algorithm can handle categorical data by either converting them to numerical data or by using specialized splitting criteria such as the chi-square test.\n",
    "\n",
    "Overall, the decision tree algorithm is a powerful and flexible machine learning algorithm that can handle a variety of data types and can be used for both regression and classification tasks. However, care must be taken to prevent overfitting, and the algorithm may require hyperparameter tuning to achieve optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af86e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977bde6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c057af05",
   "metadata": {},
   "source": [
    "### Terminologies used in Decision tree algorithm\n",
    "\n",
    "Here are some common terminologies used in the decision tree algorithm:\n",
    "\n",
    "- Root node: The topmost node of the decision tree, which represents the entire dataset.\n",
    "\n",
    "- Internal nodes: Nodes in the decision tree that represent a subset of the dataset and contain a split on one of the input features.\n",
    "\n",
    "- Leaf nodes: Nodes in the decision tree that represent the final outcome or decision of the model.\n",
    "\n",
    "- Splitting: The process of dividing the dataset into smaller subsets based on the value of a particular input feature.\n",
    "\n",
    "- Branches: The paths taken through the decision tree from the root node to a leaf node.\n",
    "\n",
    "- Entropy: A measure of the impurity or randomness in a set of data, used to determine the quality of a split in the decision tree.\n",
    "\n",
    "- Information gain: The reduction in entropy achieved by a particular split, used to determine the best feature to split on.\n",
    "\n",
    "- Gini impurity: A measure of the probability of misclassifying a random data point from a subset if it were randomly labeled according to the class distribution in the subset.\n",
    "\n",
    "- Pruning: The process of removing branches or nodes from the decision tree to prevent overfitting.\n",
    "\n",
    "- Hyperparameters: Parameters that control the behavior of the decision tree algorithm, such as the maximum depth of the tree or the minimum number of samples required to split a node. Hyperparameter tuning involves selecting the optimal values for these parameters to achieve the best performance of the model.\n",
    "\n",
    "- Decision Nodes: When a sub-node splits into further sub-nodes, it is called a decision node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459fca9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cfe52a4",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "\n",
    "Entropy is a measure of the impurity or randomness in a set of data. In the context of decision trees, entropy is used to determine the quality of a split based on the information gain achieved by that split.\n",
    "\n",
    "The formula for entropy is:\n",
    "\n",
    "$E = -\\sum_{i=1}^{n} p_i \\log_2(p_i)$\n",
    "\n",
    "where $n$ is the number of classes in the dataset, $p_i$ is the proportion of the data points that belong to class $i$, and $\\log_2$ is the base-2 logarithm. The entropy is zero when all the data points belong to the same class, and it is maximum when the data points are evenly distributed across all classes.\n",
    "\n",
    "In the context of decision trees, entropy is used to measure the impurity of a subset of data before and after a split. The **information gain** achieved by a split is defined as the difference between the entropy of the parent node and the weighted average of the entropies of the child nodes, where the weights are the proportions of the data points in each child node.\n",
    "\n",
    "The decision tree algorithm aims to maximize the information gain achieved by each split, in order to minimize the entropy of the resulting subsets and increase the homogeneity of the data within each subset.\n",
    "\n",
    "In practice, entropy is a widely used criterion for building decision trees, but other measures such as Gini impurity or misclassification error can also be used, depending on the specific problem and the nature of the data.\n",
    "\n",
    "<img src=\"images/Entropy.png\">\n",
    "<img src=\"images/Entropy1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb2af55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc235d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c7b3cec",
   "metadata": {},
   "source": [
    "### Information Gain\n",
    "\n",
    "Information gain is a measure of the reduction in entropy achieved by a particular split in a decision tree. It is used to determine the best feature to split on among all possible features.\n",
    "\n",
    "The formula for information gain is:\n",
    "\n",
    "$IG(D, F) = E(D) - \\sum_{f\\in F} \\frac{|D_f|}{|D|} E(D_f)$\n",
    "\n",
    "where $IG(D, F)$ is the information gain of the dataset $D$ with respect to the feature set $F$, $E(D)$ is the entropy of the entire dataset $D$, $D_f$ is the subset of data points in $D$ that have the value $f$ for feature $F$, and $E(D_f)$ is the entropy of the subset $D_f$. The information gain is high when the entropy of the subsets is low, meaning that the split is effective in separating the data into homogenous groups.\n",
    "\n",
    "In the context of decision trees, the information gain of each feature is calculated and compared, and the feature with the highest information gain is chosen as the splitting criterion. The algorithm recursively applies this process to create a tree until all the data points in each leaf node belong to the same class or a stopping criterion is met.\n",
    "\n",
    "In practice, information gain is a widely used criterion for building decision trees, but it has some limitations, such as a bias towards features with many possible values and the tendency to favor features with many binary splits. Alternative measures such as gain ratio and Gini index can also be used to overcome these limitations, depending on the specific problem and the nature of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30478515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f367e58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7ea490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14af13f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a decision tree classifier object\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "# Train the decision tree classifier on the training data\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict the class labels for the test data\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier using accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6732b09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
