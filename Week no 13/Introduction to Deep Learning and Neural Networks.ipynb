{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9591db7b",
   "metadata": {},
   "source": [
    "<img src=\"images/MLvsDL.jpeg\" height=400px width=400px>\n",
    "\n",
    "<h4 align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">Deep learning is a class of machine learning algorithms that uses multiple layers to progressively extract higher-level features from the raw input. (Wikipedia- Andrew Ng )</div></h4>\n",
    "\n",
    "<img src='images/combine_images (1).jpg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14217586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87356327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3271bee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfbc4f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcb7ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7645ea17",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction to Deep Learning and Neural Networks\n",
    "\n",
    "### Introduction\n",
    "- Definition of deep learning and neural networks\n",
    "- Applications of deep learning and neural networks\n",
    "- Deep learning frameworks\n",
    "\n",
    "### Building Blocks of a Neural Network\n",
    "- Neurons and their functionalities\n",
    "- Activation functions and their importance\n",
    "- Layers: input, hidden, and output\n",
    "- Forward propagation\n",
    "\n",
    "### Types of Neural Networks\n",
    "- Convolution   Neural Network\n",
    "- Recurrent Neural Network\n",
    "\n",
    "### Introduction to TensorFlow and Keras Libraries\n",
    "\n",
    "- Overview of TensorFlow and Keras libraries\n",
    "- Advantages and disadvantages of using these libraries\n",
    "\n",
    "\n",
    "### Building and Training a Simple Neural Network using Keras\n",
    "\n",
    "- Installing and importing the required libraries\n",
    "- Loading and preparing the data\n",
    "- Building the neural network model\n",
    "- Compiling the model with a loss function and optimizer\n",
    "- Training and evaluating the model\n",
    "\n",
    "### Practice Building a Neural Network for a Basic Classification Problem\n",
    "- Problem statement and dataset description\n",
    "- Loading and preparing the data\n",
    "- Building the neural network model\n",
    "- Compiling the model with a loss function and optimizer\n",
    "- Training and evaluating the model\n",
    "- Analyzing the model's performance\n",
    "\n",
    "### Conclusion\n",
    "- Summary of key concepts and techniques covered\n",
    "- Future directions and additional resources for deep learning and neural networks\n",
    "\n",
    "### Project:\n",
    "- Natural language processing using NNs: Build a model to perform natural language processing tasks such as machine translation or text classification.\n",
    "\n",
    "### Interview Questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7c21d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4517372b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22991b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb72d119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33422c93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc681ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc88b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39c7cc56",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36ea89e",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center\">a. What is Deep Learning? </h2>\n",
    "\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">Deep learning is a machine learning method that involves teaching artificial neural networks to learn from the input data by automatically discovering the relevant features required for solving complex problems. (Geoffrey Hinton – Professor at the University of Toronto)</div></hp>\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\"> Deep learning is a set of techniques that allow machines to learn by example, by constructing layered representations of data that enable learning and decision-making. (Yoshua Bengio- Professor at the University of Montreal) </div> </p>\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\"> Deep learning is a subfield of machine learning that is concerned with algorithms inspired by the structure and function of the brain, which is also known as artificial neural networks. (Yann LeCun - Director of AI Research at Facebook) </div></p>\n",
    "\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\"> Deep learning refers to artificial neural networks that are composed of many layers. (Ian Goodfellow - American computer scientist) </div></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b5c071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44763ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7479c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90d1b99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1537c0e5",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center\">b. Big Picture </h2>\n",
    "<img src=\"images/DS_MLRelationship.webp\" height=400px width=400px align=right>\n",
    "\n",
    "- **Artificial Intelligence (AI)**: Artificial Intelligence is the overarching field of study that focuses on developing intelligent systems that can perform tasks that usually require human-like intelligence.\n",
    "<br>\n",
    "- **Machine Learning (ML):** Machine Learning is a subset of AI that involves the use of statistical techniques and algorithms to enable machines to learn from data without being explicitly programmed.\n",
    "<br>\n",
    "\n",
    "- **Deep Learning (DL):** Deep Learning is a subset of machine learning that uses artificial neural networks with multiple layers to learn and extract features from complex data, such as images, video, and speech.\n",
    "<br>\n",
    "\n",
    "- **Data Science (DS):** Data Science is the field of study that involves the use of statistical and computational techniques to extract insights and knowledge from data.\n",
    "<br>\n",
    "\n",
    "- **Mathematics and Statistics:** Mathematics and Statistics are the foundational disciplines that provide the theoretical framework for developing and evaluating machine learning and deep learning algorithms.\n",
    "<br>\n",
    "\n",
    "- **Visualization:** Visualization is the process of representing data in a visual form to facilitate understanding and communication of insights and patterns in the data.\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afca69d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f08789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1e0ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d7ce06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707edbcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "578a24a7",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center\">c. Why DL is getting Famous now?</h2> \n",
    "<br>\n",
    "<img src=\"images/famous.jpeg\" height=500px width=500px align=right>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Availability of large amounts of data**\n",
    "\n",
    "\n",
    "- **Advancements in computing power**\n",
    "\n",
    "\n",
    "- **Improved algorithms and architectures**\n",
    "\n",
    "\n",
    "- **Success stories in various fields**\n",
    "\n",
    "\n",
    "- **Availability of open-source tools and libraries**\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "> <i>All the advancements, algorithms, architecture, success stories, tools and libraries, I will explain later in the notebook.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87296fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976ef036",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbaff9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98075985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a0fa60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56849540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6397f9c3",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center\">d. History of Deep Learning</h2> \n",
    "\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\"> \"AI began with an ancient wish to forge the gods.\" (Pamela McCorduck, Machines Who Think, 1979) </div></p>\n",
    "\n",
    "<img src=\"images/gub.png\" height=150px width=150px align=right>\n",
    "\n",
    "- The concept of **AI** dates back to the earliest human civilizations, where people had a desire to create entities that possessed god-like intelligence and abilities. \n",
    "\n",
    "- Humans have always been fascinated with the idea of creating something that can think, reason, and perform tasks like a human being or even better. \n",
    "\n",
    "- The pursuit of **AI** is not a new phenomenon, but rather one that has been ingrained in human consciousness for a very long time.\n",
    "\n",
    "> **Geb was the Egyptian god of the earth and a mythological member of the Ennead of Heliopolis. He could also be considered a father of snakes.**\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc6c702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70694b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1b2a78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c25f88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6edea5e8",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center\">e. Deep Learning Timeline</h2> \n",
    "\n",
    "<br>\n",
    "<img src=\"images/timeline.jpg\">\n",
    "<br>\n",
    "\n",
    "### **First Artifical Neuron :** \n",
    "- In 1943,  McCulloch and Pitts introduced the first artificial neuron, laying the foundation for neural networks.\n",
    "\n",
    "\n",
    "### **Perceptron :** \n",
    "- In 1958, the first perceptron  as a simple algorithm was introduced by Frank Rosenblatt, an American psychologist, for supervised learning of `binary classifiers`, inspired by the functioning of the human brain(neuron).\n",
    "\n",
    "\n",
    "### **Artificial Intelligence (AI):**\n",
    "- In 1956 , The term `Artificial Intelligence (AI)` was first coined by John McCarthy at the Dartmouth Conference.\n",
    "\n",
    "\n",
    "### **ADALINE:** \n",
    "- In 1960, ADALINE (Adaptive Linear Neuron) was introduced by Bernard Widrow and Ted Hoff at Stanford University as an improvement over the perceptron algorithm for pattern recognition tasks. \n",
    "- It could handle `continuous-valued` inputs and outputs, making it more versatile than perceptron. \n",
    "- ADALINE's primary application was in noise reduction in signals.\n",
    "\n",
    "\n",
    "### **XOR Problem:**  \n",
    "- In 1969, The XOR problem was introduced by MIT researchers Minsky & Seymour Papert. \n",
    "- The problem involved a neural network trying to learn the XOR function, which is not `linearly separable`. \n",
    "- This led to the discovery of the limitations of single-layer perceptrons and the need for more complex neural networks. \n",
    "- The solution to the XOR problem played a key role in the development of `multi-layer neural networks` and the resurgence of artificial neural networks in the 1980s.\n",
    "\n",
    "\n",
    "### **Backpropagation:**\n",
    "- In 1974, Backpropagation was first introduced by Paul Werbos in 1974. It was developed to overcome the limitations of single-layer neural networks in solving complex problems by enabling multi-layer networks to learn and adjust their weights through gradient descent. \n",
    "- Backpropagation allowed for the training of deep neural networks. It made it possible to solve complex problems such as image recognition, speech recognition, and natural language processing. \n",
    "- In 1986, the development of the backpropagation algorithm was re-introduced by Rumelhart, Hinton, and Williams, leading to a resurgence of interest in neural networks.\n",
    "\n",
    "\n",
    "### **Boltzmann Machine:** \n",
    "- In 1985, Boltzmann Machine was introducted by Geoffrey Hinton and Terry. It's a stochastic neural network that uses an energy-based model. \n",
    "- It allows for the learning and optimization of complex relationships between inputs and outputs in a distributed manner. \n",
    "- It was developed to address the limitations of traditional neural networks in capturing complex patterns and relationships in data. It has been used in various applications such as image recognition, speech recognition, and natural language processing.\n",
    "\n",
    "\n",
    "### **RNNs:** \n",
    "- In 1986, RNNs were developed to address the limitations of traditional feedforward neural networks in processing sequential data, such as speech and language, by allowing for the use of feedback loops and memory of previous inputs.\n",
    "\n",
    "\n",
    "### **CNNs:** \n",
    "- In the early 1990s, CNNs or Convolutional Neural Networks were first introduced by Yann LeCun, Yoshua Bengio, and Geoffrey Hinton . \n",
    "- The development of CNNs was motivated by the need for improved image recognition systems, as traditional neural networks were not well-suited for processing visual data. \n",
    "\n",
    "\n",
    "### **LeNet:** \n",
    "- In 1998, the `LeNet neural network` was introduced by Yann LeCun, Leon Bottou. The purpose of the network was to improve the accuracy of `handwritten digit recognition` in the field of computer vision. \n",
    "- It was specifically designed to handle **2D** inputs such as images and was one of the first successful implementations of CNNs. The LeNet formed the foundation of many modern CNN architectures used in computer vision tasks.\n",
    "\n",
    "\n",
    "### **Bidirectional Recurrent Neural Network (BRNN):** \n",
    "- In 1997, BRNN was first introduced by Mike and Kuldip K. Paliwal. The BRNN combines the capabilities of both forward and backward RNNs to improve the accuracy of sequence prediction tasks. \n",
    "- It was developed to overcome the limitations of traditional RNNs in handling long-term dependencies and to capture the contextual information in a sequence more effectively. \n",
    "- The BRNN has been used in various applications such as speech recognition, natural language processing, and image captioning.\n",
    "\n",
    "\n",
    "### **Long Short-Term Memory (LSTM):** \n",
    "- In 1997, LSTM was first introduced by Sepp Hochreiter. It was developed to overcome the `vanishing gradient` problem that occurs in traditional recurrent neural networks (RNNs) and to allow for the training of deep RNNs.\n",
    "- LSTM uses memory cells and gates to selectively remember or forget information, making it effective in modeling sequential data with long-term dependencies, such as natural language processing, speech recognition, and handwriting recognition.\n",
    "\n",
    "\n",
    "\n",
    "### **Deep Neural Network:** \n",
    "- In 2006, Geoff Hinton introduced the concept of deep learning and deep neural networks, the idea behind deep neural networks is to stack multiple layers of neurons to form a hierarchical representation of input data, allowing for the learning of increasingly complex and abstract features. \n",
    "- The main motivation behind deep neural networks is to improve the performance of machine learning models on complex tasks such as image recognition, speech recognition, and natural language processing.\n",
    "\n",
    "\n",
    "\n",
    "### **AlexNet:** \n",
    "- In 2012,  AlexNet, a deep convolutional neural network developed by Alex Krizhevsky, won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), demonstrating the power of deep learning for computer vision tasks.\n",
    "\n",
    "\n",
    "### **GoogleNet :** \n",
    "- In 2014,  Google developed a deep neural network, called a `GoogleNet`, also known as `Inception V1` that was able to recognize images of cats without being explicitly programmed to do so, further demonstrating the power of deep learning for unsupervised learning tasks.\n",
    "\n",
    "\n",
    "### **AlphaGo :** \n",
    "- In 2015,  AlphaGo is an AI program developed by DeepMind, a subsidiary of Google, that was designed to play the board game Go. \n",
    "- It became the first computer program to defeat a human professional Go player, `Lee Sedol`, in a five-game match. \n",
    "- The success of AlphaGo marked a significant milestone in the development of artificial intelligence and highlighted the potential for machine learning algorithms to tackle complex problems in new ways.\n",
    "\n",
    "\n",
    "### **Generative Adversarial Networks (GANs):** \n",
    "- In 2014, GANs were introduced by Ian Goodfellow and his colleagues. GANs were developed as a solution to the problem of unsupervised learning, where data is unlabelled and difficult to use for training machine learning models. \n",
    "- GANs use a generative model and a discriminative model in a game-like setting to learn the underlying distribution of data and generate new data samples that are similar to the original data. \n",
    "- The primary goal of GANs is to create more realistic synthetic data that can be used for various applications, such as image generation and data augmentation.\n",
    "\n",
    "\n",
    "### **GPT (Generative Pre-trained Transformer):** \n",
    "- GPT was introduced by OpenAI in June 2018. The goal was to develop a language model that could perform a wide range of natural language processing tasks with state-of-the-art performance. \n",
    "- The GPT architecture is based on the transformer model and is pre-trained on large amounts of text data to learn the patterns and relationships between words and phrases in the language. \n",
    "- The introduction of GPT has had significant implications for various applications, including chatbots, language translation, and text generation.\n",
    "\n",
    "\n",
    "### **GPT-3 (Generative Pre-trained Transformer 3):** \n",
    "- GPT-3 (Generative Pre-trained Transformer 3) was introduced by OpenAI in 2020. \n",
    "- It was designed as a state-of-the-art language processing AI model with an unprecedented number of parameters (175 billion), allowing it to generate human-like text and perform a wide range of natural language processing tasks with high accuracy. \n",
    "- The main goal of GPT-3 was to push the limits of what AI models can achieve in the field of language processing and to explore potential applications in various industries such as education, healthcare, and customer service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6db4671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a71843f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61610fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d914db29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a702309a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d88ddf2",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center\">f. Applications of Deep Learning</h2> \n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='images/object1.gif' align=right height=200px width=200px>\n",
    "<img src='images/object2.gif' align=right height=200px width=200px>\n",
    "\n",
    "### Image and object recognition: \n",
    "- Deep learning is widely used for image and object recognition in various applications such as security, healthcare, autonomous vehicles, and more.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src='images/object3.gif' align=right height=200px width=200px>\n",
    "<img src='images/object4.gif' align=right height=200px width=200px>\n",
    "\n",
    "### Natural language processing (NLP)\n",
    "\n",
    "- Deep learning is used in natural language processing (NLP) applications such as machine translation, sentiment analysis, chatbots, and more.\n",
    "\n",
    "    \n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "    \n",
    "<img src='images/object5.gif' align=right height=200px width=200px>\n",
    "<img src='images/object6.gif' align=right height=200px width=200px>\n",
    "    \n",
    "### Speech Recognition Systems\n",
    "- Deep learning is used in speech recognition systems to accurately transcribe spoken words into text.\n",
    "\n",
    "    \n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src='images/object7.gif' align=right height=200px width=200px>\n",
    "<img src='images/object8.gif' align=right height=200px width=200px>\n",
    "\n",
    "### Recommendation Systems\n",
    "- Deep learning algorithms are used in recommendation systems, such as those used by Amazon and Netflix, to suggest products or movies to customers.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src='images/object9.gif' align=right height=200px width=200px>\n",
    "<img src='images/object10.gif' align=right height=200px width=200px>\n",
    "\n",
    "### Fraud Detection Systems\n",
    "- Deep learning algorithms are used in fraud detection systems to identify fraudulent activities in banking and financial transactions.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src='images/object11.jpg' align=right height=200px width=200px>\n",
    "<img src='images/object12.gif' align=right height=200px width=200px>\n",
    "\n",
    "### HealthCare\n",
    "- Deep learning is used in medical applications such as disease diagnosis, drug discovery, and medical image analysis.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src='images/self.jpg' align=right height=200px width=200px>\n",
    "\n",
    "### Autonomous vehicles\n",
    "- Deep learning is used in autonomous vehicles for object detection, pedestrian detection, and scene recognition.\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src='images/robotics.jpg' align=right height=200px width=200px>\n",
    "\n",
    "### Robotics\n",
    "- Deep learning algorithms are used in robotics for object recognition, navigation, and control.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src='images/AI.gif' align=right height=200px width=200px>\n",
    "\n",
    "### Gaming\n",
    "- Deep learning is used in gaming applications for non-player character (NPC) behavior and strategy.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src='images/music.jpg' align=right height=200px width=200px>\n",
    "\n",
    "### Music generation\n",
    "- Deep learning is used in music generation to create new and original music.\n",
    "\n",
    "\n",
    "\n",
    "### Cybersecurity\n",
    "- Deep learning algorithms are used in cybersecurity for intrusion detection and network security.\n",
    "\n",
    "### Advertising\n",
    "- Deep learning is used in advertising applications for personalized targeting and ad placement.\n",
    "\n",
    "### Agriculture\n",
    "- Deep learning is used in agriculture for crop management and yield prediction.\n",
    "\n",
    "### Energy management\n",
    "- Deep learning algorithms are used in energy management for predicting energy consumption and optimizing energy usage.\n",
    "\n",
    "### Sports\n",
    "- Deep learning is used in sports for player tracking, analysis, and prediction of game outcomes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acdc71e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b78a25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d160949a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02c2e35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430b8074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b34e659",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center\">h. Challenges in Deep Learning</h2> \n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\"><strong> Deep learning is not just a tool, it's a mindset. It requires a deep understanding of mathematics, statistics, and computer science, as well as a willingness to constantly learn and adapt. (Yann LeCun, AI researcher and professor.)</strong></div></p>\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "<img src=\"images/meme.jpeg\" height=300px width=400px align=right>\n",
    "\n",
    "- **Data Availability and Quality:** Deep learning needs a lot of good quality data to work properly. Getting enough data can be expensive and time-consuming.\n",
    "\n",
    "\n",
    "\n",
    "- **Computational Resources:** Deep learning is very computationally intensive and needs powerful computers to run well. These computers can be expensive and hard to find.\n",
    "\n",
    "\n",
    "- **Interpretability:** It's hard to understand how deep learning models make their predictions, making it difficult to trust and use them in certain fields.\n",
    "\n",
    "\n",
    "- **Overfitting and Generalization:** Deep learning models can be too specialized to the training data they were trained on, and can have trouble generalizing to new data.\n",
    "\n",
    "\n",
    "- **Algorithmic Complexity:** Deep learning models can be very complicated and hard to design and optimize.\n",
    "\n",
    "\n",
    "- **Hardware Limitations:** Access to powerful hardware like GPUs can be limited, which can slow down the development of new models.\n",
    "\n",
    "\n",
    "- **Security and Privacy:** Deep  learning models can be vulnerable to attacks that can trick them into making mistakes, and using sensitive data in deep learning applications can raise privacy concerns.\n",
    "\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48411108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3815d4d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c5b88b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d7a928",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cd3ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0d7d28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2dc2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a53b878",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center\">i. Deep Learning FrameWorks</h2> \n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\"><strong>Deep learning frameworks are essential tools for building, training, and deploying deep learning models at scale. They provide a high-level API that abstracts away the low-level details, allowing developers to focus on the task at hand. (Yann LeCun, AI researcher and professor)</strong></div></p>\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"images/framework.png\" height=500px width=500px align=right>\n",
    "\n",
    "\n",
    "- **TensorFlow:** Developed by Google, TensorFlow is one of the most popular deep learning frameworks. It supports both high-level APIs for building models quickly and low-level APIs for maximum flexibility.\n",
    "\n",
    "\n",
    "- **PyTorch:** Developed by Facebook, PyTorch is known for its easy-to-use API and dynamic computational graph, which allows for efficient experimentation and debugging.\n",
    "\n",
    "\n",
    "- **Keras:** Keras is a high-level API for building deep learning models that can run on top of TensorFlow, Theano, or CNTK. It is designed to be user-friendly and easy to learn.\n",
    "\n",
    "\n",
    "- **Caffe:** Caffe (Convolutional Architecture for Fast Feature Embedding) is a deep learning framework designed for efficient processing of image and video data. It is known for its speed and scalability.\n",
    "\n",
    "\n",
    "- **MXNet:** Developed by Amazon, MXNet is a flexible and efficient deep learning framework that supports both imperative and symbolic programming models.\n",
    "\n",
    "\n",
    "- **Microsoft Cognitive Toolkit (CNTK):** CNTK is a deep learning framework developed by Microsoft that supports both Windows and Linux. It is designed for scalable distributed training and efficient inference.\n",
    "\n",
    "\n",
    "- **Theano:** Theano is a deep learning framework that allows for efficient computation of mathematical expressions, especially in matrix algebra. It is known for its fast computation and flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f3698b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b6c0ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ecaf08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e31e9d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cc8ab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21fa37d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8375493",
   "metadata": {},
   "source": [
    "# 2. Building Blocks of a Neural Network\n",
    "\n",
    "\n",
    "\n",
    "<h2 style=\"text-align:center\">a. A Biological Neuron</h2> \n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\"><strong>A biological neuron, also known as a nerve cell, is the fundamental unit of the nervous system and plays a critical role in transmitting and processing information in the body.</strong></div></p>\n",
    "\n",
    "\n",
    "<img src=\"images/bio.png\" height=600px width=600px>\n",
    "<br>\n",
    "\n",
    "It consists of four main parts:\n",
    "\n",
    "- **Cell Body:** The central part of the neuron that contains the genetic information and metabolic machinery required to maintain the cell. It is neuron's processing unit that performs mathematical operations on the input signals.\n",
    "\n",
    "\n",
    "- **Dendrites:**  Branch-like structures that receive electrical signals from other neurons and transmit them towards the cell body. It is  neuron's input connections that receive signals from other neurons or the external environment.\n",
    "\n",
    "\n",
    "- **Axon:** A long, cable-like structure that conducts electrical impulses away from the cell body towards other neurons or muscles. It is  neuron's output connection that transmits signals to other neurons or the final output layer.\n",
    "\n",
    "\n",
    "- **Synapses:** Small gaps between neurons where neurotransmitters are released to transmit signals, allowing for communication between neurons. Synapses: The connections between neurons where the strength of the signal is adjusted during training to improve the accuracy of the neural network.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<h2 style=\"text-align:center\">Thalamocortical system visualization via DigiCortex Engine</h2> \n",
    "\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\"><strong>The thalamocortical system is a complex network of connections between the thalamus (a structure in the brain that relays sensory information) and the cortex (the outer layer of the brain responsible for processing sensory and cognitive information).</strong></div></p>\n",
    "<br>\n",
    "\n",
    "<img src=\"images/one.gif\" align=right height=500px width=500px>\n",
    "<br>\n",
    "\n",
    "- The **DigiCortex Engine** is a software tool that can be used to create visualizations of the thalamocortical system, allowing researchers to better understand the complex interactions between different brain regions. The engine uses advanced computer graphics techniques to create realistic 3D models of the brain, and can simulate the behavior of neurons and synapses in real time.\n",
    "\n",
    "\n",
    "- By visualizing the thalamocortical system using the DigiCortex Engine, researchers can gain insights into how the brain processes information, how different brain regions interact with each other, and how neurological disorders may affect these interactions. This information can be used to develop new therapies and treatments for a variety of neurological and psychiatric conditions. Visualized here are `3%` of the neurons and  `0.0001%` of the synapses in the brain(16.7 Million Neurons - 2.1 Billion Synapses).\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Link: [Thalamocortical system visualization via DigiCortex Engine](https://twitter.com/lexfridman/status/1081260770464280576)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc69d28e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283b0296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08926ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9b6f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2858239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4351f28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22ad16ff",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center\">b. Artifical Neuron</h2> \n",
    "\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\"><strong> A Artificial Neuron(AN) in deep learning is a mathematical function that receives one or more inputs and produces an output. It is inspired by the biological neuron in the brain, but it is a simplified mathematical model used in artificial neural networks. It is also called Perceptron.</strong></div></p>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<img src=\"images/AN.png\" align=right width=500px height=500px>\n",
    "\n",
    "The parts of an AN and their functions are:\n",
    "\n",
    "- **Input:** The input is the information or data that is fed into the neuron. It can be one or more numerical values that represent features of the input data.\n",
    "\n",
    "\n",
    "- **Weights:** Weights are numerical values that are assigned to the inputs. They represent the strength of the connection between the input and the neuron. The weights are adjusted during the training process to improve the accuracy of the model.\n",
    "\n",
    "\n",
    "- **Bias:** The bias is a constant value added to the weighted sum of the inputs. It represents the neuron's threshold for activation.\n",
    "\n",
    "\n",
    "- **Activation function:** The activation function determines whether the neuron should fire or not based on the input and the weights. It maps the weighted sum of the inputs and the bias to an output value. Common activation functions include sigmoid, ReLU, and softmax.\n",
    "\n",
    "\n",
    "- **Output:** The output is the result of the activation function. It represents the neuron's prediction or decision based on the input data.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\"><strong>An artificial neuron takes in numerical inputs, multiplies them by learned weights, adds a bias, applies an activation function, and produces an output. The output can then be fed into other neurons to form a deep learning model.</strong></div></p>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"images/neuron1.webp\" height=600px width=600px>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### Mathematical Implementation\n",
    "\n",
    "Suppose we are building a model to predict whether a customer will churn (cancel their subscription) based on their purchase history. We have an artificial neuron with three inputs: \n",
    "\n",
    "- The number of purchases in the last 30 days\n",
    "- The average purchase amount\n",
    "- The customer's satisfaction score (a number between 1 and 10).\n",
    "\n",
    "Let's assume that the weights and bias for this neuron are:\n",
    "\n",
    "- **Weight1 (w1):** 0.3\n",
    "\n",
    "\n",
    "- **Weight2 (w2):** 0.5\n",
    "\n",
    "\n",
    "- **Weight3 (w3):** -0.1\n",
    "\n",
    "\n",
    "- **Bias (b):** 0.2\n",
    "\n",
    "\n",
    "The activation function is a sigmoid function:\n",
    "\n",
    "$$ \\mathbf{sigmoid(z) = \\frac{1}{ (1 + e^{(-z)})}}$$\n",
    "\n",
    "\n",
    "\n",
    "Now, suppose we have a customer with the following purchase history:\n",
    "\n",
    "- **Purchases in the last 30 days:** 6\n",
    "- **Average purchase amount:** $\\$20$\n",
    "- **Satisfaction score:** 7\n",
    "\n",
    "\n",
    "\n",
    "We can input these values into the artificial neuron, its output is:\n",
    "\n",
    "- `Weighted sum:` w1 * 6 + w2 * 20 + w3 * 7 + b = 0.3 * 6 + 0.5 * 20 - 0.1 * 7 + 0.2 = 5.5\n",
    "\n",
    "- `Output:` sigmoid(5.5) = 0.99592986228\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Result:** The output of the AN for this customer is approximately **1**, indicating a very high probability that they will not churn. This output can be used as input to other neurons in a deep neural network or as a prediction for this customer's churn probability.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66931d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad385f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9df2494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bd12fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb85c9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83320583",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center\">c. A Perceptron (Single Unit Perceptron)</h2> \n",
    "\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\"><strong> A perceptron is a basic computational unit of a neural network that is designed to perform binary classification of input vectors. It consists of a set of input weights, an activation function, and a threshold. The perceptron takes in input signals, applies the weights to them, sums up the weighted inputs, and then passes the result through the activation function. If the result exceeds the threshold, the perceptron outputs 1, otherwise it outputs 0.</strong></div></p>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<img src=\"images/Perceptron_1.png\" align=right width=500px height=500px>\n",
    "\n",
    "The parts of a Perceptron and their functions are:\n",
    "\n",
    "- **Input values:** These are the values that are fed into the perceptron as input.\n",
    "\n",
    "\n",
    "- **Weights:** Each input value is associated with a weight, which determines the significance of that input value in the final output.\n",
    "\n",
    "\n",
    "- **Bias:** A bias is a constant term that is added to the sum of the weighted inputs. It allows the perceptron to adjust the decision boundary.\n",
    "\n",
    "\n",
    "- **Activation function:** Usually, we use `step function` as its activation function.\n",
    "\n",
    "\n",
    "- **Threshold:** The threshold is a predefined value that is compared to the output of the activation function. If the output is greater than the threshold, the perceptron produces a positive output, otherwise it produces a negative output.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "> **Note:** It is trained using a supervised learning algorithm called the perceptron learning rule, which updates the weights based on the errors between the actual output and the desired output.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Dis-Advantages of Perceptron:\n",
    "<br>\n",
    "<img src=\"images/image.png\" align=right width=450px height=450px>\n",
    "\n",
    "- **Limited to linearly separable problems:** Perceptron can only be used for classification problems that are linearly separable, meaning that a straight line can separate the data points into different classes.Perceptrons can only learn linearly separable problems such as boolean AND problem. For non-linear problems such as the boolean XOR problem, it does not work.\n",
    "\n",
    "\n",
    "- **Binary classification only:** Perceptron is only capable of binary classification, meaning that it can only classify data into two classes.\n",
    "\n",
    "\n",
    "- **Lack of hidden layers:**  Perceptron only has one layer, so it cannot capture complex patterns in the data that require multiple layers of neurons\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "### Mathematical Implementation\n",
    "\n",
    "Let's say we have the following inputs and weights:\n",
    "\n",
    "- `Input 1:` 2\n",
    "- `Input 2:` -3\n",
    "- `Weight 1:` 1.5\n",
    "- `Weight 2:` -2.5\n",
    "- `Bias:` 1\n",
    "\n",
    "\n",
    "We can represent this as a formula:\n",
    "\n",
    "$$\\mathbf{\\text{output = activation_function((input_1 * weight_1) + (input_2 * weight_2) + bias)}}$$\n",
    "\n",
    "\n",
    "Let's use the step function as our activation function, which returns 1 if the input is greater than or equal to 0, and 0 otherwise.\n",
    "\n",
    "Using the formula, we get:\n",
    "\n",
    "$$\\mathbf{\\text{output = step_function((2 * 1.5) + (-3 * 2.5) + 1)}}$$\n",
    "\n",
    "Simplifying, we get:\n",
    "\n",
    "$$\\mathbf{\\text{output = step_function(-4.25)}}$$\n",
    "\n",
    "> **Result:** As, -4.25 is less than 0, the step function returns 0, so the output of the perceptron in this case is 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563a6e84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45fcd9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3371a6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7dafeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6c0e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae904a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e333a75",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center\">d. Concept of Activation Functions</h2> \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">\n",
    " <h3><strong>Why do Neural Networks Need an Activation Function? </strong></h3>\n",
    "<br>    \n",
    "<br>    \n",
    "<strong>Introducing non-linearity: </strong> Activation functions introduce non-linearity into the output of a neuron, which allows neural networks to model more complex relationships between inputs and outputs.</div></p>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<img src=\"images/act1.jpg\" height=600px width=600px>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## 3 Types of Neural Networks Activation Functions\n",
    "\n",
    "### a. Binary Step Function \n",
    "\n",
    "<img src=\"images/act2.jpg\" height=400px width=400px align=right>\n",
    "\n",
    "- Binary step function depends on a threshold value that decides whether a neuron should be activated or not. \n",
    "\n",
    "\n",
    "- The input fed to the activation function is compared to a certain threshold; if the input is greater than it, then the neuron is activated, else it is deactivated, meaning that its output is not passed on to the next hidden layer.\n",
    "\n",
    "- Mathematically it is represented as:\n",
    "\n",
    "$$ \\mathbf{f(x) = \\begin{cases} \n",
    "        0, & x < 0 \\\\\n",
    "        1, & x \\geq 0 \n",
    "       \\end{cases}}\n",
    "$$\n",
    "\n",
    "- **Limitation**\n",
    "\t- It cannot provide multi-value outputs—for example, it cannot be used for multi-class classification problems. \n",
    "    \n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "    \n",
    "### b. Linear Activation Function\n",
    "\n",
    "<img src=\"images/act3.jpg\" height=400px width=400px align=right>\n",
    "\n",
    "- The linear activation function, also known as `no activation` or `identity function` (multiplied x1.0), is where the activation is proportional to the input.\n",
    "\n",
    "\n",
    "- The function doesn't do anything to the weighted sum of the input, it simply spits out the value it was given.\n",
    "\n",
    "- Mathematically it is represented as:\n",
    "\n",
    "$$ \\mathbf{f(x) = x}$$\n",
    "\n",
    "- **Limitations**\n",
    "    - It’s not possible to use backpropagation as the derivative of the function is a constant and has no relation to the input x. \n",
    "    \n",
    "    - All layers of the neural network will collapse into one if a linear activation function is used. No matter the number of layers in the neural network, the last layer will still be a linear function of the first layer. So, essentially, a linear activation function turns the neural network into just one layer.\n",
    "    \n",
    "    \n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bb7856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7593bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0694a44",
   "metadata": {},
   "source": [
    "\n",
    "### c. Non-Linear Activation Functions\n",
    "\n",
    "\n",
    "Non-linear activation functions solve the following limitations of linear activation functions:\n",
    "\n",
    "- They allow backpropagation because now the derivative function would be related to the input, and it’s possible to go back and understand which weights in the input neurons can provide a better prediction.\n",
    "\n",
    "\n",
    "- They allow the stacking of multiple layers of neurons as the output would now be a non-linear combination of input passed through multiple layers. Any output can be represented as a functional computation in a neural network.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "<h3 style='text-align:center'>1. Sigmoid / Logistic Activation Function  </h3>\n",
    "\n",
    "<img src=\"images/act4.jpg\" height=400px width=400px align=right>\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\"><strong> This function takes any real value as input and outputs values in the range of 0 to 1. The larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to 0.0</strong>   \n",
    "</div></p>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Mathematically it is represented as:\n",
    "\n",
    "$$ \\mathbf{sigmoid(z) = \\frac{1}{ (1 + e^{(-z)})}}$$\n",
    "\n",
    "\n",
    "- **Advantages**\n",
    "\n",
    "\t- It is commonly used for models where we have to predict the probability as an output. Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice because of its range.\n",
    "\t-  Mean around 0.5\n",
    "    \n",
    "\t- The function is differentiable and provides a smooth gradient, i.e., preventing jumps in output values. This is represented by an S-shape of the sigmoid activation function. \n",
    "    \n",
    "<br>\n",
    "<img src=\"images/act5.jpg\" height=400px width=400px align=right>\n",
    "    \n",
    "<br>\n",
    "\n",
    "### Derivative of Sigmoid Function\n",
    "\n",
    "- Let's denote the sigmoid function as\n",
    "\n",
    "$$\\sigma(x) = \\dfrac{1}{1 + e^{-x}}$$\n",
    "\n",
    "- The derivative of sigmoid function is \n",
    "\n",
    "$$\\dfrac{d}{dx}\\sigma(x) = \\sigma(x)(1 - \\sigma(x))$$\n",
    "\n",
    "\n",
    "- Here's detail derivation\n",
    "\n",
    "$$\\begin{align}\n",
    "\\dfrac{d}{dx} \\sigma(x) &= \\dfrac{d}{dx} \\left[ \\dfrac{1}{1 + e^{-x}} \\right] \\\\\n",
    "&= \\dfrac{d}{dx} \\left( 1 + \\mathrm{e}^{-x} \\right)^{-1} \\\\\n",
    "&= -(1 + e^{-x})^{-2}(-e^{-x}) \\\\\n",
    "&= \\dfrac{e^{-x}}{\\left(1 + e^{-x}\\right)^2} \\\\\n",
    "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{e^{-x}}{1 + e^{-x}}  \\\\\n",
    "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{(1 + e^{-x}) - 1}{1 + e^{-x}}  \\\\\n",
    "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( \\dfrac{1 + e^{-x}}{1 + e^{-x}} - \\dfrac{1}{1 + e^{-x}} \\right) \\\\\n",
    "&= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( 1 - \\dfrac{1}{1 + e^{-x}} \\right) \\\\\n",
    "&= \\sigma(x) \\cdot (1 - \\sigma(x))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "- **Dis-Advantages**\n",
    "\t- The gradient values are only significant for range -3 to 3, and the graph gets much flatter in other regions.  It implies that for values greater than 3 or less than -3, the function will have very small gradients. As the gradient value approaches zero, the network ceases to learn and suffers from the Vanishing gradient problem. (Gradient $\\rightarrow$ 0 for higher values of |z|).\n",
    "\n",
    "\t- The output of the logistic function is not symmetric around zero. So the output of all the neurons will be of the same sign. This makes the training of the neural network more difficult and unstable.\n",
    "    \n",
    "    \n",
    "    \n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce31d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementation\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "sigmoid(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4fa9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(Gradient  →0 for higher values of |z|)\n",
    "\n",
    "sigmoid(10)*(1- sigmoid(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e992dc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(2)*(1- sigmoid(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12564f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74147da2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55fc1c8f",
   "metadata": {},
   "source": [
    "<h3 style='text-align:center'>2. Tanh Function (Hyperbolic Tangent) </h3>\n",
    "\n",
    "<img src=\"images/act6.jpg\" height=400px width=400px align=right>\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\"><strong> Tanh function is very similar to the sigmoid/logistic activation function, and even has the same S-shape with the difference in output range of -1 to 1. In Tanh, the larger the input (more positive), the closer the output value will be to 1.0, whereas the smaller the input (more negative), the closer the output will be to -1.0.</strong>   \n",
    "</div></p>\n",
    "\n",
    "\n",
    "\n",
    "- Mathematically it is represented as:\n",
    "\n",
    "$$\\mathbf{f(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}}$$\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Advantages**\n",
    "\t- The output of the tanh activation function is Zero centered; hence we can easily map the output values as strongly negative, neutral, or strongly positive.\n",
    "\n",
    "\t- Usually used in hidden layers of a neural network as its values lie between -1 to; therefore, the mean for the hidden layer comes out to be **0** or very close to it. It helps in **centering** the data and makes learning for the next layer much easier.\n",
    "    \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- **Dis-Advantages**\n",
    "\t- It also faces the problem of vanishing gradients similar to the sigmoid activation function. Plus the gradient of the tanh function is much steeper as compared to the sigmoid function.\n",
    "\n",
    "<img src=\"images/act7.jpg\" height=300px width=300px align=right>\n",
    "\n",
    "\n",
    "> **Note:**  Although both sigmoid and tanh face vanishing gradient issue, tanh is zero centered, and the gradients are not restricted to move in a certain direction. Therefore, in practice, tanh nonlinearity is always preferred to sigmoid nonlinearity.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Derivative of Tanh Activation Function**\n",
    "\n",
    "The derivative of the tanh activation function is:\n",
    "\n",
    "$$ \\mathbf{ \\frac{d}{dx} tanh(x) = 1 - tanh^2(x) }$$\n",
    "\n",
    "\n",
    "This can be also written as :\n",
    "\n",
    "$$ \\mathbf{ \\frac{d}{dx} tanh(x) = sech^2(x) }$$\n",
    "\n",
    "\n",
    "where $\\operatorname{sech}(x)$ is the hyperbolic secant function, defined as:\n",
    "\n",
    "$$ \\mathbf{ sech(x) = \\frac{1}{cosh(x)} = \\frac{2}{e^x + e^{-x}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d23f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation\n",
    "\n",
    "def tanh(x):\n",
    "    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "\n",
    "tanh(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaa358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of derivative of tanh function\n",
    "import math\n",
    "def tanh_derivative(x):\n",
    "    return 1 - math.tanh(x)**2\n",
    "\n",
    "\n",
    "# or \n",
    "def tanh_derivative(x):\n",
    "    sech = 1 / math.cosh(x)\n",
    "    return sech**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c8a720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bccad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh_derivative(10), tanh_derivative(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38544e1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b7541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec5aeb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a57d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bef6995b",
   "metadata": {},
   "source": [
    "<h3 style='text-align:center'>3. ReLU Function </h3>\n",
    "\n",
    "<img src=\"images/act8.jpg\" height=300px width=300px align=right>\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\"><strong> ReLU stands for Rectified Linear Unit. Although it gives an impression of a linear function, ReLU has a derivative function and allows for backpropagation while simultaneously making it computationally efficient. The main catch here is that the ReLU function does not activate all the neurons at the same time. The neurons will only be deactivated if the output of the linear transformation is less than 0.</strong>   \n",
    "</div></p>\n",
    "\n",
    "\n",
    "\n",
    "- Mathematically it is represented as:\n",
    "\n",
    "$$\\mathbf{f(x) = max(0,x)}$$\n",
    "\n",
    "- **Advantages**\n",
    "    - Since only   certain number of neurons are activated, the ReLU function is far more computationally efficient when compared to the sigmoid and tanh functions.\n",
    "    - ReLU accelerates the convergence of gradient descent towards the global minimum of the loss function due to its linear, non-saturating property.\n",
    "    \n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"images/act9.jpg\" height=300px width=300px align=right>\n",
    "\n",
    "- **Dis-Advantages**\n",
    "\n",
    "\t- The `Dying Relu Problem:` The negative side of the graph makes the gradient value zero. Due to this reason, during the backpropagation process, the weights and biases for some neurons are not updated. This can create dead neurons which never get activated. \n",
    "\n",
    "\t- All the negative input values become zero immediately, which decreases the model’s ability to fit or train from the data properly. \n",
    "    \n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "**Derivative of the ReLU Activation Function**\n",
    "\n",
    "The derivative of the ReLU activation function can be defined as follows:\n",
    "\n",
    "- If `x` is greater than or equal to 0, the derivative is 1.\n",
    "- If `x` is less than 0, the derivative is 0.\n",
    "\n",
    "In mathematical notation, we can write the derivative of Leaky ReLU as:\n",
    "\n",
    "$$ \\mathbf{f'(x) = 1, \\; if\\;  x >= 0}$$\n",
    "$$\\mathbf{f'(x) = 0, \\; if \\; x < 0}$$\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfd19c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation\n",
    "\n",
    "def relu(num):\n",
    "    return max(0, num)\n",
    "inputs = np.linspace(-10,10, 1000)\n",
    "outputs = []\n",
    "for i in inputs:\n",
    "    outputs.append(relu(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e91929",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(inputs, outputs);\n",
    "plt.title(\"ReLu\")\n",
    "plt.xlabel(\"Inputs\")\n",
    "plt.ylabel(\"Outputs\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e304463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c749991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5a122b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbbf9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdb7dd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6c1246c",
   "metadata": {},
   "source": [
    "<h3 style='text-align:center'>4. Leaky ReLU Function </h3>\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\"><strong>  Leaky ReLU is an improved version of ReLU function to solve the Dying ReLU problem as it has a small positive slope in the negative area. </strong>   \n",
    "</div></p>\n",
    "\n",
    "\n",
    "<img src=\"images/act10.jpg\" height=300px width=300px align=right>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Mathematically it is represented as :\n",
    "\n",
    "$$\\mathbf{f(x) = max(0.1x, x)}$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- **Advantages**\n",
    "\n",
    "- The advantages of Leaky ReLU are same as that of ReLU, in addition to the fact that it does enable backpropagation, even for negative input values. \n",
    "\n",
    "- By making this minor modification for negative input values, the gradient of the left side of the graph comes out to be a non-zero value. Therefore, we would no longer encounter dead neurons in that region. \n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- **Dis-Advantages**\n",
    "\t- The predictions may not be consistent for negative input values. \n",
    "    - The gradient for negative values is a small value that makes the learning of model parameters time-consuming.\n",
    "    \n",
    "<img src=\"images/act11.jpg\" height=300px width=300px align=right>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "**Derivative of the Leaky ReLU Activation Function**\n",
    "\n",
    "The derivative of the Leaky ReLU activation function can be defined as follows:\n",
    "\n",
    "- If `x` is greater than or equal to 0, the derivative is 1.\n",
    "- If `x` is less than 0, the derivative is a small constant alpha.\n",
    "\n",
    "In mathematical notation, we can write the derivative of Leaky ReLU as:\n",
    "\n",
    "$$ \\mathbf{f'(x) = 1, \\; if\\;  x >= 0}$$\n",
    "$$\\mathbf{f'(x) = \\alpha, \\; if \\; x < 0}$$\n",
    "\n",
    "where alpha is a small constant, typically set to `0.01.`\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ae0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "def leaky_relu(num):\n",
    "    return max(0.1*num, num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5c8da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.linspace(-10,10, 1000)\n",
    "outputs = []\n",
    "for i in inputs:\n",
    "    outputs.append(leaky_relu(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f9388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(inputs, outputs);\n",
    "plt.title(\"Leaky ReLu\")\n",
    "plt.xlabel(\"Inputs\")\n",
    "plt.ylabel(\"Outputs\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66484373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu_derivative(x, alpha=0.01):\n",
    "    if x >= 0:\n",
    "        return 1\n",
    "    else: return alpha\n",
    "leaky_relu_derivative(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700dbc6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d8a655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed256911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbdd26e9",
   "metadata": {},
   "source": [
    "### Test Yourself\n",
    "\n",
    "**Question 01:** Consider the following perceptron with two `inputs` (x1 and x2), `weights` (w1 and w2), and bias (b).\n",
    "\n",
    "<img src=\"images/homework.jpg\" height=300px width=300px>\n",
    "\n",
    "- a. Calculate the weighted sum: $z = (w1 * x1) + (w2 * x2) + b$\n",
    "- b. Apply the sigmoid activation function: $y = σ(z)$\n",
    "- c. If the threshold value for the sigmoid function is 0.5, what will be the output for the following inputs and weights?\n",
    "    - x1 = 0, x2 = 1, w1 = 0.5, w2 = -0.5, b = 0.2\n",
    "    - x1 = 1, x2 = 0, w1 = -0.4, w2 = 0.6, b = -0.1\n",
    "    \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "**Question 02:** Consider the following perceptron with two `inputs` (x1 and x2), `weights` (w1 and w2), and bias (b).\n",
    "\n",
    "<img src=\"images/homework.jpg\" height=300px width=300px>\n",
    "\n",
    "- a. Calculate the weighted sum: $z = (w1 * x1) + (w2 * x2) + b$\n",
    "- b. Apply the ReLU activation function: $y = max(0, z)$\n",
    "- c. What will be the output for the following inputs and weights?\n",
    "    - x1 = 0, x2 = 1, w1 = 0.5, w2 = -0.5, b = 0.2\n",
    "    - x1 = 1, x2 = 0, w1 = -0.4, w2 = 0.6, b = -0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61895227",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82b92ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6097fecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cf2bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3ae6b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2831366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c776ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb9fa22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c0acad7",
   "metadata": {},
   "source": [
    "# Mathematics Recap\n",
    "- 1. Scalar\n",
    "- 2. Vector, 1D,2D, 3D\n",
    "- 3. Vector Operations\n",
    "\t- Addition Operation\n",
    "    - Multiplication Operation(Element-wise & Dot Product)\n",
    "- 4. Differential Calculus\n",
    "\t- Chain Rule\n",
    "    - Derivative \n",
    "    - Gradient Descent Algorithm\n",
    "    - Two Examples to calculate Gradient Descent of functions with values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1913e0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e435a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "072cba9b",
   "metadata": {},
   "source": [
    "### 1. Scalar\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\"> <strong>A quantity that has magnitude but no particular direction is called scalar. For example, length, speed, mass, density, pressure, work, power, temperature, area, volume.</strong></div></p>\n",
    "    \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### 2. Vector\n",
    "  \n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\"> <strong>A quantity that has magnitude as well as direction is called vector. For example, displacement, velocity, weight, force. </strong></div></p>\n",
    "\n",
    "\n",
    "<img src=\"images/vector_vs_scalar.jpg\">\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "#### a. 1D Vector:\n",
    "\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\"> A 1D vector, also known as a scalar, is a vector that has only one component or direction. It can be represented mathematically as a single number or variable, such as x.</div></p>\n",
    "\n",
    "1D vector can be represented as:\n",
    "\n",
    "$$ \\overrightarrow{\\rm } =   \\begin{bmatrix} a_1 & a_2 & ... & a_n \\end{bmatrix}$$\n",
    "\n",
    "where $a_1, a_2, ..., a_n$ are the components of the vector.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "#### b. 2D vector: \n",
    "\n",
    "<p align='center' ><div class='alert alert-success' style='margin:20px'>A 2D vector is a vector that has two components or directions, typically represented as (x, y). It can be thought of as a point in a two-dimensional space, where the vector's magnitude is the distance from the origin to the point, and the direction is the angle between the vector and the positive x-axis. </div></p>\n",
    "\n",
    "The mathematical representation of a 2D vector is:\n",
    "$$ \\overrightarrow{\\rm v}= (a,b)= \n",
    "\\begin{bmatrix}\n",
    "a \\\\\n",
    "b\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where a and b are the components of the vector.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "#### c. 3D vector: \n",
    "\n",
    "<p align='center' ><div class='alert alert-success' style='margin:20px'>A 3D vector is a vector that has three components or directions, typically represented as (x, y, z). It can be thought of as a point in a three-dimensional space, where the vector's magnitude is the distance from the origin to the point, and the direction is the angle between the vector and the positive x-axis. </div></p>\n",
    "\n",
    "\n",
    "The mathematical representation of a 3D vector is:\n",
    "$$ \\overrightarrow{\\rm v}= (a,b,c)= \n",
    "\\begin{bmatrix}\n",
    "a \\\\\n",
    "b \\\\\n",
    "c\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where a, b and c are the components of the vector.\n",
    "\n",
    "\n",
    "<img src=\"images/2d-vs-3d-vectors.png\" height=500px width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41de8995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "\n",
    "#Scalar\n",
    "scalar = np.array(3)\n",
    "print(scalar.ndim)\n",
    "\n",
    "\n",
    "#1D Vector\n",
    "d1_vector = np.array([2,3,4,5,6])\n",
    "print(d1_vector.ndim)\n",
    "\n",
    "\n",
    "#2D Vector\n",
    "d2_vector = np.array([[2,3,4,5],\n",
    "                     [5,6,7,8]])\n",
    "print(d2_vector.ndim)\n",
    "\n",
    "\n",
    "#3D Vector\n",
    "d3_vector = np.array([[[2,3],\n",
    "                     [3,4]],\n",
    "                      \n",
    "                      [[5,4],\n",
    "                      [4,7]]\n",
    "                      ])\n",
    "print(d3_vector.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a0037e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7aefc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e95de22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86a6d0df",
   "metadata": {},
   "source": [
    "### 3. Vector Operations\n",
    "\n",
    "#### a. Addition Operation\n",
    "\n",
    "\n",
    "<p align='center' ><div class='alert alert-success' style='margin:20px'> <b>Vector addition</b> is the process of combining two or more vectors of the same dimension to produce a new vector. Geometrically, vector addition corresponds to placing the initial point of one vector at the terminal point of the other vector and then drawing a new vector from the initial point of the first vector to the terminal point of the second vector. Algebraically, the addition of two vectors A and B is performed by adding their corresponding components.</div></p>\n",
    "\n",
    "<br>\n",
    "\n",
    "**For two 1D vectors $\\overrightarrow{\\rm a}$ and $\\overrightarrow{\\rm b}$:**\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\overrightarrow{\\rm a} + \\overrightarrow{\\rm b} = \\begin{bmatrix} a_1 \\ a_2 \\ \\cdots \\ a_n \\end{bmatrix} + \\begin{bmatrix} b_1 \\ b_2 \\ \\cdots \\ b_n \\end{bmatrix} = \\\\ \\begin{bmatrix} a_1+b_1 , a_2+b_2 , \\cdots , a_n+b_n \\end{bmatrix}\n",
    "\\end{equation}$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**For two 2D vectors $\\overrightarrow{\\rm a}$ and $\\overrightarrow{\\rm b}$:**\n",
    "$$\\begin{equation}\n",
    "\\overrightarrow{\\rm a} + \\overrightarrow{\\rm b} = \\begin{bmatrix} a_1 \\\\ a_2 \\end{bmatrix} + \\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix} = \\begin{bmatrix} a_1+b_1 \\\\ a_2+b_2 \\end{bmatrix}\n",
    "\\end{equation}$$\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**For two 3D vectors $\\overrightarrow{\\rm a}$  and $\\overrightarrow{\\rm b}$:**\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\overrightarrow{\\rm a} + \\overrightarrow{\\rm b}  = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\end{bmatrix} + \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\end{bmatrix} = \\begin{bmatrix} a_1+b_1 \\\\ a_2+b_2 \\\\ a_3+b_3 \\end{bmatrix}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c986b31f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81e8456a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a26a52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2703823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67610fd0",
   "metadata": {},
   "source": [
    "#### b. Vector Multiplication\n",
    "\n",
    "- One common operation is scalar multiplication, which is the process of multiplying a vector by a scalar (a real number). Geometrically, scalar multiplication corresponds to scaling the vector by a factor of the scalar. Algebraically, scalar multiplication of a vector A by a scalar k is performed by multiplying each component of A by k.\n",
    "\n",
    "$$k.A = \\begin{bmatrix}\n",
    "ka_1 \\\n",
    "ka_2 \\\n",
    "\\cdots \\\n",
    "ka_n \\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "- Vector multiplication is the dot product or inner product, which is a scalar value obtained by multiplying the corresponding components of two vectors of the same dimension and then adding the results.\n",
    "\n",
    "$$\\vec{a} \\cdot \\vec{b} = a_1 b_1 + a_2 b_2 + \\dots + a_n b_n$$\n",
    "\n",
    "<img src=\"images/mutliplication.png\">\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**Example:**\n",
    "\n",
    "- Let $\\vec{u} = \\begin{bmatrix} 3 \\\\ -2 \\\\ 4 \\end{bmatrix}$ and $\\vec{v} = \\begin{bmatrix} 2 \\ 5 \\ -1 \\end{bmatrix}$ be two vectors in $\\mathbb{R}^3$. Find the dot product of $\\vec{u}$ . $\\vec{v}$\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "$$\\vec{u} \\cdot \\vec{v} = (3.2) + (-2.5) + (4.(-1)) = 6-10-4  = -8 $$\n",
    "\n",
    "\n",
    "> **Result**: If the dot product is `negative`, it means that the angle between the vectors is `obtuse`, or greater than 90 degrees. It means they are pointing in `opposite` directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794daf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "u = [3,-2,4]\n",
    "v = [2,5,-1]\n",
    "np.dot(u,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185ffe61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139fe153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e383ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1d69983",
   "metadata": {},
   "source": [
    "## Differential Calculus\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">Differential calculus is a branch of calculus that deals with the study of the rate at which quantities change. </div></p>\n",
    "<br>\n",
    "\n",
    "### 4.1. Chain Rule\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">Chain Rule has many applications in Machine learning and one of them is Gradient Descent Algorithm. One of its important application is <strong>Back Propagation</strong> Algorithm, which is used to train neural networks. It is based on the idea of <strong>nested/composite functions</strong>. It is an easy way to find the derivative of nested functions. We can differentiate complex functions by splitting them into simple one and them applying the diffenertiation using chain rule. For Example:   \n",
    "</div></p>\n",
    "\n",
    "\n",
    "$$ \\mathbf{\\frac{dy}{dx} = \\frac{dy}{du} \\frac{du}{dx}}  \\;\\;\\;\\; Equation -- (i)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "**Example:** Find the Derivative of the following function:\n",
    "\n",
    "$$ f(x) = (2x^2 + 8) ^2$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "- Suppose $u = 2x^2 + 8$, then $ y = u^2$ \n",
    "- First compute the derivative of (dy/du): \n",
    "$$ \\frac{dy}{du} = 2u = 2(2x^2+8) = 4x^2 +16 $$\n",
    "\n",
    "- Now compute the derivative of (du/dx): \n",
    "$$ \\frac{du}{dx} = 2x^2 + 8 = 4x$$\n",
    "\n",
    "- Now, putting the values in the equation (i)\n",
    "\n",
    "$$ \\mathbf{\\frac{dy}{dx} = \\frac{dy}{du} \\frac{du}{dx} = (4x^2 + 16)(4x) = 16x^3 + 64x = 8x(2x^2+8) }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbff65bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation\n",
    "import sympy as sym\n",
    "\n",
    "#variable\n",
    "x = sym.symbols('x')\n",
    "fx= (2*x**2 + 8)**2\n",
    "\n",
    "# Differentiation of function\n",
    "sym.diff(fx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2af61f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb86f02b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f73ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c634853",
   "metadata": {},
   "source": [
    "### 4.2 Derivative of a Function\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">A derivative/slope is a mathematical term used to describe the rate of change of a function with respect to its input variables. In simpler terms, the derivative tells us how much a function changes when its input variables change a little bit.  </div></p>\n",
    "\n",
    "**Example 1**\n",
    "\n",
    "- The derivative of the function $\\mathbf{f(x) = 3x^2 + 5x + 3}$ can be calculated as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{d}{dx}f(x) &= \\frac{d}{dx}(3x^2+5x+3)\\\\\n",
    "&= \\frac{d}{dx}(3x^2) + \\frac{d}{dx}(5x) + \\frac{d}{dx}(3)\\\\\n",
    "&= 6x + 5 + 0\\\\\n",
    "&= 6x + 5\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore, the derivative of $f(x)$ is $f'(x) = 6x + 5$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c9eeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 3*x**2 + 5*x + 3\n",
    "\n",
    "def f_derivative(x):\n",
    "    return 6*x + 5\n",
    "\n",
    "# Example usage\n",
    "x = 2\n",
    "slope = f_derivative(x)\n",
    "print(slope)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed25a67",
   "metadata": {},
   "source": [
    "> **Note:** The output will be 17, which is the slope of the function at `x=2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5919a759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11def47c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74151a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d5746a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec92e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b1275fd",
   "metadata": {},
   "source": [
    "**Example 2**\n",
    "- Find the partial derivatives of the function $\\mathbf{𝑓(𝑥,y)=3𝑥^2+5𝑥y+3}$.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "- Partial derivative with respect to $x$:\n",
    "\n",
    "\\begin{aligned} \\frac{\\partial f}{\\partial x} &= \\frac{\\partial}{\\partial x} (3x^2 + 5xy + 3) \\ &= 6x + 5y\\frac{\\partial}{\\partial x}(x) + \\frac{\\partial}{\\partial x}(3) \\ &= 6x + 5y \\end{aligned}\n",
    "\n",
    "\n",
    "\n",
    "- Partial derivative with respect to $y$:\n",
    "\n",
    "\n",
    "\\begin{aligned} \\frac{\\partial f}{\\partial y} &= \\frac{\\partial}{\\partial y} (3x^2 + 5xy + 3) \\ &= 3x^2 + 5x\\frac{\\partial}{\\partial y}(y) + \\frac{\\partial}{\\partial y}(3) \\ &= 5x \\end{aligned}\n",
    "\n",
    "\n",
    "- So, The partial derivatives of $f(x,y)$ with respect to $x$ and $y$ are:\n",
    "\n",
    "\n",
    "\\begin{aligned} \\frac{\\partial f}{\\partial x} &= 6x + 5y \\ \\frac{\\partial f}{\\partial y} &= (6x+5x5y) \\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8c55d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, y):\n",
    "    return 3*x**2 + 5*x*y + 3\n",
    "\n",
    "def df_dx(x, y):\n",
    "    return 6*x + 5*y\n",
    "\n",
    "def df_dy(x, y):\n",
    "    return 5*x\n",
    "\n",
    "#The point is (2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c1d48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97bf03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dx(2,3), df_dy(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c69be7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cf2ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724ecbf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a3c828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6302554",
   "metadata": {},
   "source": [
    "### 4.3 Gradient of the function\n",
    "\n",
    "- The gradient of the function $\\mathbf{f(x,y)=3x^2+5xy+3}$ is:\n",
    "\n",
    "$$\\nabla f(x,y) = \\begin{pmatrix} \\frac{\\partial f(x,y)}{\\partial x} \\ \\frac{\\partial f(x,y)}{\\partial y} \\end{pmatrix}$$\n",
    "\n",
    "- We have computed the partial derivatives as follows:\n",
    "\n",
    "$$\\frac{\\partial f(x,y)}{\\partial x} = 6x + 5y$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\frac{\\partial f(x,y)}{\\partial y} = 5x$$\n",
    "\n",
    "- Therefore, the gradient of the function is:\n",
    "\n",
    "$$\\nabla f(x,y) = \\begin{pmatrix} 6x + 5y \\ 5x \\end{pmatrix}$$\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Calculate the Gradient of above function at `point(4,2)` is :\n",
    "\n",
    "$$\\frac{\\partial f(x,y)}{\\partial x} = 6(4) + 5(2) = 34$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\\frac{\\partial f(x,y)}{\\partial y} = 5x = 5(4) = 20$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e593635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d631a918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x, y):\n",
    "    return 3*x**2 + 5*x*y + 3\n",
    "\n",
    "def gradient_f(x, y):\n",
    "    return np.array([6*x + 5*y, 5*x])\n",
    "\n",
    "# Calculating gradient at point (4, 2)\n",
    "grad = gradient_f(4, 2)\n",
    "print(grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a1ae36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51075a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e69cd05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700692c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4217e6db",
   "metadata": {},
   "source": [
    "# Test Yourself\n",
    "\n",
    "## Example 1:\n",
    "\n",
    "- Consider the function: $$f(x) = 2x^3 + 5x^2 - 7x + 1$$.\n",
    "\n",
    "\t- a) Find the derivative of the function.\n",
    "\n",
    "    - b) Find the equation of the tangent line to the function at the point $x=2$.\n",
    "\n",
    "\t- c) Find the point(s) where the tangent line is parallel to the line $y=3x+1$.\n",
    "\n",
    "\n",
    "\n",
    "## Example 2:\n",
    "\n",
    "- Consider the function $$f(x,y) = x^3 + 3xy^2 - 5x + y$$.\n",
    "\n",
    "\t- a) Find the partial derivative of the function with respect to $x$.\n",
    "\n",
    "\t- b) Find the partial derivative of the function with respect to $y$.\n",
    "\n",
    "\t- c) Find the gradient of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82017111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6302e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a838e2e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27a9aea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ce82a24",
   "metadata": {},
   "source": [
    "### 4.4 Gradient Descent Algorithm\n",
    "\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\"> <b>Gradient descent</b> is a numerical optimization algorithm used to find the minimum value of a function by iteratively adjusting its parameters in the direction of steepest descent of the gradient. It is commonly used in machine learning and deep learning to minimize the cost function, which measures the difference between the predicted output and the actual output. The goal is to find the parameter values that minimize the cost function, which in turn maximizes the accuracy of the prediction.  </div></p>\n",
    "\n",
    "\n",
    "**Example:** Implementation of Gradient Descent Algorithm for the function f(x) is: $$\\mathbf{f(x) = x^2 + 2x + 2}$$\n",
    "\n",
    "\n",
    "\n",
    "- Calculate the derivative of $f(x)$:\n",
    "\n",
    "$$ f'(x) = 2x + x $$\n",
    "\n",
    "\n",
    "- Choose a random initial value for $x$ as $x_0$.\n",
    "\n",
    "\n",
    "- Choose a small learning rate $\\alpha$.\n",
    "\n",
    "\n",
    "\n",
    "- Update the value of $x$ using the gradient descent algorithm:\n",
    "\n",
    "$$ x_{n+1} = x_n - \\alpha f'(x_n)$$\n",
    "\n",
    "\n",
    "- Repeat step 5 until the algorithm converges, or until the maximum number of iterations is reached.\n",
    "\n",
    "\n",
    "\n",
    "- The value of $x$ at the end of the algorithm is the approximate minimum of the function.\n",
    "\n",
    "\n",
    "- Let's say we choose $ \\mathbf{x_0 = 3}$ and learning rate $\\mathbf{\\alpha = 0.1}$. Then the gradient descent algorithm will be:\n",
    "\n",
    "$$ x_{n+1} = x_n - \\alpha f'(x_n)$$\n",
    "\n",
    "\n",
    "$$ x_{n+1} = x_n - 0.1 (2x_n+2)$$\n",
    "\n",
    "$$ x_{n+1} = 0.8x_n - 0.2$$\n",
    "\n",
    "\n",
    "\n",
    "- We can use this equation to iteratively update the value of $x$ until convergence. For example, let's do \t`5` iterations:\n",
    "\n",
    "$$ x_0 = 3 $$ \n",
    "\n",
    "$$x_1 = 0.8(3)−0.2 = 2.4 $$\n",
    "\n",
    "\n",
    "$$x_2 = 0.8(2.4)−0.2 = 1.76 $$\n",
    "\n",
    "\n",
    "$$x_3 = 0.8(1.76)−0.2 = 1.408 $$\n",
    "\n",
    "\n",
    "$$x_4 = 0.8(1.408)−0.2 = 1.264 $$\n",
    "\n",
    "\n",
    "$$x_5 = 0.8(1.264)−0.2 = 0.90112 $$\n",
    "\n",
    "\n",
    "\n",
    "> **Note**: After 5 iterations, the value of $x$ has converged to approximately 0.90112, which is the approximate minimum of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f96d269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeb3d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9055067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66484a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be90bb75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae69d214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20314f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072b39c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef0a912d",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center\">e. Concept of Layers of Neural Network</h2> \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">\n",
    " <h3><strong> Single-layer Feedforward Neural Networks VS Multiple Hidden Layers Neural Network </strong></h3>   \n",
    "</div></p>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"images/Single.png\" height=500px width=450px align=left>\n",
    "<img src=\"images/multi.jpeg\" height=500px width=400px align=right>\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "### Why Layers are used in Deep Learning?\n",
    "\n",
    "<img src=\"images/layer.webp\" height=500px width=400px>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- **Feature extraction:** Each layer in a deep neural network extracts features from the input data. The initial layers capture low-level features, and the subsequent layers combine them to form higher-level features. This hierarchical feature extraction enables the network to learn complex representations of the data.\n",
    "\n",
    "- **Non-linearity:** Layers in deep learning models use non-linear activation functions that enable the network to learn complex relationships between the input and output. Non-linear activation functions like ReLU, sigmoid, and tanh introduce non-linearity into the network and enable it to approximate complex functions.\n",
    "\n",
    "- **Generalization:** Layers help in generalizing the learned features to new, unseen data. By learning features that are relevant to the task, the network can generalize better to new data and achieve better performance.\n",
    "\n",
    "- **Parameter sharing:** Layers in a neural network share parameters across different input regions. This parameter sharing reduces the number of parameters to be learned, making it easier to train deep neural networks.\n",
    "\n",
    "- **Hierarchical organization:** The use of layers in deep learning models results in a hierarchical organization of the network. This hierarchical organization enables the network to learn complex features and relationships by combining simpler features learned in earlier layers.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Single-hidden layer neural network:\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">\n",
    " A single-hidden layer neural network is a type of artificial neural network architecture that consists of an input layer, a single hidden layer, and an output layer. The hidden layer sits between the input and output layers and contains one or more hidden neurons. Each neuron in the hidden layer receives input from the input layer and produces an output that is sent to the output layer. The weights and biases of the network are learned through a process called backpropagation during training.  \n",
    "</div></p>\n",
    "\n",
    "\n",
    "The layers in a single-hidden layer neural network can be defined as follows:\n",
    "\n",
    "- **Input Layer:** \n",
    "\t- This layer consists of the input variables/features that are fed into the network. \n",
    "    - Each input feature is represented by a neuron in the input layer.\n",
    "\n",
    "- **Hidden Layer:** \n",
    "\t- This layer contains one or more hidden neurons that perform computations on the input data. \n",
    "    - Each neuron in the hidden layer receives input from the input layer and produces an output that is sent to the output layer. \n",
    "    - The activation functions used in the hidden layer can be sigmoid, ReLU, or tanh.\n",
    "\n",
    "- **Output Layer:** \n",
    "\t- This layer produces the final output of the network. The number of neurons in the output layer depends on the type of problem being solved. \n",
    "    - For example, for binary classification, there will be one output neuron that outputs a value between 0 and 1, while for multiclass classification, there will be multiple output neurons, with each neuron representing a class.\n",
    "\n",
    "<img src=\"images/Single.png\" height=500px width=450px align=center>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Multilayer Neural Network(Deep Neural Network)\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">\n",
    " A multilayer neural network, also known as a deep neural network, is a neural network composed of more than one hidden layer. Each layer in a multilayer neural network can consist of multiple nodes, or neurons, which process information and pass it on to the next layer.\n",
    "</div></p>\n",
    "\n",
    "The layers in a multilayer neural network can be divided into three main types:\n",
    "\n",
    "- **Input layer:** \n",
    "\t- The input layer receives the data to be processed by the network. \n",
    "    - Each node in the input layer represents one feature of the input data.\n",
    "\n",
    "- **Hidden layers:** \n",
    "\t- The hidden layers are responsible for transforming the input data into a form that can be used to make predictions. \n",
    "    - Each node in a hidden layer receives input from the nodes in the previous layer, performs a computation, and passes the result on to the nodes in the next layer. \n",
    "    - The number of hidden layers and the number of nodes in each layer are hyperparameters that can be tuned to optimize the performance of the network.\n",
    "\n",
    "- **Output layer:** \n",
    "\t- The output layer produces the final output of the network. The number of nodes in the output layer depends on the type of problem being solved. \n",
    "    - For example, in a binary classification problem, the output layer might have a single node that produces a probability of the input belonging to one of the two classes. \n",
    "    - In a multiclass classification problem, the output layer might have multiple nodes that produce probabilities of the input belonging to each class. \n",
    "    - In a regression problem, the output layer might have a single node that produces a continuous output value.\n",
    "\n",
    "<img src=\"images/multi.jpeg\" height=500px width=400px align=center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071f192e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf92be8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a2bae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfcdde0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca018812",
   "metadata": {},
   "source": [
    "# Real World Problem : Bank Transaction\n",
    "\n",
    "Let's say we want to predict the number of bank transactions a person will do in a month based on two features: - \n",
    "- The number of children they have and\n",
    "- The number of existing bank accounts they hold.\n",
    "\n",
    "\n",
    "<img src =\"images/transactions.png\" height=600px width=600px>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19044bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe24ad68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3025e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f68015eb",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "### 1. Simple Linear Regression Approach\n",
    "\n",
    "\n",
    "Let $x_1$ be the number of children and $x_2$ be the number of existing bank accounts. Then, we can represent the number of bank transactions $\\mathbf{y}$ as:\n",
    "\n",
    "$$ \\mathbf{y = b + w_1 * x_1 + w_2 * x_2} $$\n",
    "\n",
    "\n",
    "\n",
    "<img src =\"images/transaction1.png\" height=600px width=600px>\n",
    "\n",
    "\n",
    "We have randomly chosen the following values:\n",
    "\n",
    "- Number of children: 5\n",
    "- Number of bank accounts: 3\n",
    "- Weights: [0.5, 0.2]\n",
    "- Intercept: 0.1\n",
    "\n",
    "The mathematical equation for the predicted number of bank transactions, y, can be expressed as:\n",
    "\n",
    "$$ \\mathbf{y = 0.5 * 5 + 0.2 * 3 + 0.1 }$$\n",
    "\n",
    "Simplifying, we get:\n",
    "\n",
    "$$ \\mathbf{y = 3.0 }$$\n",
    "\n",
    "> **Result:** Our prediction for the number of bank transactions a person with 5 children and 3 bank accounts will do in a month is 3.0 by using SLR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af19895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs/features for model\n",
    "inputs = np.array([5,3])\n",
    "\n",
    "#weights for the features\n",
    "weights = np.array([0.5, 0.2])\n",
    "\n",
    "#intercept\n",
    "bias = 0.1\n",
    "\n",
    "# number of transactions\n",
    "y = bias + np.dot(inputs, weights)\n",
    "\n",
    "print(f\" Number of transactions are : {np.round(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2455af9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8280170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1916c6af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5a36f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef97cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1a30aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecc4280c",
   "metadata": {},
   "source": [
    "### 2. Neural Network Approach\n",
    "\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">\n",
    "We want to build a neural network with one hidden layer of two nodes, which can learn to predict the number of <b>bank transactions</b> a person will do in a month by taking into account the number of children they have and the number of existing bank accounts they hold.\n",
    " \n",
    "</div></p>\n",
    "\n",
    "In this case, we already calculated the values of nodes of hidden layer without using any activation function, now, we just want to predict number of transactions.\n",
    "\n",
    "<img src=\"images/transaction3.png\" height=500px width=500px align=right>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**For Node-1 of Hidden Layer:**\n",
    "\n",
    "$$\\text{node1 = no. of children * weight + no. of accounts * weight}$$\n",
    "\n",
    "$$\\text{node1 = 2.1 + 3.1 = 5}$$\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    " \n",
    "**For Node-2 of Hidden Layer:**\n",
    "\n",
    "$$\\text{node2 = no. of children * weight + no. of accounts * weight}$$\n",
    "\n",
    "$$\\text{node2 = 2.1 + 3.(-1) = -2+3 = 1}$$\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    " \n",
    " **For Output of Hidden Layer:**\n",
    "\n",
    "$$\\text{output = node1 * weight + node2 * weight}$$\n",
    "\n",
    "$$\\text{output = 5.2 + 1.(-1) = 10-1 = 9}$$\n",
    "\n",
    "\n",
    "> **Result:** Our prediction for the number of bank transactions a person with 2 children and 3 bank accounts will do in a month is 9 by using Neural Network.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"images/transaction2.png\" height=500px width=500px align=center>\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<h3 style=\"color:red\" align=center>Welcome to Forward Propagation</h3>\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">\n",
    "<b>Forward propagation in neural network is the process of computing the output of the network for a given input by passing it through the layers of neurons in a forward direction, from input to output layer. This involves multiplying the input values with weights, adding biases, and passing the resulting values through an activation function to obtain the output of each neuron in the network.</b> \n",
    "</div></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04dccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation\n",
    "\n",
    "#inputs and weights of neural network\n",
    "inputs = np.array([2,3])\n",
    "weights = {'node_0':np.array([1,1]),\n",
    "          'node_1':np.array([-1,1]),\n",
    "          'output':np.array([2,-1])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6cd1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#value of node 1 of hidden layer\n",
    "\n",
    "node_0_value = (inputs*weights['node_0']).sum()\n",
    "node_0_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37723b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#value of node 2 of hidden layer\n",
    "\n",
    "node_1_value = (inputs*weights['node_1']).sum()\n",
    "node_1_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f750c97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hidden layer values\n",
    "hidden_layer_values = np.array([node_0_value,node_1_value])\n",
    "hidden_layer_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a68cf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output of neural network\n",
    "\n",
    "output = (hidden_layer_values * weights['output']).sum()\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656bb917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cc34b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93afac80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ddf6e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d58731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "738b88f8",
   "metadata": {},
   "source": [
    "### Another Neural Network\n",
    "\n",
    "**In this case, we want to predict the number of transactions, by calculating values of nodes of hidden layer, and using `relu activation` function, our target value is 30.**\n",
    "\n",
    "<img src=\"images/transaction4.png\" align=right>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- Lets the number of children are 5, and bank accounts are 3 in this case.\n",
    "\n",
    "**For Node-1 of Hidden Layer:**\n",
    "\n",
    "$$\\text{node1 = no. of children * weight + no. of accounts * weight}$$\n",
    "\n",
    "$$\\text{node1 = 5.4 + 3.2 = 26}$$\n",
    "$$\\text{node1_output = relu(26) = 26}$$\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    " \n",
    "**For Node-2 of Hidden Layer:**\n",
    "\n",
    "$$\\text{node2 = no. of children * weight + no. of accounts * weight}$$\n",
    "\n",
    "$$\\text{node2 = 5.(-5) + 3.4 = -25+12 = -13}$$\n",
    "$$\\text{node2_output = relu(-13) = 0}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    " \n",
    " **For Output of Hidden Layer:**\n",
    "\n",
    "$$\\text{output = node1_output * weight + node2_output * weight}$$\n",
    "\n",
    "$$\\text{output = 26.2 + 0.7 = 52 }$$\n",
    "\n",
    "\n",
    "> **Result:** Our prediction for the number of bank transactions a person with 5 children and 3 bank accounts will do in a month is 52 by using Neural Network.But our target value is 30, it means we have loss of 52-30 = 22.\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436c3adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation\n",
    "\n",
    "#inputs and weights of neural network\n",
    "inputs = np.array([3,5])\n",
    "weights = {'node_0':np.array([2,4]),\n",
    "          'node_1':np.array([4,-5]),\n",
    "          'output':np.array([2,-7])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5215029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#value of node 1 of hidden layer\n",
    "\n",
    "from keras.activations import relu\n",
    "\n",
    "node_0_value = (inputs*weights['node_0']).sum()\n",
    "\n",
    "#passing weighted sum through activation function\n",
    "node_0_output = relu(node_0_value)\n",
    "\n",
    "node_0_output.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae2bbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_0_output = relu(node_0_value).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe615f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#value of node 2 of hidden layer\n",
    "\n",
    "from keras.activations import relu\n",
    "\n",
    "node_1_value = (inputs*weights['node_1']).sum()\n",
    "\n",
    "#passing weighted sum through activation function\n",
    "node_1_output = relu(node_1_value).numpy()\n",
    "\n",
    "node_1_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d820d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hidden_layer_outputs\n",
    "\n",
    "hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "output = (hidden_layer_outputs*weights['output']).sum()\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86e5ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d27a375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889708d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3624d51a",
   "metadata": {},
   "source": [
    "### Another Neural Network\n",
    "\n",
    "**In this case, we want to predict the number of transactions, by calculating values of nodes of hidden layer, and using `relu activation` function,but there is some modification in weights of the nodes of hidden layer,  our target value is 30.**\n",
    "\n",
    "<img src=\"images/transaction5.png\" align=right>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c859dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation\n",
    "\n",
    "#inputs and weights of neural network\n",
    "inputs = np.array([3,5])\n",
    "weights = {'node_0':np.array([2,2]),\n",
    "          'node_1':np.array([2,-5]),\n",
    "          'output':np.array([2,-7])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d04432",
   "metadata": {},
   "outputs": [],
   "source": [
    "#value of node 1 of hidden layer\n",
    "\n",
    "from keras.activations import relu\n",
    "\n",
    "node_0_value = (inputs*weights['node_0']).sum()\n",
    "\n",
    "#passing weighted sum through activation function\n",
    "node_0_output = relu(node_0_value)\n",
    "\n",
    "node_0_output.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a7a8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_0_output = relu(node_0_value).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2176d4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#value of node 2 of hidden layer\n",
    "\n",
    "from keras.activations import relu\n",
    "\n",
    "node_1_value = (inputs*weights['node_1']).sum()\n",
    "\n",
    "#passing weighted sum through activation function\n",
    "node_1_output = relu(node_1_value).numpy()\n",
    "\n",
    "node_1_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0783744",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hidden_layer_outputs\n",
    "\n",
    "hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "output = (hidden_layer_outputs*weights['output']).sum()\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47443548",
   "metadata": {},
   "source": [
    "\n",
    "> **Result:** Our prediction for the number of bank transactions a person with 5 children and 3 bank accounts will do in a month is 32 by using Neural Network.But our target value is 30, it means we have loss of 32-30 = 2. By changing the weights, the model prediction and target values are closer to each other.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "<h3 style=\"color:red\" align=center>Welcome to Back-Propagation</h3>\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">\n",
    "<b>Back propagation is a way of adjusting the weights in a neural network so that it can better predict the correct output. It involves working backwards from the output layer to the input layer and using the difference between the predicted and actual outputs to adjust the weights. The goal is to minimize the error between the predicted output and the actual output.</b> \n",
    "</div></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12488a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b35c51e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859c6a02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2770032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e51f61e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2fd515",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "994b786f",
   "metadata": {},
   "source": [
    "### Some Practice Question\n",
    "\n",
    "- Calculate the output of this neural network by using `Relu` Activation Function.\n",
    "\n",
    "<img src=\"images/transaction6.png\" height=500px width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04333562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacb9680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c6e7c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7c9810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c35ab26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8559e245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be13f742",
   "metadata": {},
   "source": [
    "<h2  style=\"color:green;\" align=center>Symbols and Notation used in Neural Network</h2>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"images/ANN4-Table.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da411600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3df960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aebf44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad7dca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ffb5a88",
   "metadata": {},
   "source": [
    "### Concept of Forward Propagation\n",
    "\n",
    "- Why we need Forward Propagation?\n",
    "- What is Forward Propagation?\n",
    "- How it works?\n",
    "- Example\n",
    "\n",
    "\n",
    "\n",
    "<h3 style=\"color:red\" align=center>Why we need Forward Propagation?</h3>\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">\n",
    "<b>Forward propagation is needed in neural networks to make predictions or decisions based on input data. The goal of a neural network is to learn to map inputs to outputs through a series of mathematical operations performed on the input data. Forward propagation is the process of passing the input data through the neural network to produce an output prediction.</b> </div></p>\n",
    "\n",
    "<br>\n",
    "\n",
    "<h3 style=\"color:red\" align=center>What is Forward Propagation?</h3>\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">\n",
    "<b>Forward propagation is the process of computing the output of a neural network given a set of input values. It involves passing the input data through each layer of the network, where each layer performs a series of mathematical operations on the input data to produce an output value. The output of one layer becomes the input to the next layer until the output of the final layer is produced.</b> </div></p>\n",
    "\n",
    "<br>\n",
    "\n",
    "<h3 style=\"color:red\" align=center>How it works?</h3>\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\"><b>Forward propagation works by taking the input data and passing it through the neural network layer by layer. Each layer performs a series of mathematical operations on the input data, which is then passed to the next layer. The output of the final layer represents the prediction or decision made by the neural network based on the input data.</b> </div></p>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "**Example**\n",
    "For example, imagine a neural network that is designed to predict whether an image contains a dog or a cat. The input to the neural network would be an image of either a dog or a cat, and the output would be a prediction of whether the image contains a dog or a cat. **Forward propagation** would involve passing the image through each layer of the neural network, with each layer performing a series of mathematical operations on the image data to produce an output value. The output of the final layer would represent the prediction of whether the image contains a dog or a cat.\n",
    "\n",
    "<img src=\"images/picture5.gif\"  height=400px width=500px>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fda230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f278c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95294282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9f5998f",
   "metadata": {},
   "source": [
    "## Mathematics of Forward Propagation\n",
    "\n",
    "\n",
    "### Step by Step Guidelines\n",
    "\n",
    "\n",
    "- Initialize the weights and biases for each layer randomly.\n",
    "- Feed the input features to the input layer.\n",
    "- Multiply the input values with the weights of the first hidden layer, and add the bias term.\n",
    "- Apply an activation function (such as ReLU or sigmoid) to the weighted sum to obtain the output of the first hidden layer.\n",
    "- Repeat steps 3 and 4 for the second hidden layer.\n",
    "- Finally, multiply the output of the second hidden layer with the weights of the output layer, and add the bias term.\n",
    "- Apply the activation function (such as ReLU or sigmoid) to the weighted sum to obtain the predicted output value.\n",
    "\n",
    "\n",
    "\n",
    "### Problem:\n",
    "\n",
    "- Suppose we want to predict our test score based on how many hours we sleep and how many hours we study the night before.\n",
    "\n",
    "- In other words, we want to predict output value $\\mathbf{y}$ which are scores for a given set of input values $\\mathbf{X}$ which are hours of (sleep, study). \n",
    "\n",
    "<img src=\"images/transaction8.png\" align=left height=400px width=400px>\n",
    "<img src=\"images/transaction7.png\" align=right height=450px width=450px>\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- **This is a supervised regression problem.**\n",
    "\n",
    "\n",
    "\n",
    "- **Our network has 2 inputs, 3 hidden units, and 1 output.**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "For this example:\n",
    "\n",
    "**First Step:** \n",
    "\n",
    "$$z^{(2)} = XW^{(1)}  \\tag 1$$\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Second Step:**\n",
    "\n",
    "$$ \\begin{align} a^{(2)} = f \\left( z^{(2)} \\right) &  \\tag 2  \\end{align}$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Third Step:**\n",
    "\n",
    "$$ \\begin{align}   z^{(3)} = a^{(2)} W^{(2)}   \\tag 3\\\\ \\end{align}$$\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Fourth Step:**\n",
    "\n",
    "$$ \\begin{align}   \\hat y = f \\left( z^{(3)} \\right) \\tag 4\\end{align}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Each input value in matrix $\\mathbf{X}$ should be multiplied by a corresponding weight and then added together with all the other results for each neuron.\n",
    "\n",
    "\n",
    "$\\mathbf{z^{(2)}}$ is the activity of our second layer and it can be calculated as the following:\n",
    "$$z^{(2)} = XW^{(1)} \\tag 1 $$\n",
    "\n",
    "\n",
    "$$= \\begin{bmatrix}\n",
    "        3 & 5  \\\\\n",
    "        5 & 1  \\\\\n",
    "        10 & 2  \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}                                                              \n",
    "        W_{11}^{(1)} & W_{12}^{(1)}  & W_{13}^{(1)}\\\\\n",
    "        W_{21}^{(1)} & W_{22}^{(1)}  & W_{23}^{(1)}  \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "$$ = \\begin{bmatrix}\n",
    "3 W_{11}^{(1)} + 5 W_{21}^{(1)} & 3 W_{12}^{(1)} + 5 W_{22}^{(1)} & 3 W_{13}^{(1)} + 5 W_{23}^{(1)} \\\\\n",
    "5 W_{11}^{(1)} +  W_{21}^{(1)} & 5 W_{12}^{(1)} +  W_{22}^{(1)} & 5 W_{13}^{(1)} + W_{23}^{(1)} \\\\\n",
    "10 W_{11}^{(1)} + 2 W_{21}^{(1)} & 10 W_{12}^{(1)} + 2 W_{22}^{(1)} & 10 W_{13}^{(1)} + 2 W_{23}^{(1)} \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa3fb3a",
   "metadata": {},
   "source": [
    "Note that each entry in $z$ is a sum of weighted inputs to each hidden neuron. $z$ is $3 \\times 3$ matrix, one row for each sample, and one column for each hidden unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591937df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input features \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "inputs = np.array([[3,5],\n",
    "                  [5,1],\n",
    "                  [10,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a023a048",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights for layer1\n",
    "layer_1_weights = np.random.randint(-5, 15, size=(2,3))\n",
    "layer_2_weights = np.random.randint(-5, 15, size=(3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3977e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1_values = np.dot(inputs, weights)\n",
    "layer_1_outputs = relu(layer_1_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef920272",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_2_values = np.dot(layer_1_outputs, layer_2_weights)\n",
    "layer_2_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562f12b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de465b72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6393d14a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ad91178",
   "metadata": {},
   "source": [
    "### Concept of Back Propagation\n",
    "\n",
    "- Why we need Back Propagation?\n",
    "- What is Back Propagation?\n",
    "- How it Works?\n",
    "- Example\n",
    "\n",
    "\n",
    "\n",
    "<h3 style=\"color:red\" align=center>Why we need Back Propagation?</h3>\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">\n",
    "<b>We need back propagation to train a neural network to make better predictions by adjusting the weights and biases of the network. Back propagation allows us to calculate the error or loss of the network and adjust the weights and biases to minimize that error, improving the accuracy of the network.</b> </div></p>\n",
    "\n",
    "<br>\n",
    "\n",
    "<h3 style=\"color:red\" align=center>What is Back Propagation?</h3>\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">\n",
    "<b>Back propagation is a process used in training a neural network to adjust the weights and biases of the network to minimize the error or loss. It works by calculating the gradient of the error with respect to the weights and biases of the network using the chain rule of calculus, and then using this gradient to update the weights and biases through gradient descent.</b> </div></p>\n",
    "\n",
    "<br>\n",
    "\n",
    "<h3 style=\"color:red\" align=center>How it works?</h3>\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\"><b>Back propagation works by first performing forward propagation to calculate the predicted output of the network for a given input. Then, the error or loss of the network is calculated by comparing the predicted output to the actual output. The error is then propagated backwards through the network, layer by layer, to calculate the gradient of the error with respect to the weights and biases of the network using the chain rule of calculus. The weights and biases are then updated using the calculated gradient through gradient descent.</b> </div></p>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "**Example**\n",
    "Suppose we have a neural network that is trained to recognize handwritten digits. Back propagation is used to adjust the weights and biases of the network to minimize the error or loss of the network. The error is calculated by comparing the predicted output of the network for a given input (i.e., an image of a handwritten digit) to the actual output (i.e., the correct label for that digit). The error is then propagated backwards through the network, layer by layer, to calculate the gradient of the error with respect to the weights and biases of the network using the chain rule of calculus. The weights and biases are then updated using the calculated gradient through gradient descent. This process is repeated for many training examples until the network is able to accurately recognize handwritten digits.\n",
    "\n",
    "<img src=\"images/picture6.gif\"  height=400px width=500px>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4085058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd636d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80de5fb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e077fad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a1e233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871fd4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112c6da0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f209970",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5e3601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4909c76b",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center\">e. Implementation of Perceptron from Srcatch</h2> \n",
    "\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">\n",
    "Perceptron  is the most fundamental type of element in a neural network. \n",
    " \n",
    "</div></p>\n",
    "\n",
    "\n",
    "<img src=\"images/image1.png\" height=700px width=700px>\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Linearly Separable data\n",
    "\n",
    "- A **perceptron** can only solve linearly separable problems. What is a linearly separable problem ? Here are a couple of examples that show us linearly separable data. For example, two if the iris species are linearly separable by a hyperplane (in this case a single line). Similarly, an OR gate is also an example of a linearly separable dataset.\n",
    "\n",
    "<img src=\"images/image2.png\" height=700px width=700px>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ff1264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    " \n",
    "# OR gate data\n",
    "x = np.array([[1,0],\n",
    "              [0,1],\n",
    "              [0,0],\n",
    "              [1,1]])\n",
    "y   = np.array([1,1,0,1])\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "plt.scatter(x[:,0],x[:,1],c=y)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c730a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize just 2 species (setosa, versicolor) that are linearly separable \n",
    "# using the predictors (Sepel Length, Sepal, Width)\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "data = iris.data\n",
    " \n",
    "# visualize just the first 100 rows (so that it contains only the species setosa and versicolor)\n",
    "# We are specifically not plotting the third species (virginica), because it is not linearly separable.\n",
    "plt.scatter(data[0:100,0],data[0:100,1],c=iris.target[0:100])\n",
    "plt.xlabel(\"sepal length\")\n",
    "plt.ylabel(\"sepal width\")\n",
    "plt.title(\"iris species - Setosa, Versicolor\")\n",
    "plt.savefig(\"iris.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5393c40",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "\n",
    "\n",
    "<img src=\"images/image3.png\" height=600px width=600px>\n",
    "\n",
    "\n",
    "\n",
    "What we are essentially trying to do is to find out values for weights and bias in such a way that\n",
    "\n",
    "$$ \\mathbf{activation(x_1*w_1 + x_2*w_2 + b) = output}$$\n",
    "\n",
    "The activation function in the case of perceptron is simple stepper function\n",
    "\n",
    "$$ \\mathbf{y = f(x)}$$\n",
    "\n",
    "where\n",
    "\n",
    "$$ f(x) = 1 \\; if \\; x >= 0$$\n",
    "$$ f(x) = 0 \\; if \\; x > 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9886acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace (-5,5,100)\n",
    "y = np.zeros(len(x))\n",
    "y[x>=0] = 1\n",
    "y[x<0] = 0\n",
    " \n",
    "plt.scatter(x,y)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Activation function - Binary step function \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd161bd6",
   "metadata": {},
   "source": [
    "### OR gate problem (or any other linearly separable problem) using a simple, single layer perceptron. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdca7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR gate data\n",
    "x = np.array([[1,0],\n",
    "              [0,1],\n",
    "              [0,0],\n",
    "              [1,1]])\n",
    "y   = np.array([1,1,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffbe6ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb029c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded3b667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50112a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97537af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward propagation\n",
    "\n",
    "def forward_prop(row) :\n",
    "    y_hat = np.dot(x[row],w) + b\n",
    "    if y_hat > 0 :\n",
    "        return 1\n",
    "    else :\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4484ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3791161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcc747b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b585eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# back propagation\n",
    "\n",
    "def backward_prop(y_hat, row) :\n",
    "    global b,w\n",
    "    w[0]  = w[0] + alpha * (y[row] - y_hat) * x[row][0]\n",
    "    w[1]  = w[1] + alpha * (y[row] - y_hat) * x[row][1]\n",
    "    b     = b + alpha * (y[row] - y_hat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7816368c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a89efb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ac2b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4e0503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights and bias initialization\n",
    "\n",
    "w = np.random.normal(size=2)\n",
    "b = np.random.normal()\n",
    " \n",
    "# learning rate. This is exactly the same term that we have already learnt in gradient descent.\n",
    "alpha = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbad82b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the predicted y_hat, for the test data set.\n",
    "def predict(x) :\n",
    "    y = []\n",
    "     \n",
    "    # the user could be sending multiple rows. compute y_hat for each of the rows in the test dataset.\n",
    "    for row in x :\n",
    "         \n",
    "        # weighted sum\n",
    "        y_pred = np.dot(row,w) + b\n",
    "         \n",
    "        # run the weighted sum throught he activation function.\n",
    "        if y_pred > 0 :\n",
    "            y_pred = 1\n",
    "        else :\n",
    "            y_pred = 0\n",
    "             \n",
    "        # append the predicted y (y_hat)to an array\n",
    "        y.append(y_pred)\n",
    "         \n",
    "    # return the predicted array of y_hat values for the corresponding test data (x)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cef5f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4473b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f477791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d2f71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs\n",
    "for epoch in range(1000) :\n",
    "     \n",
    "    # for each row in x (cycle through the dataset)\n",
    "    for row in range(x.shape[0]) :\n",
    "         \n",
    "        # for each row in x, predict y_hat\n",
    "        y_hat = forward_prop(row)\n",
    " \n",
    "        # for each row calculate weights\n",
    "        backward_prop(y_hat,row)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626604b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a871ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf4a6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dd5710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83621b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53287bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df252bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95333fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f84d9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde0733e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92e6089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef9d46a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9039a648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abd73f1d",
   "metadata": {},
   "source": [
    "<h2 align=center style=\"color:green;\">Fundamentals of Tensor-Flow</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675666c6",
   "metadata": {},
   "source": [
    "- **What is TensorFlow?**\n",
    "\n",
    "\t- Open-source library for graph-based numerical computation\n",
    "\t- Developed by the Google Brain Team\n",
    "    \n",
    "    \n",
    "- **Low and high level APIs**\n",
    "\t- Addition, multiplication, differentiation\n",
    "\t- Machine learning models\n",
    "\n",
    "- **Important changes in TensorFlow 2.0**\n",
    "\t- Eager execution by default\n",
    "\t- Model building with Keras and Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11b637c",
   "metadata": {},
   "source": [
    "**What is a tensor?**\n",
    "- Generalization of vectors and matrices\n",
    "- Collection of numbers\n",
    "- Specific shape\n",
    "\n",
    "<img src=\"images/tensors.jpg\" height=600px width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a298ba",
   "metadata": {},
   "source": [
    "### Defining tensors in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6241a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 0D Tensor\n",
    "d0 = tf.ones((1,))\n",
    "d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3792c2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D Tensor\n",
    "d1 = tf.ones((2,))\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce692704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Tensor\n",
    "d2 = tf.ones((2,2))\n",
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794cf1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D Tensor\n",
    "d3 = tf.ones((2,2,2))\n",
    "d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1eac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d3.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515fff59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76320c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b781ec2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0c07926",
   "metadata": {},
   "source": [
    "### Defining constants in TensorFlow\n",
    "\n",
    "- A constant is the simplest category of tensor\n",
    "\t- Not trainable\n",
    "\t- Can have any dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb10ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import constant\n",
    "\n",
    "# Define a 2x3 constant.\n",
    "a = constant(3, shape=[2, 3])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75e67aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a 2x2 constant.\n",
    "b = constant([1, 2, 3, 4], shape=[2, 2])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89599f8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0482f57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "128bc69e",
   "metadata": {},
   "source": [
    "### Defining and initializing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1afba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a variable\n",
    "a0 = tf.Variable([1, 2, 3, 4, 5, 6], dtype=tf.float32)\n",
    "a1 = tf.Variable([1, 2, 3, 4, 5, 6], dtype=tf.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79eb343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a constant\n",
    "b = tf.constant(2, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a319c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute their product\n",
    "c0 = tf.multiply(a0, b)\n",
    "c1 = a0*b\n",
    "c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7d1e09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641cc460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8e7d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab816cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527da54b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea3662dd",
   "metadata": {},
   "source": [
    "<h2 align=center style=\"color:green;\">Fundamentals of Keras</h2>\n",
    "\n",
    "<br/>\n",
    "Building a neural net from scratch typically involves defining\n",
    "\n",
    "- Layers\n",
    "- Linking the layers\n",
    "- loss function\n",
    "- weight adjustments etc\n",
    "\n",
    "\n",
    "Defining these manually is very time consuming and daunting for newbies. What is needed is an abstract layer above Tensorflow, that makes building neural nets much quicker and easier.\n",
    "\n",
    "\n",
    "\n",
    "> **Keras is the answer. Keras is a high level Python based API that can be used to build neural nets by leveraging Tensorflow. By the way, Tensorflow is not the only deep learning package out there. Here is a quick visual that shows us where Keras and Tensorflow stand in the hierarchy.**\n",
    "\n",
    "\n",
    "<img src=\"images/image4.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211fc2cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb80fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3587322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2d0bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932be8f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acb5bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99123a44",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center\">Iris Classification using Tensorflow</h2> \n",
    "\n",
    "\n",
    "<!-- <p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">\n",
    "Perceptron  is the most fundamental type of element in a neural network. \n",
    " \n",
    "</div></p> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33001cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Tensorflow and keras library\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9836e199",
   "metadata": {},
   "source": [
    "Building a neural net from scratch typically involves defining\n",
    "\n",
    "- Layers\n",
    "- Linking the layers\n",
    "- loss function\n",
    "- weight adjustments etc\n",
    "\n",
    "\n",
    "Defining these manually is very time consuming and daunting for newbies. What is needed is an abstract layer above Tensorflow, that makes building neural nets much quicker and easier.\n",
    "\n",
    "\n",
    "\n",
    "**Keras is the answer. Keras is a high level Python based API that can be used to build neural nets by leveraging Tensorflow. By the way, Tensorflow is not the only deep learning package out there. Here is a quick visual that shows us where Keras and Tensorflow stand in the hierarchy.**\n",
    "\n",
    "\n",
    "<img src=\"images/image4.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f501a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load iris dataset\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    " \n",
    "# preview the iris data\n",
    "print ( iris.data[0:5,:]  ) # data\n",
    "print ( iris.target[0:5]  ) # target species\n",
    " \n",
    "# train/test split @ 20% test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data , iris.target, test_size=0.2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7014af42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06285a86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4803cbb5",
   "metadata": {},
   "source": [
    "### Step 1 – What type of neural network are we building ?\n",
    "\n",
    "**model = keras.Sequential()**\n",
    "\n",
    "- There are two types of Neural networks that can be build in Keras\n",
    "\n",
    "\t- **Sequential**\n",
    "    - **Functional**\n",
    "\n",
    "This classification is related to the structure of the Neural Network. However, most of the time we will be using Sequential model. It can solve most of the problems. In a sequential neural net, neurons are arranged in layers and in sequence . The firing and wiring happen in sequence, hence the name. \n",
    "\n",
    "\n",
    "<img src=\"images/image5.png\" height=900px width=900px>\n",
    "\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa5303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load Sequential model\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "model = keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89db2ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4ffa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daf868a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bd1728",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8847aa8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8f6688",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4be4e56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8790744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3453300a",
   "metadata": {},
   "source": [
    "\n",
    "### Step 2 – How are the neurons connected ?\n",
    "\n",
    "- We are building a **Dense neural network.**\n",
    "\n",
    "```\n",
    "model.add(keras.layers.Dense(4,input_shape=(4,)))\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">\n",
    "A Dense neural network is one in which each neuron is connected to all other neurons in the previous and next layers. \n",
    "</div></p>\n",
    "\n",
    "<img src=\"images/image6.png\" height=750px width= 750px>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Also, with this statement, we are just building the input layer. An input layer with 4 nodes, one node for each of the inputs. Naturally, the assumption at this point would be that there would be as many nodes in the input layer as the number of inputs. So, why specify the input_shape parameter again ? In later examples we will see that the input data shape need not always match with the input nodes. We specify the input_shape parameter as a tuple. In this case the input is a 1-d vector. \n",
    "\n",
    "> **Note:** The parameter input_shape is only used when creating the first layer. The next set of steps (hidden layer and output layer) do not need this parameter.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f9da46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add input layer of 4 neurons\n",
    "\n",
    "model.add(keras.layers.Dense(4, input_shape=(4,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568263ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6075af8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8846b4a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012b4526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d68877b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3c211a6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Step 3 – Hidden Layers\n",
    "\n",
    "This is where the magic happens. Let’s try it with just one hidden layer.\n",
    "\n",
    "```\n",
    "model.add(keras.layers.Dense(8,activation=\"relu\"))\n",
    "\n",
    "```\n",
    "\n",
    "<img src=\"images/image7.png\" height=700px width= 700px>\n",
    "\n",
    "\n",
    "Irrespective of the layer (input, hidden or output), the way to add layers is using the add function. That should make things easy for us. The new parameter that we see in the hidden layer is the `activation` parameter.\n",
    "\n",
    "<img src=\"images/image8.png\" height=700px width= 700px>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7275aadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add one hidden layer of 8 neurons \n",
    "\n",
    "model.add(keras.layers.Dense(8, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c439f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ec813a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eb86f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717842df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "645bdba9",
   "metadata": {},
   "source": [
    "\n",
    "### Step 4 – Output Layer\n",
    "\n",
    "- After the hidden layer is added, we add the output layer. Since we are doing a multi-class classification, the preferred activation function is called as a softmax – more on this later. A softmax activation function gives out multiple probability values and the one with the highest probability is the predicted output.\n",
    "\n",
    "```\n",
    "model.add(keras.layers.Dense(3,activation=\"softmax\"))\n",
    "```\n",
    "\n",
    "<img src=\"images/image9.png\" height=800px width= 800px>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcf03a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b0e4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4c81b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee974afc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10207b07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff65218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b72b293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ad1484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ac96a82",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Step 5 – Compile the model\n",
    "\n",
    "- We have created the structure of the neural net – layer by layer. At each step, we have defined the number of nodes and the activation function to be used. Once we have completed it, we now have to compile the model.\n",
    "\n",
    "\n",
    "```\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "- We have just defined how the neural net should look like. With the **compile ( )** method, Keras translates the parameters you have specified into an optimized series of steps that can then be executed on the computer. Without the compile step, you cannot fit (train) the model. We will see how we can use metrics in a bit, but optimizer and loss parameters requrie quite a bit of explanation.\n",
    "\n",
    "- Typically, Machine Learning algorithm requires some kind of a loss function to be minimized. Gradient Descent is a commonly used loss function. For classification problems, a common loss function is **Cross Entropy**. Cross Entropy is also called as Log Loss. Mathematically, this is a how a cross entropy function can be defined for 2 classes.\n",
    "\n",
    "\n",
    "<img src=\"images/image10.webp\" height=900px width= 900px>\n",
    "\n",
    "\n",
    "If the model has predicted the species to be setosa with a probability of `0.2`, the loss function can be calculated as follows.\n",
    "\n",
    "$$\\mathbf{\\text{L_CrossEntropy} = -(0log 0.2 + (1 - 0)log(1 - 0.2) )  = - (log 0.8) = -(- 0.096) = 0.096  }$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5091079",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9182573b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ce1f93a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8300ab5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d83e63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5c670b0",
   "metadata": {},
   "source": [
    "### Graphical Representation of Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a00571",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.array([0.0001,0.001,0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99])\n",
    " \n",
    "# when y = 1, what is the loss function ?\n",
    "y = 1\n",
    "\n",
    "l = - (y * np.log10(p) + (1-y) * np.log10(1-p)  )\n",
    " \n",
    "# now plot it to see how the loss function decreases as the predicted value approaches the actual value (of y = 1)\n",
    " \n",
    "plt.scatter(p,l)\n",
    "plt.xlabel(\"Different predicted values when the actual value is 1\")\n",
    "plt.ylabel(\"Loss function\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37085d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90c2b477",
   "metadata": {},
   "source": [
    "> What this plot means is that the more the predicted value deviates from the actual value, the more the loss function is. For example, when the predicted value reaches close to the actual value (of 1 in this case), the loss function gets closer and closer to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fb35c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f58be66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7377b97c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a7489d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "746ace0c",
   "metadata": {},
   "source": [
    "### Step 6 – Fit the model with training data\n",
    "\n",
    "- This is where we train the dataset. The word `epoch` represents one complete iteration over the training dataset. With each epoc (one pass over the entire dataset) the weights are adjusted and the accuracy slowly increases. Since we have accuracy as a metric in step 5, it is shown at each of the training epoch. That way we see how the accuracy increases with each epoch.\n",
    "\n",
    "```\n",
    "model.fit(X_train, y_train, epochs=100)\n",
    "```\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d9e4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaa8a00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705f0359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba60b7de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c1055f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5403384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6b0e233",
   "metadata": {},
   "source": [
    "\n",
    "### Step 7 – Predict data\n",
    "\n",
    "- Once the model is trained, we can start predicting your test data. \n",
    "\n",
    "```\n",
    "model.predict(testData)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"images/image12.png\" height=800px width=800px>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bf6170",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56fcb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458de175",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_class = np.argmax(predictions, axis=1)\n",
    "predictions_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4898e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41391f21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47f6549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8321bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a900a8d",
   "metadata": {},
   "source": [
    "### Step 8 – Evaluate Model\n",
    "\n",
    "Since the output is categorical data, a quick `confusion matrix` will show use how far we are from the model.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc0c0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "  \n",
    "cm = confusion_matrix(y_test, predictions_class)\n",
    "sns.heatmap(cm , annot=True);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfecd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "  \n",
    "accuracy_score(y_test,predictions_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914583f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89331b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73299fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be1a599d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Step 9 – Optimize Model\n",
    "\n",
    "- There are a couple of ways to optimize for higher accuracy. One way is to increase the nodes in the hidden layer. \n",
    "\n",
    "\n",
    "    \n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">    Choosing the size and complexity of a neural network (like the numbner of nodes and the number of hidden layers) is more art than science. \n",
    "</div></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec95f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(4,input_shape=(4,)))\n",
    "model.add(keras.layers.Dense(8,activation=\"relu\"))\n",
    "\n",
    "# BEGIN Change - add one more hidden layer\n",
    "model.add(keras.layers.Dense(8,activation=\"relu\"))\n",
    "# END Change\n",
    "\n",
    "\n",
    "model.add(keras.layers.Dense(3,activation=\"softmax\"))\n",
    " \n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "model.fit(X_train, y_train, epochs=100)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7f3545",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "predictions_class = np.argmax(predictions,axis=1)\n",
    " \n",
    "from sklearn.metrics import accuracy_score\n",
    "  \n",
    "accuracy_score(y_test,predictions_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068fbbad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753361da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74a52c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949ec8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3461fc65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02180df5",
   "metadata": {},
   "source": [
    "### whole code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe9150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model creation\n",
    "#model = keras.Sequential()\n",
    "\n",
    "# input layer\n",
    "#model.add(keras.layers.Dense(4,input_shape=(4,)))\n",
    "\n",
    "# hidden layers\n",
    "#model.add(keras.layers.Dense(8,activation=\"relu\"))\n",
    "#model.add(keras.layers.Dense(8,activation=\"relu\"))\n",
    "\n",
    "\n",
    "# output layer\n",
    "#model.add(keras.layers.Dense(3,activation=\"softmax\"))\n",
    "\n",
    "\n",
    "# model compilation\n",
    "#model.compile(optimizer='adam',\n",
    "#              loss='sparse_categorical_crossentropy',\n",
    "#              metrics=['accuracy'])\n",
    " \n",
    "    \n",
    "#model.fit(X_train, y_train, epochs=100)\n",
    " \n",
    "    \n",
    "#model prediction\n",
    "\n",
    "#y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e4cb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0623dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model summary\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63f872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred_class = np.argmax(y_pred,axis=1)\n",
    "#y_pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc016a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7368251b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1da0d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e50795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b259e937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360d56e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c129b7c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65068fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eaef93d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7667ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eb10e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09191fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563d418c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723bac92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ab55eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445a434d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7519d0c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c022f759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd6a8f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ac649d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cec04fe",
   "metadata": {},
   "source": [
    "<!-- How to improve performance of Neural Networks -- >\n",
    "\n",
    "<! -- https://d4datascience.com/2016/09/29/fbf/comment-page-1/ -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51166adb",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red\" align=center>How we can improve performance of a Artificial Neural Network</h3>\n",
    "\n",
    "\n",
    "<br>\n",
    "<img src=\"images/Hyperparameters.png\" height=400px width=600px align=right>\n",
    "<br>\n",
    "\n",
    "It simply means that how we can decide:\n",
    "\n",
    "- **Number of hidden layers**\n",
    "- **Number of neurons per layer**\n",
    "- **Activation function**\n",
    "- **Learning rate**\n",
    "- **EarlyStopping**\n",
    "- **Batch Size**\n",
    "- **Dropout Rate**\n",
    "- **Regularization**\n",
    "- **Momentum**\n",
    "- **Weight Initialization**\n",
    "- **Optimization Algorithms**\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "> **Hyperparameter tuning** consists of finding a set of optimal hyperparameter values for a learning algorithm while applying this optimized algorithm to any data set. That combination of hyperparameters maximizes the model's performance, minimizing a predefined loss function to produce better results with fewer errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3db3ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2fa96c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69045f15",
   "metadata": {},
   "source": [
    "### Number of hidden layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e303712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0e52f8b",
   "metadata": {},
   "source": [
    "### Number of neurons per layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbe5a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5924454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d9f822a",
   "metadata": {},
   "source": [
    "### Activation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbd79b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aa0150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98b9bfd6",
   "metadata": {},
   "source": [
    "### Learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcd8918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d6c6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cab7386a",
   "metadata": {},
   "source": [
    "### EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508084d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53a643c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000221c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37739bd5",
   "metadata": {},
   "source": [
    "### Batch Size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b14cd75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f49f806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48741a63",
   "metadata": {},
   "source": [
    "### Dropout Rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7477f21e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3fd500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db29e559",
   "metadata": {},
   "source": [
    "### Regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f62a44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19412a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08751d1e",
   "metadata": {},
   "source": [
    "### Momentum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbe38fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6096ea2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e11f2b51",
   "metadata": {},
   "source": [
    "### Weight Initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446e25e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f730244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e5fdb2f",
   "metadata": {},
   "source": [
    "### Optimization Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b2b0d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a59f252",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ab8e71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7606534b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a94c5c3",
   "metadata": {},
   "source": [
    "### RandomizedSearchCV vs GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af376f02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05409131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9168ebd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c398bd8",
   "metadata": {},
   "source": [
    "### Sklearn Recap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379550d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4272373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0137169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f12669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e605defb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8cb5a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c841ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23da2293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bab46232",
   "metadata": {},
   "source": [
    "# 4. Deep Learning Models & Neural Networks\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\"><strong>Deep learning models and neural networks are powerful tools that are revolutionizing many fields, from computer vision to natural language processing such as healthcare, finance, and entertainment to extract insights and make predictions from large datasets. These models are capable of learning from examples, improving their performance over time, and have shown great potential in solving complex problems.</strong></div></p>\n",
    "\n",
    "<img src=\"images/picture1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e56b8a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e42bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20f2b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a017e475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8e259b8",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center\">a. Deep Learning Models: Supervised Learning</h2> \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">\n",
    "<b>Supervised learning</b> is a type of machine learning in which an algorithm learns from labeled data to make predictions or decisions about unseen data. In deep learning, supervised learning involves using a large amount of labeled data to train a model to accurately predict a target variable based on input variables.   \n",
    "</div></p>\n",
    "\n",
    "<br>\n",
    "<img src=\"images/Supervised-learning.png\" height=500px width=400px align=left>\n",
    "<br>\n",
    "\n",
    "<img src=\"images/picture2.png\" height=500px width=500px align=right>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795e2606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ffd620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8f8a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a32642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d227815e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "408b2612",
   "metadata": {},
   "source": [
    "\n",
    "<h2 style=\"text-align:center\">b. Deep Learning Models: Un-Supervised Learning</h2> \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">\n",
    "In deep learning, <b>Un-Supervised learning</b> involves training a neural network on a dataset without any explicit labeling or categorization of the data. The goal is to enable the network to learn and discover underlying patterns or structure in the data without being explicitly told what to look for.</div></p>\n",
    "\n",
    "<br>\n",
    "<img src=\"images/unsupervised-learning.png\" height=500px width=400px align=left>\n",
    "<br>\n",
    "\n",
    "<img src=\"images/picture3.png\" height=500px width=500px align=right>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85e350d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68843ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61440312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a22755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d77358a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30eb12ea",
   "metadata": {},
   "source": [
    "\n",
    "<h2 style=\"text-align:center\">c. Deep Learning Models: Reinforcement Learning</h2> \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">\n",
    "In deep learning, <b>Reinforcement learning</b> involves training an agent to learn how to interact with an environment in order to maximize a reward signal. The agent takes actions in the environment and receives feedback in the form of rewards or penalties, allowing it to learn the optimal policy for achieving its goal. </div></p>\n",
    "<br>\n",
    "<img src=\"images/Reinforcement.png\" height=600px width=600px >\n",
    "<br>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e8ff64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540bd80f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd7e4de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2a8d12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "628fe8c0",
   "metadata": {},
   "source": [
    "\n",
    "<h2 style=\"text-align:center\">d. Deep Learning Models: Hybrid/Semi-Supervised Learning</h2> \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\">\n",
    "In Deep Learning, <b>Hybrid/semi-supervised learning</b> combines both labeled and unlabeled data to train a deep learning model. The labeled data is used to train the model in a supervised manner, while the unlabeled data is used to assist the learning process by providing additional information to improve model performance. This approach is especially useful when obtaining large amounts of labeled data is difficult or expensive.</div></p>\n",
    "<br>\n",
    "<img src=\"images/Semi.png\" height=600px width=600px >\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf43bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421054c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8824e2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e55c741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810ed24a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416a7765",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aa3baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12dea05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1975423b",
   "metadata": {},
   "source": [
    "# 5. How to do Deep Learning?\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\"><div class=\"alert alert-success\" style=\"margin: 20px\"><strong>Deep learning is a subset of machine learning that involves the use of artificial neural networks to analyze and learn from large amounts of data. The process involves feeding large amounts of data into a neural network, which is then trained to identify patterns and relationships within the data. These patterns are then used to make predictions or classify new data. The network learns by adjusting the weights of its connections between neurons, based on how well it performs on the task at hand, until it achieves the desired level of accuracy.</strong></div></p>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"images/picture4.gif\" align=left height=500px width=500px>\n",
    "<img src=\"images/picture5.gif\" align=right height=500px width=450px>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeaf81d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3253acf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ded983f",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align:center\">a. Deep Learning: Supervised Learning</h2> \n",
    "\n",
    "<img src=\"images/S1.png\" height=750px width=750px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8a6fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19249e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a8ddbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d3c249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d1366b8",
   "metadata": {},
   "source": [
    "### MultiLayer Perceptron or Single hidden Layer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbc4f96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be65e58b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44738518",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59535fa2",
   "metadata": {},
   "source": [
    "### Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd01933c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454228c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62805ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b63a4e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a157a298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96de71de",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47c16da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff392d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d337d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e829bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d216ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02099aa6",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18276f51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a816b13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906a419c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b79cb50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664f44dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb29826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35664b97",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82bb4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7992692d",
   "metadata": {},
   "source": [
    "### Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17928a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7adf3b60",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a9b037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4865ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f98456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1925d85f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5725528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0e4a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1e7de6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc971857",
   "metadata": {},
   "source": [
    "## list of tools, techniques, and approaches in deep learning:\n",
    "\n",
    "- - Frameworks: TensorFlow, Keras, PyTorch, Caffe, Theano, MXNet\n",
    "- Neural network architectures: Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM), Autoencoders, Generative Adversarial Networks (GAN), Deep Belief Networks (DBN)\n",
    "- Regularization techniques: Dropout, L1 and L2 regularization, Early stopping, Batch normalization\n",
    "- Optimization techniques: Stochastic Gradient Descent (SGD), Adam, Adagrad, Adadelta, RMSprop\n",
    "- Preprocessing techniques: Data augmentation, Scaling, Normalization\n",
    "- Transfer learning: Fine-tuning pre-trained models on new tasks, feature extraction\n",
    "- Ensembling: Bagging, Boosting, Stacking\n",
    "- Hyperparameter tuning: Grid search, Random search, Bayesian optimization\n",
    "- Explainability: LIME, SHAP, Grad-CAM\n",
    "- Interpretability: Integrated Gradients, Layer-wise Relevance Propagation (LRP)\n",
    "- Reinforcement learning: Q-learning, Policy Gradient methods\n",
    "- GAN applications: Image-to-image translation, Text-to-image synthesis, Super-resolution\n",
    "- NLP techniques: Word embeddings, Attention Mechanisms, Transformers, BERT, GPT\n",
    "- Computer vision techniques: Object detection, Image segmentation, Face recognition, Pose estimation\n",
    "- Time-series analysis: LSTM, GRU, Time-series forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdbee79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f6f7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b94a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d105d40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148345dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d9b6fe8",
   "metadata": {},
   "source": [
    "# Deep Learning Project Life Cycle:\n",
    "<br>\n",
    "\n",
    "- **Problem Definition:** The first step is to define the problem that you want to solve using deep learning. It involves understanding the business problem, collecting data, defining the scope, and identifying the goals.\n",
    "<br>\n",
    "- **Data Collection and Preparation:** The next step is to collect data that is relevant to the problem you are trying to solve. This data may be in different formats, such as images, text, or audio. Once you have the data, you need to preprocess it to make it suitable for deep learning algorithms. This may include cleaning the data, converting it to a common format, and splitting it into training, validation, and testing sets.\n",
    "<br>\n",
    "\n",
    "- **Model Selection and Design:** In this step, you need to select an appropriate deep learning model that can solve the problem you defined in step one. You may also need to design your own model if none of the existing models fit your problem. You need to take into consideration factors such as the size of the dataset, the complexity of the problem, and the computational resources available.\n",
    "<br>\n",
    "\n",
    "- **Model Training:** Once you have selected the deep learning model, you need to train it using the training set. This involves feeding the model with input data and adjusting its parameters to minimize the error between the predicted and actual outputs. The training process may take several iterations, depending on the complexity of the model and the size of the dataset.\n",
    "<br>\n",
    "\n",
    "- **Model Evaluation:** After training the model, you need to evaluate its performance using the validation set. This involves measuring metrics such as accuracy, precision, recall, and F1 score. If the model's performance is not satisfactory, you may need to revisit steps three and four.\n",
    "<br>\n",
    "\n",
    "- **Model Deployment:** Once you are satisfied with the model's performance, you can deploy it to solve the problem in the real world. This involves integrating the model into the existing system and providing an interface for users to interact with it. You may also need to monitor the model's performance and retrain it periodically to keep it up to date.\n",
    "<br>\n",
    "\n",
    "- **Model Maintenance:** The final step is to maintain the model by updating it with new data and retraining it periodically to improve its performance. You may also need to troubleshoot issues that arise during the model's deployment and address them promptly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c48aa3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621b5964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2f10ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09ddf40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4f2b246",
   "metadata": {},
   "source": [
    "# Tools and Techniques\n",
    "\n",
    "\n",
    "#### Problem Definition:\n",
    "- Tools: Jupyter Notebooks, Google Colab, Anaconda\n",
    "- Techniques: Brainstorming, Research Papers, Industry Reports, Surveys\n",
    "\n",
    "#### Data Collection:\n",
    "- Tools: Web Scrapers, APIs, Data Warehouses, Open Data Sources\n",
    "- Techniques: Data Annotation, Data Cleaning, Data Pre-processing\n",
    "\n",
    "#### Data Preparation:\n",
    "- Tools: Numpy, Pandas, Matplotlib, Seaborn, Scikit-Learn\n",
    "- Techniques: Data Visualization, Exploratory Data Analysis, Feature Engineering, Data Augmentation, Data Normalization\n",
    "\n",
    "#### Model Selection:\n",
    "- Tools: Tensorflow, PyTorch, Keras, Scikit-Learn\n",
    "- Techniques: Convolutional Neural Networks, Recurrent Neural Networks, Transfer Learning, Autoencoders, Generative Adversarial Networks\n",
    "\n",
    "#### Model Training:\n",
    "- Tools: GPUs, TPUs, Cloud Computing, Distributed Computing\n",
    "- Techniques: Stochastic Gradient Descent, Batch Normalization, Dropout, Early Stopping, Hyperparameter Tuning\n",
    "\n",
    "#### Model Evaluation:\n",
    "- Tools: Scikit-Learn, Tensorboard, Keras, PyTorch\n",
    "- Techniques: Confusion Matrix, Precision, Recall, F1 Score, Receiver Operating Characteristic (ROC) Curve, Mean Squared Error (MSE)\n",
    "\n",
    "#### Model Deployment:\n",
    "- Tools: Flask, Django, AWS Lambda, AWS Sagemaker, Google Cloud Functions\n",
    "- Techniques: RESTful APIs, Docker, Kubernetes, Serverless Architecture, CI/CD Pipelines\n",
    "\n",
    "#### Model Monitoring and Maintenance:\n",
    "- Tools: Tensorboard, Prometheus, Grafana, ELK Stack, AWS CloudWatch\n",
    "- Techniques: Metrics Monitoring, Logging, Alerting, Troubleshooting, Upgrades and Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a762392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea2387d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439c1e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45961a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50322f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfffc69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a0f2ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d820e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df9bb65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5eff9faf",
   "metadata": {},
   "source": [
    "## Building Blocks of a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc205ef",
   "metadata": {},
   "source": [
    "## Practice Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c4a703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize bias1\n",
    "bias1 = Variable(1.0)\n",
    "\n",
    "# Initialize weights1 as 3x2 variable of ones\n",
    "weights1 = Variable(ones((3, 2)))\n",
    "\n",
    "# Perform matrix multiplication of borrower_features and weights1\n",
    "product1 = matmul(borrower_features, weights1)\n",
    "\n",
    "# Apply sigmoid activation function to product1 + bias1\n",
    "dense1 = keras.activations.sigmoid(product1 + bias1)\n",
    "\n",
    "# Print shape of dense1\n",
    "print(\"\\n dense1's output shape: {}\".format(dense1.shape))\n",
    "\n",
    "\n",
    "\n",
    "# From previous step\n",
    "bias1 = Variable(1.0)\n",
    "weights1 = Variable(ones((3, 2)))\n",
    "product1 = matmul(borrower_features, weights1)\n",
    "dense1 = keras.activations.sigmoid(product1 + bias1)\n",
    "\n",
    "# Initialize bias2 and weights2\n",
    "bias2 = Variable(1.0)\n",
    "weights2 = Variable(ones((2, 1)))\n",
    "\n",
    "# Perform matrix multiplication of dense1 and weights2\n",
    "product2 = matmul(dense1,weights2)\n",
    "\n",
    "# Apply activation to product2 + bias2 and print the prediction\n",
    "prediction = keras.activations.sigmoid(product2 + bias2)\n",
    "print('\\n prediction: {}'.format(prediction.numpy()[0,0]))\n",
    "print('\\n actual: 1')\n",
    "\n",
    "\n",
    "# Compute the product of borrower_features and weights1\n",
    "products1 = matmul(borrower_features, weights1)\n",
    "\n",
    "# Apply a sigmoid activation function to products1 + bias1\n",
    "dense1 = keras.activations.sigmoid(products1+bias1)\n",
    "\n",
    "# Print the shapes of borrower_features, weights1, bias1, and dense1\n",
    "print('\\n shape of borrower_features: ', borrower_features.shape)\n",
    "print('\\n shape of weights1: ', weights1.shape)\n",
    "print('\\n shape of bias1: ', bias1.shape)\n",
    "print('\\n shape of dense1: ', dense1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b7f35d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d59d19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the first dense layer\n",
    "dense1 = keras.layers.Dense(7, activation='sigmoid')(borrower_features)\n",
    "\n",
    "# Define a dense layer with 3 output nodes\n",
    "dense2 = keras.layers.Dense(3, activation='sigmoid')(dense1)\n",
    "\n",
    "# Define a dense layer with 1 output node\n",
    "predictions = keras.layers.Dense(1,activation='sigmoid')(dense2)\n",
    "\n",
    "# Print the shapes of dense1, dense2, and predictions\n",
    "print(\"\\n shape of input features: \", borrower_features.shape)\n",
    "print('\\n shape of dense1: ', dense1.shape)\n",
    "print('\\n shape of dense2: ', dense2.shape)\n",
    "print('\\n shape of predictions: ', predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c95712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2016de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20486d39",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "\n",
    "- Sigmoid(2-classes)\n",
    "- Relu(in hidden layers)\n",
    "- softmax(for >2 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b883c2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary classification problem\n",
    "\n",
    "# Construct input layer from features\n",
    "inputs = constant(bill_amounts, float32)\n",
    "\n",
    "# Define first dense layer\n",
    "dense1 = keras.layers.Dense(3, activation='relu')(inputs)\n",
    "\n",
    "# Define second dense layer\n",
    "dense2 = keras.layers.Dense(2, activation='relu')(dense1)\n",
    "\n",
    "# Define output layer\n",
    "outputs = keras.layers.Dense(1, activation='sigmoid')(dense2)\n",
    "\n",
    "# Print error for first five examples\n",
    "error = default[:5] - outputs.numpy()[:5]\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab5cf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-classification problems\n",
    "\n",
    "# Construct input layer from borrower features\n",
    "inputs = constant(borrower_features, float32)\n",
    "\n",
    "# Define first dense layer\n",
    "dense1 = keras.layers.Dense(10, activation='sigmoid')(inputs)\n",
    "\n",
    "# Define second dense layer\n",
    "dense2 = keras.layers.Dense(8, activation='relu')(dense1)\n",
    "\n",
    "# Define output layer\n",
    "outputs = keras.layers.Dense(6, activation='softmax')(dense2)\n",
    "\n",
    "# Print first five predictions\n",
    "print(outputs.numpy()[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864bfdbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9bfce30",
   "metadata": {},
   "source": [
    "### The Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1410678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The local minima\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Initialize x_1 and x_2\n",
    "x_1 = Variable(6.0,tf.float32)\n",
    "x_2 = Variable(0.3,tf.float32)\n",
    "\n",
    "# Define the optimization operation\n",
    "opt = keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "for j in range(100):\n",
    "\t# Perform minimization using the loss function and x_1\n",
    "\topt.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
    "\t# Perform minimization using the loss function and x_2\n",
    "\topt.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
    "\n",
    "# Print x_1 and x_2 as numpy arrays\n",
    "print(x_1.numpy(), x_2.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9112ac19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d19b3bc2",
   "metadata": {},
   "source": [
    "### Avoiding local minima\n",
    "The previous problem showed how easy it is to get stuck in local minima. We had a simple optimization problem in one variable and gradient descent still failed to deliver the global minimum when we had to travel through local minima first. One way to avoid this problem is to use momentum, which allows the optimizer to break through local minima. We will again use the loss function from the previous problem, which has been defined and is available for you as loss_function().\n",
    "\n",
    "<img src=\"images/local_minima.png\">\n",
    "\n",
    "Several optimizers in tensorflow have a momentum parameter, including SGD and RMSprop. You will make use of RMSprop in this exercise. Note that x_1 and x_2 have been initialized to the same value this time. Furthermore, keras.optimizers.RMSprop() has also been imported for you from tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb765afe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb798d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize x_1 and x_2\n",
    "x_1 = Variable(0.05,float32)\n",
    "x_2 = Variable(0.05,float32)\n",
    "\n",
    "# Define the optimization operation for opt_1 and opt_2\n",
    "opt_1 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.99)\n",
    "opt_2 = keras.optimizers.RMSprop(learning_rate=0.01, momentum=0.00)\n",
    "\n",
    "for j in range(100):\n",
    "\topt_1.minimize(lambda: loss_function(x_1), var_list=[x_1])\n",
    "    # Define the minimization operation for opt_2\n",
    "\topt_2.minimize(lambda: loss_function(x_2), var_list=[x_2])\n",
    "\n",
    "# Print x_1 and x_2 as numpy arrays\n",
    "print(x_1.numpy(), x_2.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731c303f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77c5368b",
   "metadata": {},
   "source": [
    "### Initilization in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50de8f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the layer 1 weights\n",
    "w1 = Variable(tf.random.normal([23,7]))\n",
    "\n",
    "# Initialize the layer 1 bias\n",
    "b1 = Variable(ones([7]))\n",
    "\n",
    "# Define the layer 2 weights\n",
    "w2 = Variable(tf.random.normal([7,1]))\n",
    "\n",
    "# Define the layer 2 bias\n",
    "b2 = Variable(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c43399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d98ff9ac",
   "metadata": {},
   "source": [
    "### Defining  model and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c89a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "def model(w1, b1, w2, b2, features = borrower_features):\n",
    "\t# Apply relu activation functions to layer 1\n",
    "\tlayer1 = keras.activations.relu(matmul(features, w1) + b1)\n",
    "    # Apply dropout rate of 0.25\n",
    "\tdropout = keras.layers.Dropout(0.25)(layer1)\n",
    "\treturn keras.activations.sigmoid(matmul(dropout, w2) + b2)\n",
    "\n",
    "# Define the loss function\n",
    "def loss_function(w1, b1, w2, b2, features = borrower_features, targets = default):\n",
    "\tpredictions = model(w1, b1, w2, b2)\n",
    "\t# Pass targets and predictions to the cross entropy loss\n",
    "\treturn keras.losses.binary_crossentropy(targets, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952f84a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "for j in range(100):\n",
    "    # Complete the optimizer\n",
    "\topt.minimize(lambda: loss_function(w1, b1, w2, b2), \n",
    "                 var_list=[w1, b1, w2, b2])\n",
    "\n",
    "# Make predictions with model using test features\n",
    "model_predictions = model(w1, b1, w2, b2, test_features)\n",
    "\n",
    "# Construct the confusion matrix\n",
    "confusion_matrix(test_targets, model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45233f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e084c844",
   "metadata": {},
   "source": [
    "### The Sequential Model in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2838b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Keras sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Define the first dense layer\n",
    "model.add(keras.layers.Dense(16, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# Define the second dense layer\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Print the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c27f82",
   "metadata": {},
   "source": [
    "### Compiling a Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dbdeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the first dense layer\n",
    "model.add(keras.layers.Dense(16, activation='sigmoid', input_shape=(784,)))\n",
    "\n",
    "# Apply dropout to the first layer's output\n",
    "model.add(keras.layers.Dropout(0.25))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile('adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Print a model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcd77d1",
   "metadata": {},
   "source": [
    "### Defining a multiple input model\n",
    "In some cases, the sequential API will not be sufficiently flexible to accommodate your desired model architecture and you will need to use the functional API instead. If, for instance, you want to train two models with different architectures jointly, you will need to use the functional API to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d57be72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For model 1, pass the input layer to layer 1 and layer 1 to layer 2\n",
    "m1_layer1 = keras.layers.Dense(12, activation='sigmoid')(m1_inputs)\n",
    "m1_layer2 = keras.layers.Dense(4, activation='softmax')(m1_layer1)\n",
    "\n",
    "# For model 2, pass the input layer to layer 1 and layer 1 to layer 2\n",
    "m2_layer1 = keras.layers.Dense(12, activation='relu')(m2_inputs)\n",
    "m2_layer2 = keras.layers.Dense(4, activation='softmax')(m2_layer1)\n",
    "\n",
    "# Merge model outputs and define a functional model\n",
    "merged = keras.layers.add([m1_layer2, m2_layer2])\n",
    "model = keras.Model(inputs=[m1_inputs, m2_inputs], outputs=merged)\n",
    "\n",
    "# Print a model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d6f324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training with keras\n",
    "\n",
    "# Define a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Define a hidden layer\n",
    "model.add(keras.layers.Dense(16, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(keras.layers.Dense(4, activation='relu'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile('SGD', loss='categorical_crossentropy')\n",
    "\n",
    "# Complete the fitting operation\n",
    "model.fit(sign_language_features, sign_language_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cdc58c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e8dad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Metrics and validation with Keras\n",
    "\n",
    "# Define sequential model\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Define the first layer\n",
    "model.add(keras.layers.Dense(32, activation='sigmoid', input_shape=(784,)))\n",
    "\n",
    "# Add activation function to classifier\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Set the optimizer, loss function, and metrics\n",
    "model.compile(optimizer='RMSProp', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Add the number of epochs and the validation split\n",
    "model.fit(sign_language_features, sign_language_labels, epochs=10, validation_split=0.10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6325bf0",
   "metadata": {},
   "source": [
    "### Evaluating models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204fa945",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "767d2c5f",
   "metadata": {},
   "source": [
    "### Preparing to train with Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa479a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns for bedrooms and bathrooms\n",
    "bedrooms = feature_column.numeric_column(\"bedrooms\")\n",
    "bathrooms = feature_column.numeric_column(\"bathrooms\")\n",
    "\n",
    "# Define the list of feature columns\n",
    "feature_list = [bedrooms, bathrooms]\n",
    "\n",
    "def input_fn():\n",
    "\t# Define the labels\n",
    "\tlabels = np.array(housing[\"price\"])\n",
    "\t# Define the features\n",
    "\tfeatures = {'bedrooms':np.array(housing['bedrooms']), \n",
    "                'bathrooms':np.array(housing['bathrooms'])}\n",
    "\treturn features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0481126",
   "metadata": {},
   "source": [
    "### Defining the Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137e48eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model and set the number of steps\n",
    "model = estimator.DNNRegressor(feature_columns=feature_list, hidden_units=[2,2])\n",
    "model.train(input_fn, steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1f2902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(4,input_shape=(4,)))\n",
    "# BEGIN change - increase the number of nodes from 8 to 20\n",
    "model.add(keras.layers.Dense(20,activation=\"relu\"))\n",
    "# END change\n",
    "model.add(keras.layers.Dense(3,activation=\"softmax\"))\n",
    " \n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "model.fit(X_train, y_train, epochs=100)\n",
    " \n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_class = np.argmax(y_pred,axis=1)\n",
    " \n",
    "from sklearn.metrics import accuracy_score\n",
    "  \n",
    "accuracy_score(y_test,y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b6e680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e2ebb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(4,input_shape=(4,)))\n",
    "model.add(keras.layers.Dense(8,activation=\"relu\"))\n",
    "# BEGIN Change - add one more hidden layer\n",
    "model.add(keras.layers.Dense(8,activation=\"relu\"))\n",
    "# END Change\n",
    " \n",
    "model.add(keras.layers.Dense(3,activation=\"softmax\"))\n",
    " \n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "model.fit(X_train, y_train, epochs=100)\n",
    " \n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_class = np.argmax(y_pred,axis=1)\n",
    " \n",
    "from sklearn.metrics import accuracy_score\n",
    "  \n",
    "accuracy_score(y_test,y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2afc6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa2b7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a558d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bfbc49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7c895b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e187498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5722fd44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d71656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a5ebe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c71575d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f6994c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5f20d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6829c2c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "063874f1",
   "metadata": {},
   "source": [
    "## Introduction to TensorFlow and Keras Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2de761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining tensors in TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# 0D tensor\n",
    "\n",
    "d0 = tf.ones((1,))\n",
    "d0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d93509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1D tensor\n",
    "d1 = tf.ones((2,))\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9ba27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D tensor\n",
    "d2 = tf.ones((2,2))\n",
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4228f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the 2d tensor\n",
    "d2.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92306cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05c5b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D tensor\n",
    "d3 = tf.ones((2,2,2))\n",
    "d3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c633db50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50d992ee",
   "metadata": {},
   "source": [
    "### Defining Constants in Tensorflow\n",
    "\n",
    "A constant is a simplest category of tensor\n",
    "- Not Trainable\n",
    "- Can have any dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764af7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a 2x3 constant\n",
    "\n",
    "tf.constant(3, shape=(2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1639d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a 2x2 constant\n",
    "\n",
    "tf.constant([1,2,3,4], shape=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32374119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f4f8f20",
   "metadata": {},
   "source": [
    "### Defining and Initializing Variables in Tensorflow\n",
    "\n",
    "- A variable's value can be modified. This will be useful when we want to train a model by updating its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c75a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a variable\n",
    "\n",
    "# Here is 1D tensor of 6 elements having float16 bit type\n",
    "a0 = tf.Variable([1,2,3,4,5,6], dtype=tf.float32)\n",
    "a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff47203c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is 1D tensor of 6 elements having int6 bit type\n",
    "a0 = tf.Variable([1,2,3,4,5,6], dtype=tf.int16)\n",
    "a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1fa7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a constant\n",
    "b = tf.constant(2, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd7c832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute their product\n",
    "\n",
    "c0 = tf.multiply(a0, b)\n",
    "c0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac01e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = a0*b\n",
    "c1.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5633a6f4",
   "metadata": {},
   "source": [
    "### Applying the Addition Operator\n",
    "\n",
    "- The `add()` operation performs `element-wise addition` with two tensors.\n",
    "- Element-wise addition requires both tensors to have the same shape:\n",
    "    - Addition of two scalars is a scalar\n",
    "    - Addition of two vectors is a vector\n",
    "    - Addition of two matrics is a matrix.\n",
    "- The `add()` is overloaded, which means that we can also perform addition using the plus symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0880b204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import constant and add from tensorflow\n",
    "\n",
    "from tensorflow import constant, add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57bf37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining 0-dimensional tensors\n",
    "A0 = constant([1])\n",
    "B0 = constant([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0abff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining 1-dimensional tensors\n",
    "A1 = constant([1,2])\n",
    "B1 = constant([3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a49068c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining 2-dimensional tensors\n",
    "A2 = constant([[1,2],[3,4]])\n",
    "B2 = constant([[5,6],[7,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22129ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform tensor addition with add\n",
    "C0 = add(A0,B0)\n",
    "C1 = add(A1,B1)\n",
    "C2 = add(A2,B2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10afd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2.numpy(), B2.numpy(), C2.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac66ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c34cd478",
   "metadata": {},
   "source": [
    "### Applying the Multiplication Operator\n",
    "\n",
    "- **Element-wise multiplication** performed using `multiply()` operation\n",
    "    - The tensors must have the same shape\n",
    "    - E.g. [1,2,3] and [2,3,5]\n",
    "    \n",
    "- **Matrix Multiplication** performed with `matmul()` operator\n",
    "    - The `matmul(A,B)` operation multiplies A by B\n",
    "    - Number of columns of A must equal the number of rows of B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165dbbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import operators from tensorflow\n",
    "\n",
    "from tensorflow import ones, matmul, multiply\n",
    "\n",
    "# Defining tensors\n",
    "\n",
    "A0 = ones((1))\n",
    "A31 = ones([3,1])\n",
    "A34 = ones([3,4])\n",
    "A43 = ones([4,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c3b558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What type of operations are valid?\n",
    "\n",
    "multiply(A0, A0)\n",
    "multiply(A34, A34)\n",
    "multiply(A31, A31)\n",
    "multiply(A43, A43)\n",
    "\n",
    "\n",
    "matmul(A34, A43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa80330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What type of operations are invalid?\n",
    "\n",
    "# matmul(A31, A34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc07322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e5f7544",
   "metadata": {},
   "source": [
    "### Summing over tensor Dimensions\n",
    "\n",
    "- The `reduce_sum()` operator sums over the dimensions of a tensor\n",
    "    - `reduce_sum(A)` sums over all dimensions of A\n",
    "    - `reduce_sum(A,i)` sums over dimension i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cb217b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import operations from tensorflow\n",
    "\n",
    "from tensorflow import ones, reduce_sum\n",
    "\n",
    "# Define a 2x3x4 tensor of ones\n",
    "\n",
    "A = ones([2,3,4])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d264f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum over all the dimensions\n",
    "B = reduce_sum(A)\n",
    "\n",
    "# sum over dimension 0,1,2\n",
    "\n",
    "B0 = reduce_sum(A, 0)\n",
    "B1 = reduce_sum(A, 1)\n",
    "B2 = reduce_sum(A, 2)\n",
    "\n",
    "B, B0, B1, B2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a93814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2514b9d",
   "metadata": {},
   "source": [
    "### Advanced Operations\n",
    "\n",
    "- **gradient():** Compute the slope of a function at a point.\n",
    "    - Find optimum of a function(point where gradient=0)\n",
    "- **reshape():** Reshape a tensor (e.g. 10x10 to 100x1)\n",
    "- **random():** Populates tensor with entries drawn from a probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1501c41",
   "metadata": {},
   "source": [
    "#### Gradients in Tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160f8f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define x\n",
    "x = tf.Variable(-1.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aeae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define y within instance of GradientTape\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "#     y = x**2\n",
    "    y = tf.multiply(x,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dffe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the gradient of y at x=-1\n",
    "g = tape.gradient(y,x)\n",
    "g.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e4517e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69255196",
   "metadata": {},
   "source": [
    "#### Reshape a grayscale image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6158bc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate grayscale image\n",
    "gray= tf.random.uniform([2,2], maxval=255, dtype='int32')\n",
    "gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eef796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape grayscale image\n",
    "tf.reshape(gray, [2*2,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a13ac6",
   "metadata": {},
   "source": [
    "#### Reshape a color image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78602c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate color image\n",
    "color = tf.random.uniform([2, 2, 3], maxval=255, dtype='int32')\n",
    "\n",
    "# Reshape color image\n",
    "color = tf.reshape(color, [2*2, 3])\n",
    "color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3818fb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b040c4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import Variable\n",
    "scalar = Variable(1.0, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffceff4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "292bfad6",
   "metadata": {},
   "source": [
    "### Train a Linear Model In Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180062af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input column\n",
    "# output column\n",
    "# slope , intercept variable\n",
    "# loss_function\n",
    "# optimizer\n",
    "# Define a linear regression model\n",
    "# Define a linear regression model\n",
    "def linear_regression(intercept, slope, features = size_log):\n",
    "\treturn intercept + slope*features\n",
    "\n",
    "# Set loss_function() to take the variables as arguments\n",
    "def loss_function(intercept, slope, features = size_log, targets = price_log):\n",
    "\t# Set the predicted values\n",
    "\tpredictions = linear_regression(intercept, slope, features)\n",
    "    \n",
    "    # Return the mean squared error loss\n",
    "\treturn keras.losses.mse(targets, predictions)\n",
    "\n",
    "# Compute the loss for different slope and intercept values\n",
    "print(loss_function(0.1, 0.1).numpy())\n",
    "print(loss_function(0.1, 0.5).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63258462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an Adam optimizer with learning rate of 0.5\n",
    "opt = keras.optimizers.Adam(0.5)\n",
    "\n",
    "for j in range(100):\n",
    "\t# Apply minimize, pass the loss function, and supply the variables\n",
    "\topt.minimize(lambda: loss_function(intercept, slope), var_list=[intercept, slope])\n",
    "\n",
    "\t# Print every 10th value of the loss\n",
    "\tif j % 10 == 0:\n",
    "\t\tprint(loss_function(intercept, slope).numpy())\n",
    "\n",
    "# Plot data and regression line\n",
    "plot_results(intercept, slope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7272ea7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training a linear model into batches\n",
    "# Initialize Adam optimizer\n",
    "opt = keras.optimizers.Adam()\n",
    "\n",
    "# Load data in batches\n",
    "for batch in pd.read_csv('kc_house_data.csv', chunksize=100):\n",
    "\tsize_batch = np.array(batch['sqft_lot'], np.float32)\n",
    "\n",
    "\t# Extract the price values for the current batch\n",
    "\tprice_batch = np.array(batch['price'], np.float32)\n",
    "\n",
    "\t# Complete the loss, fill in the variable list, and minimize\n",
    "\topt.minimize(lambda: loss_function(intercept, slope, price_batch, size_batch), var_list=[intercept, slope])\n",
    "\n",
    "# Print trained parameters\n",
    "print(intercept.numpy(), slope.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0bff1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daa23fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96121825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397947c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d39586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e09fd75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02220b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde86e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1442214c",
   "metadata": {},
   "source": [
    "## Interview Questions\n",
    "- What is deep learning, and how is it different from traditional machine learning?\n",
    "- What is a neural network, and how does it work?\n",
    "- What are the applications of deep learning and neural networks?\n",
    "- What are the building blocks of a neural network?\n",
    "- What are neurons, and how do they function?\n",
    "- What is an activation function, and why is it important?\n",
    "- What are the different types of layers in a neural network?\n",
    "- What is forward propagation, and how does it work?\n",
    "- What are the different types of neural networks, and when are they used?\n",
    "- What is a Convolution Neural Network, and how does it work?\n",
    "- What is a Recurrent Neural Network, and how does it work?\n",
    "- What is TensorFlow, and how is it used in deep learning?\n",
    "- What is Keras, and how is it used in deep learning?\n",
    "- What are the advantages and disadvantages of using TensorFlow and Keras?\n",
    "- How do you build and train a simple neural network using Keras?\n",
    "- How do you load and prepare data for training a neural network?\n",
    "- What is a loss function, and how is it used in training a neural network?\n",
    "- What is an optimizer, and how is it used in training a neural network?\n",
    "- How do you evaluate the performance of a trained neural network?\n",
    "- How do you build a neural network for a basic classification problem?\n",
    "- What are some common problems that can arise when training a neural network?\n",
    "- How do you analyze the performance of a trained neural network?\n",
    "- What are some future directions for deep learning and neural networks?\n",
    "- What are some additional resources for learning about deep learning and neural networks?\n",
    "- Can you explain the concept of transfer learning and how it can be applied in deep learning for NLP?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79306e52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09a44ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf77730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db01619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f24ec14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243e92b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(lst):\n",
    "    for i in range(len(lst)):\n",
    "        lst[i] *= lst[i]\n",
    "    return lst\n",
    "lst = [2]\n",
    "func(lst)\n",
    "print(func(lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2560b30f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cdeb8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23c2646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914d9c57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
