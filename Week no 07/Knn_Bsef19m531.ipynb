{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b525179",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Nearest Neighbors Methods - The KNN Algorithm</h1>\n",
    "\n",
    "### KNN Algorithm\n",
    "- Basic Idea\n",
    "- Formal Definition\n",
    "- KNN Decision Boundary\n",
    "- A supervised, non-parametric algorithm\n",
    "- Used for classification and regression\n",
    "- An Instance-based learning algorithm\n",
    "- A lazy learning algorithm\n",
    "- Characteristics of kNN\n",
    "- Practical issues\n",
    "\n",
    "### Similarity/Distance Metrics \n",
    "- Constraints/Properties on Distance Metrics\n",
    "- Euclidean Distance\n",
    "- Manhatten Distance\n",
    "- Minkowski distance\n",
    "- Chebyshev Distance\n",
    "- Norm of a vector and Its Properties\n",
    "- Cosine Distance \n",
    "- Practical Issues in computing distance\n",
    "\n",
    "### The KNN algorithm and Implementation\n",
    "- KNN regression and classification with examples\n",
    "- Space and Time complexity\n",
    "- Choosing the value of K - The Theory.\n",
    "- Tuning the hyperparameter K - The method\n",
    "- KNN: The good, the bad and the ugly\n",
    "    \n",
    "### Algorithm Convergence\n",
    "- Error Convergence\n",
    "- Learning Problem\n",
    "- Bayes Optimal Classifier\n",
    "- 1-NN Error as n → ∞\n",
    "\n",
    "\n",
    "### KNN Enhancements\n",
    "- Parzen Windows and Kernels (Fast KNN) \n",
    "- Performance of KNN Algorithm\n",
    "- K-D Trees\n",
    "- Locality-sensitive Hashing\n",
    "- Inverted Lists\n",
    "\n",
    "### The Curse of Dimensionality\n",
    "- KNN Assumption\n",
    "- Demonstration\n",
    "- How does KNN work at all?\n",
    "\n",
    "### Dimensionality Reduction(Optional)\n",
    "- Why? and Benefits.\n",
    "- Difference between Feature Selection and Feature Extraction\n",
    "- Feature Selection methods\n",
    "- Feature Extraction\n",
    "- Principal Component Analysis\n",
    "    - Geometric Intuition\n",
    "    - Mathematical Formulation\n",
    "    - How do we choose K?\n",
    "    - Practical Consideration and Limitations\n",
    "\n",
    "### Model Evaluation Techniques\n",
    "- Classification Accuracy (0/1 Loss)\n",
    "- TP, TN, FP and FN\n",
    "- Confusion Matrix\n",
    "- Sensitivity, Specificity, Precision Trade-offs, ROC, AUC\n",
    "- F1-Score and Matthew’s Correlation Coefficient\n",
    "- Multi-class Classification, Evaluation, Micro, Macro Averaging\n",
    "\n",
    "**KNN: Python Implementation**    \n",
    "**KNN: Scikit-learn implementation**    \n",
    "**Interview Questions.**   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c2bd88",
   "metadata": {},
   "source": [
    "# KNN Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5065047",
   "metadata": {},
   "source": [
    "### Basic Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d205601",
   "metadata": {},
   "source": [
    "- K-Nearest Neighbour is one of the simplest Machine Learning algorithms based on Supervised Learning technique.\n",
    "- It assumes the similarity between the new data points and available data points and put the new data points into the category that is most similar to the available categories.Where'k' in KNN is a parameter that refers to the number of nearest neighbours to include in the majority of the voting process.\n",
    "- It stores all the available data and classifies a new data point based on the similarity. This means when new data appears then it can be easily classified into a well suite category by using K- NN algorithm.\n",
    "- **Example:** Suppose, we have an image of a creature that looks similar to cat and dog, but we want to know either it is a cat or dog. So for this identification, we can use the KNN algorithm, as it works on a similarity measure. Our KNN model will find the similar features of the new data set to the cats and dogs images and based on the most similar features it will put it in either cat or dog category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48a032d",
   "metadata": {},
   "source": [
    "### Formal Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7620e7",
   "metadata": {},
   "source": [
    "\"The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning algorithm, which uses proximity to make classifications or predictions about the grouping of an individual data point.\" (IBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4700e175",
   "metadata": {},
   "source": [
    "<img src=\"images/knn1.PNG\" height=600px width=600px align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ed2cb0",
   "metadata": {},
   "source": [
    "### How does basic K-NN work?\n",
    "The K-NN working can be explained on the basis of the below algorithm:\n",
    "\n",
    "**Step-1:** Select the number K of the neighbors\n",
    "\n",
    "**Step-2:** Calculate the Euclidean distance of K number of neighbors.\n",
    "\n",
    "**Step-3:** Take the K nearest neighbors as per the calculated Euclidean distance.\n",
    "\n",
    "**Step-4:** Among these k neighbors, count the number of the data points in each category.\n",
    "\n",
    "**Step-5:** Assign the new data points to that category for which the number of the neighbor is maximum.\n",
    "\n",
    "**Step-6:** Our model is ready."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c39eb0a",
   "metadata": {},
   "source": [
    "### KNN Decision Boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a93795",
   "metadata": {},
   "source": [
    "A decision boundary or decision surface is a hypersurface that partitions the underlying vector space into two or more sets, one for each class. The classifier will classify all the points on one side of the decision boundary as belonging to one class and all those on the other side as belonging to the other class.\n",
    "\n",
    "- We can draw decision boundary for all classification algorithms.\n",
    "- Decision boundary can be linear(if the decision surface is a hyperplane, then the classification problem is linear, and the classes are linearly separable as in SVM) or non linear(in case of Decision Tree). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3443e10c",
   "metadata": {},
   "source": [
    "<img src=\"images/knn2.png\" height=600px width=600px align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab53c269",
   "metadata": {},
   "source": [
    "### Supervised and Non-parametric Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ca8180",
   "metadata": {},
   "source": [
    "Let’s first start by establishing some definitions and notations. We will use **x** to denote a feature (aka. predictor, attribute) and **y** to denote the target (aka. label, class) we are trying to predict.\n",
    "\n",
    "KNN falls in the supervised learning family of algorithms. Informally, this means that we are given a labelled dataset consiting of training observations **(x,y)** and would like to capture the relationship between x and y. More formally, our goal is to learn a function **h:X→Y** so that given an unseen observation x, **h(x)** can confidently predict the corresponding output y.\n",
    "\n",
    "K-NN is a **non-parametric** algorithm, which means it does not make any assumption on underlying data. It means it makes no explicit assumptions about the functional form of h, avoiding the dangers of mismodeling the underlying distribution of the data. For example, suppose our data is highly non-Gaussian but the learning model we choose assumes a Gaussian form. In that case, our algorithm would make extremely poor predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e9dc28",
   "metadata": {},
   "source": [
    "### Classification and Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02fbfa0",
   "metadata": {},
   "source": [
    "K-NN algorithm can be used for Regression as well as for Classification but mostly it is used for the Classification problems.KNN algorithm at the training phase just stores the dataset and when it gets new data, then it classifies that data into a category that is much similar to the new data.\n",
    "\n",
    "Regression problems use a similar concept as classification problem, but in this case, the average the k nearest neighbors is taken to make a prediction about a classification. The main distinction here is that classification is used for discrete values, whereas regression is used with continuous ones.\n",
    "\n",
    "**Example:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05298551",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98bb4295",
   "metadata": {},
   "source": [
    "### An Instance-based learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2801820",
   "metadata": {},
   "source": [
    "- The **Machine Learning** systems which are categorized as **instance-based learning** are the systems that learn the training examples by heart and then generalizes to new instances based on some similarity measure. \n",
    "\n",
    "- It is called instance-based because it builds the hypotheses from the training instances. It is also known as **memory-based learning** or **lazy-learning** (because they delay processing until a new instance must be classified).\n",
    "\n",
    "- The time complexity of this algorithm depends upon the size of training data. Each time whenever a new query is encountered, its previously stores data is examined. And assign to a target function value for the new instance.  \n",
    "\n",
    "- **KNN** is an example of instance based learning algorithm.\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "- Instead of estimating for the entire instance set, local approximations can be made to the target function.\n",
    "\n",
    "- This algorithm can adapt to new data easily, one which is collected as we go.\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "- Classification costs are high.\n",
    "\n",
    "- Large amount of memory required to store the data, and each query involves starting the identification of a local model from scratch.\n",
    "\n",
    "\n",
    "**Lazy Learning:** No learning of the model is required and all of the work happens at the time a prediction is requested. As such, KNN is often referred to as a lazy learning algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c50092",
   "metadata": {},
   "source": [
    "### Characteristics of KNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f48ac4",
   "metadata": {},
   "source": [
    "\n",
    "- Nearest-neighbor classification is an element of more general approaches called **instance-based learning**. It needs specific training instances to create predictions without having to support an abstraction (or model) derived from data.\n",
    "\n",
    "- Instance-based learning algorithms needed a **proximity measure** to decide the similarity or distance among instances and a classification function that restores the predicted class of a test instance depending on its proximity to other instances.\n",
    "\n",
    "- **Lazy learners** including nearest-neighbor classifiers do not need model building. But defining a test example can be quite cheap because it is required to calculate the proximity values individually among the test and training examples. In contrast, eager learners spend the number of their computing resources for model building. Because a model has been constructed, defining a test example is completely quick.\n",
    "\n",
    "- Nearest-neighbor classifiers create their predictions depending on **local data**, whereas decision tree and rule-based classifiers try to discover a global model that fits the whole input space. Due to the classification decisions being create locally, nearest-neighbor classifiers are affected by noise.\n",
    "\n",
    "- Nearest-neighbor classifiers can make false predictions unless the suitable proximity measure and data preprocessing phases are taken. \n",
    "  - **For instance**, consider that it is required to define a set of people based on attributes such as height (measured in meters) and weight (measured in pounds). The height attribute has a low variability, ranging from 1.5 m to 1.85 m, whereas the weight attribute can change from 90 lb. to 250 lb. If the scale of the attributes is not taken into the application, the proximity measure can be dominated by differences in the weights of a person."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d088759c",
   "metadata": {},
   "source": [
    "### Practical issues with KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5fe6e0",
   "metadata": {},
   "source": [
    "- Accuracy depends on the quality of the data\n",
    "- With large data, the prediction stage might be slow\n",
    "- Sensitive to the scale of the data and irrelevant features\n",
    "- Require high memory – need to store all of the training data\n",
    "- Given that it stores all of the training, it can be computationally expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940e0e08",
   "metadata": {},
   "source": [
    "# Distance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9accc0",
   "metadata": {},
   "source": [
    "Distance metrics are a key part of several machine learning algorithms. These distance metrics are used in both supervised and unsupervised learning, generally to calculate the similarity between data points.\n",
    "An effective distance metric improves the performance of our machine learning model, whether that’s for classification tasks or clustering.\n",
    "\n",
    "For the algorithm to work best on a particular dataset we need to choose the most appropriate distance metric accordingly. There are a lot of different distance metrics available, but we are only going to talk about a few widely used ones. Euclidean distance function is the most popular one among all of them as it is set default in the SKlearn KNN classifier library in python.\n",
    "\n",
    "Let’s start with the most commonly used distance metric – Euclidean Distance.\n",
    "\n",
    "### Euclidean Distance\n",
    "\n",
    "Euclidean Distance represents the shortest distance between two points.\n",
    "\n",
    "Most machine learning algorithms including K-Means use this distance metric to measure the similarity between observations. Let’s say we have two points as shown below:\n",
    "\n",
    "<img src=\"images/knn4.PNG\" height=500px width=500px align='center'>\n",
    "\n",
    "So, the Euclidean Distance between these two points A and B will be:\n",
    "\n",
    "<img src=\"images/knn5.PNG\" height=500px width=500px align='center'>\n",
    "\n",
    "Here’s the formula for Euclidean Distance:\n",
    "\n",
    "<img src=\"images/knn7.PNG\" height=300px width=300px align='center'>\n",
    "\n",
    "We use this formula when we are dealing with 2 dimensions. We can generalize this for an n-dimensional space as:\n",
    "\n",
    "<img src=\"images/knn8.PNG\" height=300px width=300px align='center'>\n",
    "\n",
    "Where,\n",
    "- n = number of dimensions\n",
    "- pi, qi = data points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98c1cad",
   "metadata": {},
   "source": [
    "### Manhatten Distance\n",
    "\n",
    "Manhattan Distance is the sum of absolute differences between points across all the dimensions.\n",
    "\n",
    "                                        OR\n",
    "The distance between two points is the sum of the absolute differences of their Cartesian coordinates.\n",
    "\n",
    "\n",
    "This distance is also known as taxicab distance or city block distance, that is because the way this distance is calculated. This distance is preferred over Euclidean distance when we have a case of high dimensionality.\n",
    "\n",
    "Again consider the above points. We can represent Manhattan Distance as:\n",
    "\n",
    "<img src=\"images/knn6.PNG\" height=500px width=500px align='center'>\n",
    "\n",
    "Since the above representation is 2 dimensional, to calculate Manhattan Distance, we will take the sum of absolute distances in both the x and y directions. So, the Manhattan distance in a 2-dimensional space is given as:\n",
    "\n",
    "<img src=\"images/knn9.PNG\" height=300px width=300px align='center'>\n",
    "\n",
    "And the generalized formula for an n-dimensional space is given as:\n",
    "\n",
    "<img src=\"images/knn10.PNG\" height=300px width=300px align='center'>\n",
    "\n",
    "Where,\n",
    "- n = number of dimensions\n",
    "- pi, qi = data points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb1967a",
   "metadata": {},
   "source": [
    "### Minkowski Distance\n",
    "Minkowski Distance is the generalized form of Euclidean and Manhattan Distance.\n",
    "\n",
    "It is a metric intended for real-valued vector spaces. We can calculate Minkowski distance only in a normed vector space, which means in a space where distances can be represented as a vector that has a length and the lengths cannot be negative.\n",
    "\n",
    "The formula for Minkowski Distance is given as:\n",
    "\n",
    "<img src=\"images/knn11.PNG\" height=300px width=300px align='center'>\n",
    "\n",
    "This above formula for Minkowski distance is in generalized form and we can manipulate it to get different distance metrices.\n",
    "\n",
    "The p value in the formula can be manipulated to give us different distances like:\n",
    "\n",
    "- p = 1, when p is set to 1 we get Manhattan distance\n",
    "- p = 2, when p is set to 2 we get Euclidean distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4385eb0a",
   "metadata": {},
   "source": [
    "### Chebyshev Distance\n",
    "The Chebyshev distance between two n-dimensional points or vectors is the maximum absolute magnitude of the differences between the coordinates of the points. \n",
    "\n",
    "For the Cartesian coordinate system, the Chebyshev distance between two points can be determined as the sum of the absolute differences of their Cartesian coordinates. Other names for the Chebyshev distance are maximum metric and L∞ metric. \n",
    "\n",
    "The Chebyshev distance between two points p and q with coordinates pi and qi is\n",
    "\n",
    "<img src=\"images/cformulae.png\" height=300px width=300px align='center'>\n",
    "\n",
    "For example, consider the two points in a 3D grid p (x₁,y₁,z₁) = p (2,3,4) and q (x₂,y₂,z₂) = q (5,9,11). Then the Chebyshev distance between points p and q is\n",
    "\n",
    "<img src=\"images/cform.png\" height=300px width=300px align='center'>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f491b81",
   "metadata": {},
   "source": [
    "### Comparison \n",
    "\n",
    "<img src=\"images/knn.jpg\" height=600px width=600px align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1eaf4a",
   "metadata": {},
   "source": [
    "### Cosine Distance\n",
    "This distance metric is used mainly to calculate similarity between two vectors. \n",
    "\n",
    "It is measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in the same direction. It is often used to measure document similarity in text analysis.\n",
    "\n",
    "<img src=\"images/cosine.png\" height=300px width=300px align='center'>\n",
    "\n",
    "Using this formula we will get a value which tells us about the similarity between the two vectors and 1-cosΘ will give us their cosine distance.\n",
    "\n",
    "Using this distance we get values between 0 and 1, where 0 means the vectors are 100% similar to each other and 1 means they are not similar at all.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560c2c70",
   "metadata": {},
   "source": [
    "### Norm of a vector and Its Properties\n",
    "\n",
    "\n",
    "A **norm** is a way to measure the size of a vector, a matrix, or a tensor. In other words, norms are a class of functions that enable us to quantify the magnitude of a vector. \n",
    "\n",
    "\n",
    "**WHAT ARE THE PROPERTIES OF A NORM?**\n",
    "\n",
    "- **Non-negativity:** It should always be non-negative.\n",
    "- **Definiteness:** It is zero if and only if the vector is zero, i.e., zero vector.\n",
    "- **Triangle inequality:** The norm of a sum of two vectors is no more than the sum of their norms.\n",
    "- **Homogeneity:** Multiplying a vector by a scalar multiplies the norm of the vector by the absolute value of the scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97af896",
   "metadata": {},
   "source": [
    "## The KNN algorithm and Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0d6a67",
   "metadata": {},
   "source": [
    "### KNN regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5e45b6",
   "metadata": {},
   "source": [
    "Let us start with a simple example. Consider the following table – it consists of the height, age and weight (target) value for 10 people. As you can see, the weight value of ID11 is missing. We need to predict the weight of this person based on their height and age.\n",
    "\n",
    "<img src=\"images/ex1.png\" height=500px width=500px align='center'>\n",
    "\n",
    "For a clearer understanding of this, below is the plot of height versus age from the above table:\n",
    "\n",
    "<img src=\"images/ex2.png\" height=500px width=600px align='center'>\n",
    "\n",
    "In the above graph, the y-axis represents the height of a person (in feet) and the x-axis represents the age (in years). The points are numbered according to the ID values. The yellow point (ID 11) is our test point.\n",
    "\n",
    "If I ask you to identify the weight of ID11 based on the plot, what would be your answer? You would likely say that since ID11 is closer to points 5 and 1, so it must have a weight similar to these IDs, probably between 72-77 kgs (weights of ID1 and ID5 from the table). That actually makes sense, but how do you think the algorithm predicts the values?\n",
    "\n",
    "The KNN algorithm uses ‘feature similarity’ to predict the values of any new data points. This means that the new point is assigned a value based on how closely it resembles the points in the training set. From our example, we know that ID11 has height and age similar to ID1 and ID5, so the weight would also approximately be the same.\n",
    "\n",
    "Had it been a classification problem, we would have taken the mode as the final prediction. In this case, we have two values of weight – 72 and 77. Any guesses on how the final value will be calculated? The average of the values is taken to be the final prediction.\n",
    "\n",
    "Below is a stepwise explanation of the algorithm:\n",
    "- First, the distance between the new point and each training point is calculated.\n",
    "<img src=\"images/ex3.png\" height=500px width=600px align='center'>\n",
    "\n",
    "- The closest k data points are selected (based on the distance). In this example, points 1, 5, 6 will be selected if the value of k is 3. \n",
    "<img src=\"images/ex5.png\" height=500px width=600px align='center'>\n",
    "\n",
    "     The average of these data points is the final prediction for the new point. Here, we have weight of \n",
    "                                   ID11 = (77+72+60)/3 = 69.66 kg \n",
    "                                   \n",
    "- For the value of k=5, the closest point will be ID1, ID4, ID5, ID6, ID10.\n",
    "\n",
    "<img src=\"images/ex6.png\" height=500px width=600px align='center'>\n",
    "\n",
    "<img src=\"images/ex7.png\" height=400px width=400px align='center'>\n",
    "\n",
    "      The prediction for ID11 will be :\n",
    "                                   ID 11 =  (77+59+72+60+58)/5 \n",
    "                                   ID 11 = 65.2 kg\n",
    "                                   \n",
    "We notice that based on the k value, the final result tends to change. Then how can we figure out the optimum value of k? Let us decide it based on the error calculation for our train and validation set (after all, minimizing the error is our final goal!).                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed314f5",
   "metadata": {},
   "source": [
    "### Python implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e2b28bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "url = (\n",
    "   \"https://archive.ics.uci.edu/ml/machine-learning-databases\"\n",
    "    \"/abalone/abalone.data\"\n",
    ")\n",
    "abalone = pd.read_csv(url, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10b8b588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0      1      2      3       4       5       6      7   8\n",
       "0  M  0.455  0.365  0.095  0.5140  0.2245  0.1010  0.150  15\n",
       "1  M  0.350  0.265  0.090  0.2255  0.0995  0.0485  0.070   7\n",
       "2  F  0.530  0.420  0.135  0.6770  0.2565  0.1415  0.210   9\n",
       "3  M  0.440  0.365  0.125  0.5160  0.2155  0.1140  0.155  10\n",
       "4  I  0.330  0.255  0.080  0.2050  0.0895  0.0395  0.055   7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abalone.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3751e56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone.columns = [\n",
    "     \"Sex\",\n",
    "     \"Length\",\n",
    "     \"Diameter\",\n",
    "    \"Height\",\n",
    "     \"Whole weight\",\n",
    "     \"Shucked weight\",\n",
    "     \"Viscera weight\",\n",
    "     \"Shell weight\",\n",
    "     \"Rings\",\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72a1177f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Shucked weight</th>\n",
       "      <th>Viscera weight</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>Rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.070</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>F</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.210</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.155</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.055</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sex  Length  Diameter  Height  Whole weight  Shucked weight  Viscera weight  \\\n",
       "0   M   0.455     0.365   0.095        0.5140          0.2245          0.1010   \n",
       "1   M   0.350     0.265   0.090        0.2255          0.0995          0.0485   \n",
       "2   F   0.530     0.420   0.135        0.6770          0.2565          0.1415   \n",
       "3   M   0.440     0.365   0.125        0.5160          0.2155          0.1140   \n",
       "4   I   0.330     0.255   0.080        0.2050          0.0895          0.0395   \n",
       "\n",
       "   Shell weight  Rings  \n",
       "0         0.150     15  \n",
       "1         0.070      7  \n",
       "2         0.210      9  \n",
       "3         0.155     10  \n",
       "4         0.055      7  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abalone.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccc3ff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "abalone = abalone.drop(\"Sex\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3da71b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = abalone.drop(\"Rings\", axis=1)\n",
    "X = X.values\n",
    "y = abalone[\"Rings\"]\n",
    "y = y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13eb9b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.59739395, 0.9518455 , 0.40573594, ..., 0.20397872, 0.14342627,\n",
       "       1.10583307])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data_point = np.array([0.569552,0.446407,0.154437,1.016849,0.439051,0.222526, 0.291208, ]) #testdata\n",
    "distances = np.linalg.norm(X - new_data_point, axis=1) #return vector of distances\n",
    "distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42a7125e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4045, 1902, 1644], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=3\n",
    "nearest_neighbor_ids = distances.argsort()[:k] #tells you which three neighbors are closest to your new_data_point.\n",
    "nearest_neighbor_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79d3ebec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9, 11, 10], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest_neighbor_rings = y[nearest_neighbor_ids]\n",
    "nearest_neighbor_rings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c49775ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = nearest_neighbor_rings.mean()\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b3a5a8",
   "metadata": {},
   "source": [
    "### Space and Time complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15abe6f1",
   "metadata": {},
   "source": [
    "**Training Phase:** \n",
    "\n",
    "   - In training phase as we need to store all the datapoints.\n",
    "   - The space complexity would be **O(n*d)** where n represents number of datapoints and d represents the number of features that determine each datapoint.\n",
    "\n",
    "   - The time that it takes to calculate the n datapoints with d features would would be O(n*d).\n",
    "\n",
    "**Testing Phase:**\n",
    "\n",
    "   - Testing phase or Run time complexity is **O(n*k*d)** where k represents number of nearest neighbors that needs to be considered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f29791",
   "metadata": {},
   "source": [
    "### Choosing the value of K - The Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99961fee",
   "metadata": {},
   "source": [
    "if **k=1**\n",
    "- Sensitive to noise\n",
    "- High variance\n",
    "- Increasing k makes algorithm less sensitive to noise \n",
    "\n",
    "**k=n**\n",
    "- Decreasing k enables capturing finer structure of space\n",
    "\n",
    "Idea: Pick k not too large, but not too small (depends on data)\n",
    "How?\n",
    "\n",
    "**Choice of k**\n",
    "- Learn the best hyper-parameter, k using the data.\n",
    "- Split data into training and validation.\n",
    "- Start from k=1 and keep iterating by carrying out (5 or 10, for example) cross-validation and computing the loss on the     validation data using the training data.\n",
    "- Choose the value for k that minimizes validation loss.\n",
    "- This is the only learning required for kNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1614e96d",
   "metadata": {},
   "source": [
    "### K Fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786433bc",
   "metadata": {},
   "source": [
    "Consider the below example:\n",
    "\n",
    "Suppose we have data D and and we have divided our data in training data (80%) and testing data (20%)\n",
    "After splitting the total data set (D_n) into training (D_Train) and test (D_Test) data set, in the ratio of 80:20, we further randomly split the training data into 4 equal parts. D_Train into D1 (20%), D2(20%), D3(20%), D4(20%)\n",
    "D1, D2, D3, and D4 are the four randomly split equal parts of D_Train. Once done with the splitting, we proceed as follows:\n",
    "\n",
    "<img src=\"images/kfold.png\" height=500px width=600px align='center'>\n",
    "\n",
    "**Step-1:** For K=1, we pick D1, D2, and D3 as my training data set and set D4 as my cross-validation data and find the nearest neighbors and calculate its accuracy.\n",
    "\n",
    "**Step-2:** Again, for K=1, we pick D1, D2, and D4 as my training data set and set D3 as my cross-validation data, I find the nearest neighbors and calculate its accuracy.\n",
    "\n",
    "we repeat the above steps with D2 and D1 as my cross-validation data set and calculate the corresponding accuracy. Once done with it, I get 4 accuracies for the same value of K=1. So I consider the mean of these accuracies and assign it as the final value when my K=1.\n",
    "\n",
    "Now, I repeat the above steps for K=2 and find the mean accuracy for K=2. So on and so forth, I calculate the accuracies for different values of K.\n",
    "\n",
    "Now, note that for each value of K I had to compute the accuracy 4 times. This is because I randomly split my training data set into 4 equal parts. Suppose I had randomly split my data set into 5 equal parts then I would have to compute 5 different accuracies for each value of K and take their mean.\n",
    "\n",
    "**Please Note: Capital “K” stands for the K value in KNN and lower “k” stands for k value in k-fold cross-validation**\n",
    "\n",
    "So, k value in k-fold cross-validation for the above example is 4 (i.e k=4), had we split the training data into 5 equal parts, the value of k=5.\n",
    "\n",
    "k = number of parts we randomly split our training data set into.\n",
    "\n",
    "Now, we are using the entire 80% of our data to compute the nearest neighbors as well as the K value in KNN.\n",
    "\n",
    "Just one drawback with k-fold cross-validation is that we are repeating the computations for each value of K (of KNN). So it basically increases the time complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f72c46",
   "metadata": {},
   "source": [
    "**Limitations of KNN:**\n",
    "1. Doesn’t work well with a large dataset\n",
    "2. Doesn’t work well with a high number of dimensions\n",
    "3. Sensitive to outliers and missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cd47e0",
   "metadata": {},
   "source": [
    "## Curse of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9d5ae5",
   "metadata": {},
   "source": [
    "### Assumption\n",
    "First, it needs you to trust the assumption that it’s nearby neighbors are similar to it. There are some cases where this will be true. For instance, pieces of produce in a grocery store are commonly sorted into similar groups.\n",
    "\n",
    "Second, you need to have a way to measure distance. Note that this is dimensional distance, not necessarily physical distance. It refers to the differs between the two data points you’re trying to compare.\n",
    "\n",
    "This means that your success when using the k-nearest neighbors algorithm is very dependent on having a dense data set. This makes it especially vulnerable to the “Curse of Dimensionality”.\n",
    "\n",
    "The special challenge with k-nearest neighbors is that it requires a point to be close in every single dimension. k-nearest neighbors needs all points to be close along every axis in the data space. And each new axis added, by adding a new dimension, makes it harder and harder for two specific points to be close to each other in every axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203d8be1",
   "metadata": {},
   "source": [
    "## KNN Enhancements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520ba405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8428a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a927b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
