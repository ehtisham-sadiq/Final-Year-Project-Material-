{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b525179",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Nearest Neighbors Methods - The KNN Algorithm</h1>\n",
    "\n",
    "### KNN Algorithm\n",
    "- Basic Idea\n",
    "- Formal Definition\n",
    "- KNN Decision Boundary\n",
    "- A supervised, non-parametric algorithm\n",
    "- Used for classification and regression\n",
    "- An Instance-based learning algorithm\n",
    "- A lazy learning algorithm\n",
    "- Characteristics of kNN\n",
    "- Practical issues\n",
    "\n",
    "### Similarity/Distance Metrics \n",
    "- Constraints/Properties on Distance Metrics\n",
    "- Euclidean Distance\n",
    "- Manhatten Distance\n",
    "- Minkowski distance\n",
    "- Chebyshev Distance\n",
    "- Norm of a vector and Its Properties\n",
    "- Cosine Distance \n",
    "- Practical Issues in computing distance\n",
    "\n",
    "### The KNN algorithm and Implementation\n",
    "- KNN regression and classification with examples\n",
    "- Space and Time complexity\n",
    "- Choosing the value of K - The Theory.\n",
    "- Tuning the hyperparameter K - The method\n",
    "- KNN: The good, the bad and the ugly\n",
    "    \n",
    "### Algorithm Convergence\n",
    "- Error Convergence\n",
    "- Learning Problem\n",
    "- Bayes Optimal Classifier\n",
    "- 1-NN Error as n → ∞\n",
    "\n",
    "\n",
    "### KNN Enhancements\n",
    "- Parzen Windows and Kernels (Fast KNN) \n",
    "- Performance of KNN Algorithm\n",
    "- K-D Trees\n",
    "- Locality-sensitive Hashing\n",
    "- Inverted Lists\n",
    "\n",
    "### The Curse of Dimensionality\n",
    "- KNN Assumption\n",
    "- Demonstration\n",
    "- How does KNN work at all?\n",
    "\n",
    "### Dimensionality Reduction(Optional)\n",
    "- Why? and Benefits.\n",
    "- Difference between Feature Selection and Feature Extraction\n",
    "- Feature Selection methods\n",
    "- Feature Extraction\n",
    "- Principal Component Analysis\n",
    "    - Geometric Intuition\n",
    "    - Mathematical Formulation\n",
    "    - How do we choose K?\n",
    "    - Practical Consideration and Limitations\n",
    "\n",
    "### Model Evaluation Techniques\n",
    "- Classification Accuracy (0/1 Loss)\n",
    "- TP, TN, FP and FN\n",
    "- Confusion Matrix\n",
    "- Sensitivity, Specificity, Precision Trade-offs, ROC, AUC\n",
    "- F1-Score and Matthew’s Correlation Coefficient\n",
    "- Multi-class Classification, Evaluation, Micro, Macro Averaging\n",
    "\n",
    "**KNN: Python Implementation**    \n",
    "**KNN: Scikit-learn implementation**    \n",
    "**Interview Questions.**   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c2bd88",
   "metadata": {},
   "source": [
    "# KNN Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5065047",
   "metadata": {},
   "source": [
    "### Basic Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d205601",
   "metadata": {},
   "source": [
    "- K-Nearest Neighbour is one of the simplest Machine Learning algorithms based on Supervised Learning technique.\n",
    "- It assumes the similarity between the new data points and available data points and put the new data points into the category that is most similar to the available categories.Where'k' in KNN is a parameter that refers to the number of nearest neighbours to include in the majority of the voting process.\n",
    "- It stores all the available data and classifies a new data point based on the similarity. This means when new data appears then it can be easily classified into a well suite category by using K- NN algorithm.\n",
    "- **Example:** Suppose, we have an image of a creature that looks similar to cat and dog, but we want to know either it is a cat or dog. So for this identification, we can use the KNN algorithm, as it works on a similarity measure. Our KNN model will find the similar features of the new data set to the cats and dogs images and based on the most similar features it will put it in either cat or dog category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48a032d",
   "metadata": {},
   "source": [
    "### Formal Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7620e7",
   "metadata": {},
   "source": [
    "\"The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning algorithm, which uses proximity to make classifications or predictions about the grouping of an individual data point.\" (IBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4700e175",
   "metadata": {},
   "source": [
    "<img src=\"images/knn1.PNG\" height=600px width=600px align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ed2cb0",
   "metadata": {},
   "source": [
    "### How does basic K-NN work?\n",
    "The K-NN working can be explained on the basis of the below algorithm:\n",
    "\n",
    "**Step-1:** Select the number K of the neighbors\n",
    "\n",
    "**Step-2:** Calculate the Euclidean distance of K number of neighbors.\n",
    "\n",
    "**Step-3:** Take the K nearest neighbors as per the calculated Euclidean distance.\n",
    "\n",
    "**Step-4:** Among these k neighbors, count the number of the data points in each category.\n",
    "\n",
    "**Step-5:** Assign the new data points to that category for which the number of the neighbor is maximum.\n",
    "\n",
    "**Step-6:** Our model is ready."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c39eb0a",
   "metadata": {},
   "source": [
    "### KNN Decision Boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a93795",
   "metadata": {},
   "source": [
    "A decision boundary or decision surface is a hypersurface that partitions the underlying vector space into two or more sets, one for each class. The classifier will classify all the points on one side of the decision boundary as belonging to one class and all those on the other side as belonging to the other class.\n",
    "\n",
    "- We can draw decision boundary for all classification algorithms.\n",
    "- Decision boundary can be linear(if the decision surface is a hyperplane, then the classification problem is linear, and the classes are linearly separable as in SVM) or non linear(in case of Decision Tree). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3443e10c",
   "metadata": {},
   "source": [
    "<img src=\"images/knn2.PNG\" height=600px width=600px align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab53c269",
   "metadata": {},
   "source": [
    "### Supervised and Non-parametric Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ca8180",
   "metadata": {},
   "source": [
    "Let’s first start by establishing some definitions and notations. We will use **x** to denote a feature (aka. predictor, attribute) and **y** to denote the target (aka. label, class) we are trying to predict.\n",
    "\n",
    "KNN falls in the supervised learning family of algorithms. Informally, this means that we are given a labelled dataset consiting of training observations **(x,y)** and would like to capture the relationship between x and y. More formally, our goal is to learn a function **h:X→Y** so that given an unseen observation x, **h(x)** can confidently predict the corresponding output y.\n",
    "\n",
    "K-NN is a **non-parametric** algorithm, which means it does not make any assumption on underlying data. It means it makes no explicit assumptions about the functional form of h, avoiding the dangers of mismodeling the underlying distribution of the data. For example, suppose our data is highly non-Gaussian but the learning model we choose assumes a Gaussian form. In that case, our algorithm would make extremely poor predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e9dc28",
   "metadata": {},
   "source": [
    "### Classification and Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02fbfa0",
   "metadata": {},
   "source": [
    "K-NN algorithm can be used for Regression as well as for Classification but mostly it is used for the Classification problems.KNN algorithm at the training phase just stores the dataset and when it gets new data, then it classifies that data into a category that is much similar to the new data.\n",
    "\n",
    "Regression problems use a similar concept as classification problem, but in this case, the average the k nearest neighbors is taken to make a prediction about a classification. The main distinction here is that classification is used for discrete values, whereas regression is used with continuous ones.\n",
    "\n",
    "**Example:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05298551",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98bb4295",
   "metadata": {},
   "source": [
    "### An Instance-based learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2801820",
   "metadata": {},
   "source": [
    "- The **Machine Learning** systems which are categorized as **instance-based learning** are the systems that learn the training examples by heart and then generalizes to new instances based on some similarity measure. \n",
    "\n",
    "- It is called instance-based because it builds the hypotheses from the training instances. It is also known as **memory-based learning** or **lazy-learning** (because they delay processing until a new instance must be classified).\n",
    "\n",
    "- The time complexity of this algorithm depends upon the size of training data. Each time whenever a new query is encountered, its previously stores data is examined. And assign to a target function value for the new instance.  \n",
    "\n",
    "- **KNN** is an example of instance based learning algorithm.\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "- Instead of estimating for the entire instance set, local approximations can be made to the target function.\n",
    "\n",
    "- This algorithm can adapt to new data easily, one which is collected as we go.\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "- Classification costs are high.\n",
    "\n",
    "- Large amount of memory required to store the data, and each query involves starting the identification of a local model from scratch.\n",
    "\n",
    "\n",
    "**Lazy Learning:** No learning of the model is required and all of the work happens at the time a prediction is requested. As such, KNN is often referred to as a lazy learning algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c50092",
   "metadata": {},
   "source": [
    "### Characteristics of KNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f48ac4",
   "metadata": {},
   "source": [
    "\n",
    "- Nearest-neighbor classification is an element of more general approaches called **instance-based learning**. It needs specific training instances to create predictions without having to support an abstraction (or model) derived from data.\n",
    "\n",
    "- Instance-based learning algorithms needed a **proximity measure** to decide the similarity or distance among instances and a classification function that restores the predicted class of a test instance depending on its proximity to other instances.\n",
    "\n",
    "- **Lazy learners** including nearest-neighbor classifiers do not need model building. But defining a test example can be quite cheap because it is required to calculate the proximity values individually among the test and training examples. In contrast, eager learners spend the number of their computing resources for model building. Because a model has been constructed, defining a test example is completely quick.\n",
    "\n",
    "- Nearest-neighbor classifiers create their predictions depending on **local data**, whereas decision tree and rule-based classifiers try to discover a global model that fits the whole input space. Due to the classification decisions being create locally, nearest-neighbor classifiers are affected by noise.\n",
    "\n",
    "- Nearest-neighbor classifiers can make false predictions unless the suitable proximity measure and data preprocessing phases are taken. \n",
    "  - **For instance**, consider that it is required to define a set of people based on attributes such as height (measured in meters) and weight (measured in pounds). The height attribute has a low variability, ranging from 1.5 m to 1.85 m, whereas the weight attribute can change from 90 lb. to 250 lb. If the scale of the attributes is not taken into the application, the proximity measure can be dominated by differences in the weights of a person."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d088759c",
   "metadata": {},
   "source": [
    "### Practical issues with KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5fe6e0",
   "metadata": {},
   "source": [
    "- Accuracy depends on the quality of the data\n",
    "- With large data, the prediction stage might be slow\n",
    "- Sensitive to the scale of the data and irrelevant features\n",
    "- Require high memory – need to store all of the training data\n",
    "- Given that it stores all of the training, it can be computationally expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940e0e08",
   "metadata": {},
   "source": [
    "# Distance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9accc0",
   "metadata": {},
   "source": [
    "Distance metrics are a key part of several machine learning algorithms. These distance metrics are used in both supervised and unsupervised learning, generally to calculate the similarity between data points.\n",
    "An effective distance metric improves the performance of our machine learning model, whether that’s for classification tasks or clustering.\n",
    "\n",
    "For the algorithm to work best on a particular dataset we need to choose the most appropriate distance metric accordingly. There are a lot of different distance metrics available, but we are only going to talk about a few widely used ones. Euclidean distance function is the most popular one among all of them as it is set default in the SKlearn KNN classifier library in python.\n",
    "\n",
    "Let’s start with the most commonly used distance metric – Euclidean Distance.\n",
    "\n",
    "### Euclidean Distance\n",
    "\n",
    "Euclidean Distance represents the shortest distance between two points.\n",
    "\n",
    "Most machine learning algorithms including K-Means use this distance metric to measure the similarity between observations. Let’s say we have two points as shown below:\n",
    "\n",
    "<img src=\"images/knn4.PNG\" height=500px width=500px align='center'>\n",
    "\n",
    "So, the Euclidean Distance between these two points A and B will be:\n",
    "\n",
    "<img src=\"images/knn5.PNG\" height=500px width=500px align='center'>\n",
    "\n",
    "Here’s the formula for Euclidean Distance:\n",
    "\n",
    "<img src=\"images/knn7.PNG\" height=300px width=300px align='center'>\n",
    "\n",
    "We use this formula when we are dealing with 2 dimensions. We can generalize this for an n-dimensional space as:\n",
    "\n",
    "<img src=\"images/knn8.PNG\" height=300px width=300px align='center'>\n",
    "\n",
    "Where,\n",
    "- n = number of dimensions\n",
    "- pi, qi = data points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98c1cad",
   "metadata": {},
   "source": [
    "### Manhatten Distance\n",
    "\n",
    "Manhattan Distance is the sum of absolute differences between points across all the dimensions.\n",
    "\n",
    "                                        OR\n",
    "The distance between two points is the sum of the absolute differences of their Cartesian coordinates.\n",
    "\n",
    "\n",
    "This distance is also known as taxicab distance or city block distance, that is because the way this distance is calculated. This distance is preferred over Euclidean distance when we have a case of high dimensionality.\n",
    "\n",
    "Again consider the above points. We can represent Manhattan Distance as:\n",
    "\n",
    "<img src=\"images/knn6.PNG\" height=500px width=500px align='center'>\n",
    "\n",
    "Since the above representation is 2 dimensional, to calculate Manhattan Distance, we will take the sum of absolute distances in both the x and y directions. So, the Manhattan distance in a 2-dimensional space is given as:\n",
    "\n",
    "<img src=\"images/knn9.PNG\" height=300px width=300px align='center'>\n",
    "\n",
    "And the generalized formula for an n-dimensional space is given as:\n",
    "\n",
    "<img src=\"images/knn10.PNG\" height=300px width=300px align='center'>\n",
    "\n",
    "Where,\n",
    "- n = number of dimensions\n",
    "- pi, qi = data points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb1967a",
   "metadata": {},
   "source": [
    "### Minkowski Distance\n",
    "Minkowski Distance is the generalized form of Euclidean and Manhattan Distance.\n",
    "\n",
    "It is a metric intended for real-valued vector spaces. We can calculate Minkowski distance only in a normed vector space, which means in a space where distances can be represented as a vector that has a length and the lengths cannot be negative.\n",
    "\n",
    "The formula for Minkowski Distance is given as:\n",
    "\n",
    "<img src=\"images/knn11.PNG\" height=300px width=300px align='center'>\n",
    "\n",
    "This above formula for Minkowski distance is in generalized form and we can manipulate it to get different distance metrices.\n",
    "\n",
    "The p value in the formula can be manipulated to give us different distances like:\n",
    "\n",
    "- p = 1, when p is set to 1 we get Manhattan distance\n",
    "- p = 2, when p is set to 2 we get Euclidean distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4385eb0a",
   "metadata": {},
   "source": [
    "### Chebyshev Distance\n",
    "The Chebyshev distance between two n-dimensional points or vectors is the maximum absolute magnitude of the differences between the coordinates of the points. \n",
    "\n",
    "For the Cartesian coordinate system, the Chebyshev distance between two points can be determined as the sum of the absolute differences of their Cartesian coordinates. Other names for the Chebyshev distance are maximum metric and L∞ metric. \n",
    "\n",
    "The Chebyshev distance between two points p and q with coordinates pi and qi is\n",
    "\n",
    "<img src=\"images/cformulae.png\" height=300px width=300px align='center'>\n",
    "\n",
    "For example, consider the two points in a 3D grid p (x₁,y₁,z₁) = p (2,3,4) and q (x₂,y₂,z₂) = q (5,9,11). Then the Chebyshev distance between points p and q is\n",
    "\n",
    "<img src=\"images/cform.png\" height=300px width=300px align='center'>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f491b81",
   "metadata": {},
   "source": [
    "### Comparison \n",
    "\n",
    "<img src=\"images/knn.jpg\" height=600px width=600px align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1eaf4a",
   "metadata": {},
   "source": [
    "### Cosine Distance\n",
    "This distance metric is used mainly to calculate similarity between two vectors. \n",
    "\n",
    "It is measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in the same direction. It is often used to measure document similarity in text analysis.\n",
    "\n",
    "<img src=\"images/cosine.png\" height=300px width=300px align='center'>\n",
    "\n",
    "Using this formula we will get a value which tells us about the similarity between the two vectors and 1-cosΘ will give us their cosine distance.\n",
    "\n",
    "Using this distance we get values between 0 and 1, where 0 means the vectors are 100% similar to each other and 1 means they are not similar at all.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560c2c70",
   "metadata": {},
   "source": [
    "### Norm of a vector and Its Properties\n",
    "\n",
    "\n",
    "A **norm** is a way to measure the size of a vector, a matrix, or a tensor. In other words, norms are a class of functions that enable us to quantify the magnitude of a vector. \n",
    "\n",
    "\n",
    "**WHAT ARE THE PROPERTIES OF A NORM?**\n",
    "\n",
    "- **Non-negativity:** It should always be non-negative.\n",
    "- **Definiteness:** It is zero if and only if the vector is zero, i.e., zero vector.\n",
    "- **Triangle inequality:** The norm of a sum of two vectors is no more than the sum of their norms.\n",
    "- **Homogeneity:** Multiplying a vector by a scalar multiplies the norm of the vector by the absolute value of the scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97af896",
   "metadata": {},
   "source": [
    "### Practical Issues in computing distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec49f693",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98d8ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5df2cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9117b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 2.46860857e+00, -1.83256468e+00,  2.92913502e+00,\n",
       "         -1.58752889e+00],\n",
       "        [ 1.49560407e+00, -5.03016146e-01,  3.34799662e-01,\n",
       "          1.41306917e+00],\n",
       "        [-5.87512817e-01, -6.99464894e-03,  3.53586145e-01,\n",
       "         -1.35523642e+00],\n",
       "        [ 1.62804920e+00, -1.56355157e+00,  2.77343570e+00,\n",
       "         -2.43525035e+00],\n",
       "        [-1.07637151e+00,  8.97026797e-01, -1.50950251e+00,\n",
       "          1.07541303e+00],\n",
       "        [-1.07301040e+00,  3.55946035e-01, -2.28488201e-01,\n",
       "         -1.03311348e+00],\n",
       "        [ 1.86421051e+00, -5.72796908e-01,  2.88819837e-01,\n",
       "          1.97327676e+00],\n",
       "        [-7.97059544e-01,  6.51590924e-01, -1.08777124e+00,\n",
       "          7.46826653e-01],\n",
       "        [-3.43040211e-01,  1.28589509e-01, -1.08124996e-01,\n",
       "         -2.72427209e-01],\n",
       "        [ 2.91591039e+00, -2.20158151e+00,  3.54752570e+00,\n",
       "         -2.01974420e+00],\n",
       "        [-1.41274837e+00,  7.75902302e-01, -1.02935939e+00,\n",
       "         -1.58565165e-01],\n",
       "        [ 9.84334951e-01, -3.66231453e-01,  3.03740521e-01,\n",
       "          7.92466134e-01],\n",
       "        [-5.39987803e-01,  2.09677379e-01, -1.87419828e-01,\n",
       "         -4.00434825e-01],\n",
       "        [ 6.33170535e-01, -1.45703382e-01, -1.77174650e-02,\n",
       "          8.61241111e-01],\n",
       "        [-7.58610989e-01,  6.74469964e-01, -1.16407348e+00,\n",
       "          9.23205141e-01],\n",
       "        [-9.70679834e-01,  8.27015199e-01, -1.40412532e+00,\n",
       "          1.04048486e+00],\n",
       "        [ 1.85880410e+00, -6.46036821e-01,  4.65578211e-01,\n",
       "          1.67462193e+00],\n",
       "        [ 9.44913659e-01, -3.78412043e-01,  3.55233915e-01,\n",
       "          6.55729869e-01],\n",
       "        [-1.11019707e+00,  9.27711335e-01, -1.56285523e+00,\n",
       "          1.11896616e+00],\n",
       "        [ 1.18962969e+00, -3.76451451e-01,  2.10214441e-01,\n",
       "          1.21649827e+00],\n",
       "        [-2.32646209e+00,  1.26305919e+00, -1.66033182e+00,\n",
       "         -3.18485433e-01],\n",
       "        [-1.28434124e+00,  5.84345793e-01, -6.48820021e-01,\n",
       "         -6.17504824e-01],\n",
       "        [-4.08827830e-01,  4.69461815e-01, -8.78622069e-01,\n",
       "          9.12005447e-01],\n",
       "        [-6.13375201e-01,  4.76678992e-01, -7.78404772e-01,\n",
       "          4.77918689e-01],\n",
       "        [ 7.13394322e-01, -6.82187036e-01,  1.20830781e+00,\n",
       "         -1.05558350e+00],\n",
       "        [ 3.03795980e-01, -2.46387371e-01,  4.09943142e-01,\n",
       "         -2.76969498e-01],\n",
       "        [ 1.29295243e+00, -4.88035529e-01,  4.15521870e-01,\n",
       "          1.01362927e+00],\n",
       "        [ 9.17111796e-01, -2.64783370e-01,  1.01759316e-01,\n",
       "          1.03728540e+00],\n",
       "        [ 4.17577519e-01, -1.86687747e-01,  2.03125088e-01,\n",
       "          2.13676936e-01],\n",
       "        [-2.41008280e+00,  1.07611435e+00, -1.16910646e+00,\n",
       "         -1.23860894e+00],\n",
       "        [ 1.09801141e+00, -7.90940585e-01,  1.24555151e+00,\n",
       "         -6.11608500e-01],\n",
       "        [-4.93099500e-01,  4.92243135e-02,  1.66130736e-01,\n",
       "         -9.21978117e-01],\n",
       "        [ 1.15074863e+00, -9.68009501e-01,  1.63514450e+00,\n",
       "         -1.18491547e+00],\n",
       "        [-6.25704839e-01,  5.45891340e-01, -9.35439973e-01,\n",
       "          7.20735477e-01],\n",
       "        [-2.04141012e+00,  7.82502969e-01, -6.84405429e-01,\n",
       "         -1.55363514e+00],\n",
       "        [-7.27298840e-01,  6.26815025e-01, -1.06904097e+00,\n",
       "          8.07601407e-01],\n",
       "        [-1.62618453e+00,  1.31834362e+00, -2.19309919e+00,\n",
       "          1.48047630e+00],\n",
       "        [-1.07305627e+00,  9.35226309e-01, -1.60197800e+00,\n",
       "          1.23230127e+00],\n",
       "        [-1.08055188e+00,  8.65688559e-01, -1.43279935e+00,\n",
       "          9.43403351e-01],\n",
       "        [-9.28879199e-01,  7.87394136e-01, -1.33415800e+00,\n",
       "          9.80006766e-01],\n",
       "        [ 1.67735108e+00, -1.24370242e+00,  1.98676995e+00,\n",
       "         -1.07291531e+00],\n",
       "        [ 7.87147357e-02, -8.53425604e-03, -2.49159554e-02,\n",
       "          1.44532175e-01],\n",
       "        [-8.58830512e-01,  3.97962992e-01, -4.50968527e-01,\n",
       "         -3.84704832e-01],\n",
       "        [-1.55921050e+00,  3.77701668e-01, -1.18475055e-03,\n",
       "         -2.04692554e+00],\n",
       "        [ 3.55975664e-01, -2.35697974e-01,  3.54667179e-01,\n",
       "         -1.17228991e-01],\n",
       "        [ 1.80645939e+00, -6.42955551e-01,  4.88297570e-01,\n",
       "          1.56836441e+00],\n",
       "        [ 7.23043992e-01, -2.01548728e-01,  6.31439334e-02,\n",
       "          8.45964076e-01],\n",
       "        [-1.19425691e+00,  1.06947564e+00, -1.85077153e+00,\n",
       "          1.48340509e+00],\n",
       "        [-1.26989059e+00,  1.05615360e+00, -1.77579999e+00,\n",
       "          1.26035842e+00],\n",
       "        [ 1.62316960e+00, -1.44169219e+00,  2.48729711e+00,\n",
       "         -1.96969689e+00]]),\n",
       " array([1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        0, 1, 1, 0, 0, 1]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.make_classification(n_samples=50,n_features=4, n_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6053e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d89f25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
