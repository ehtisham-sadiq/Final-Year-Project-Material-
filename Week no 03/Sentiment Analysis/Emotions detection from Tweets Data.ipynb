{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8fbfa56",
   "metadata": {},
   "source": [
    "## MultiClass Classification - Emotions Detection from Text Message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e609385c",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "- Data Source\n",
    "- Data Description\n",
    "- Goal\n",
    "- Importing Packages and Loading Data\n",
    "- EDA and Feature Engineering\n",
    "- Text Preprocessing\n",
    "- Multi-Classification Models\n",
    "    - Spliting the data: train and test\n",
    "    - Models\n",
    "- Comparison of models performance\n",
    "- Model Evaluation\n",
    "    - Precision, Recall, F1-Score\n",
    "    - Confusion Matrix\n",
    "- Predictions\n",
    "- Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6ef1fa",
   "metadata": {},
   "source": [
    "\n",
    "### Source of Dataset\n",
    "\n",
    "> We will be using the [Emotions dataset for NLP](https://www.kaggle.com/praveengovi/emotions-dataset-for-nlp) by Praveen.\n",
    "\n",
    "### Format of Dataset\n",
    "\n",
    "> | text         | emotion |\n",
    "> |--------------|---------|\n",
    "> |i didnt feel humiliated | sadness |\n",
    "> |i can go from feeling so hopeless to so damned hopeful just from being around... | sadness |\n",
    "> |im grabbing a minute to post i feel greedy wrong | anger |\n",
    "> |i am ever feeling nostalgic about the fireplace i will know that it is still... | love |\n",
    "\n",
    "> **Note:** ***text*** and ***emotion*** are separated by a semi-colon ***';'***.\n",
    "<br>\n",
    "<pre>\n",
    "i didnt feel humiliated;sadness\n",
    "i am feeling grouchy;anger\n",
    "...\n",
    "</pre>\n",
    "\n",
    "### Data Description:\n",
    "The data is basically a collection of tweets annotated with the emotions behind them. We have three columns \n",
    "- `emotion`\n",
    "- `text`  \n",
    "\n",
    "In `text`, we have the raw text message. In `emotion`, we have the emotion behind the message."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e54715",
   "metadata": {},
   "source": [
    "### Goal:\n",
    "Emotion detection from text is one of the challenging problems in NLP. Humans have a variety of emotions and it is difficult to collect enough records for each emotion. Here we have a labeled data for emotion detection and the objective is to build an efficient model to detect emotion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d092825a",
   "metadata": {},
   "source": [
    "### Importing Packages and Loading the datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbeaae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import opendatasets as od\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9187cc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.kaggle.com/datasets/pashupatigupta/emotion-detection-from-text\"\n",
    "# od.download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19e18269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplace i will know that it is still on the property</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                           text  \\\n",
       "0  i didnt feel humiliated                                                                                        \n",
       "1  i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake   \n",
       "2  im grabbing a minute to post i feel greedy wrong                                                               \n",
       "3  i am ever feeling nostalgic about the fireplace i will know that it is still on the property                   \n",
       "4  i am feeling grouchy                                                                                           \n",
       "\n",
       "   emotion  \n",
       "0  sadness  \n",
       "1  sadness  \n",
       "2  anger    \n",
       "3  love     \n",
       "4  anger    "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dataset/train.txt', delimiter=\";\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eabcdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat dataset/test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d615c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6c2413",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    "# total 40000 rows and 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc680916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive view of the categorical column\n",
    "df.emotion.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cafa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the unique values of emotion along with their count\n",
    "df.emotion.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4eb79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = df.text.apply(lambda x: text_preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32245ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import contractions\n",
    "from cleantext import clean\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "wn = WordNetLemmatizer()\n",
    "\n",
    "def text_preprocessing(mystr):\n",
    "    mystr = mystr.lower()                                               # case folding\n",
    "    mystr = re.sub('\\w*\\d\\w*', '', mystr)                               # remove digits\n",
    "    mystr = re.sub('\\n', ' ', mystr)                                    # replace new line characters with space\n",
    "    mystr = re.sub('[‘’“”…]', '', mystr)                                # removing double quotes and single quotes\n",
    "    mystr = re.sub('<.*?>', '', mystr)                                  # removing html tags \n",
    "    mystr = re.sub(r'\\[.*?\\]', '', mystr)                               # remove text in square brackets\n",
    "    mystr = re.sub('https?://\\S+|www.\\.\\S+', '', mystr)                 # removing URLs\n",
    "    mystr = re.sub('\\n', ' ', mystr)                                    # replace new line characters with space\n",
    "    mystr = clean(mystr, no_emoji=True)                                 # remove emojis\n",
    "    mystr = ''.join([c for c in mystr if c not in string.punctuation])  # remove punctuations\n",
    "    mystr = ' '.join([contractions.fix(word) for word in mystr.split()])# expand contractions\n",
    "    \n",
    "    tokens = word_tokenize(mystr)                                       # tokenize the string\n",
    "    mystr = ''.join([c for c in mystr if c not in string.punctuation])  # remove punctuations\n",
    "    tokens = [token for token in tokens if token not in stop_words]     # remove stopwords\n",
    "#   tokens = [ps.stem(token) for token in tokens]                       # stemming\n",
    "    tokens = [wn.lemmatize(token) for token in tokens]                   # lemmatization\n",
    "    new_str = ' '.join(tokens)\n",
    "    return new_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743d3b15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "857d4af6",
   "metadata": {},
   "source": [
    "### EDA and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8823ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()\n",
    "# There is no missing values in any row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b911a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()\n",
    "# There is total 91 duplicated values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a578c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all the duplicate values\n",
    "df.drop_duplicates(ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb0dd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()\n",
    "# There is total 91 duplicated values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c2cf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new column which stores total number of characters in each tweet\n",
    "print(df.text[0])\n",
    "len(df.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad4ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.text[22])\n",
    "len(df.text[22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09c7f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.text[201])\n",
    "len(df.text[201])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a655deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['char_length'] = df.text.apply(lambda  x : len(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ff2143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new column which stores total number of tokens/words in each tweet\n",
    "len(df.text[0].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e786951",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.text[22].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01f05a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.text[45].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c4b38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.text[2985].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cc80c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['token_length'] = df.text.apply(lambda x: len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9fee08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33c795d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0fe768b8",
   "metadata": {},
   "source": [
    "#### sentiment values distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddf99d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(8,8))\n",
    "sns.countplot(y= df.emotion)\n",
    "# plt.xticks(rotation=90)\n",
    "plt.yticks(size=12)\n",
    "plt.xticks(size=13)\n",
    "plt.title(\"Emotion-Type vs Values\", fontdict={'fontsize':20})\n",
    "plt.xlabel(\"Values\", fontdict={'fontsize':15})\n",
    "plt.ylabel(\"Emotion-Type\", fontdict={'fontsize':15})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f4b8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e5bc9db",
   "metadata": {},
   "source": [
    "#### Percentage of each Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728f0a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.pie(df.emotion.value_counts(), labels=df.emotion.value_counts().index, autopct='%.2f')\n",
    "plt.title(\"Percentage of each emotion\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382147f5",
   "metadata": {},
   "source": [
    "#### Distribution of Character Length in each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14418183",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.distplot(df.char_length)\n",
    "plt.title(\"Number of Characters in the tweet\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9363755a",
   "metadata": {},
   "source": [
    "#### Distribution of tokens/words in the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed813d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "sns.distplot(df.token_length)\n",
    "plt.title(\"Number of tokens(words) in the tweet\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d510def9",
   "metadata": {},
   "source": [
    "#### Distribution of top 5 emotions character-length wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd1fac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Method\n",
    "df1 = df.groupby('emotion')['char_length'].count().sort_values(ascending=False).head(5).reset_index()\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf97225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second method\n",
    "for sentiment in df.emotion.value_counts().sort_values()[-5:].index.tolist():\n",
    "    print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1b93dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sentiment in df.sentiment.value_counts().sort_values()[-5:].index.tolist():\n",
    "#     print(df[df['sentiment']==sentiment]['char_length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55c8d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "for emotion in df['emotion'].value_counts().sort_values()[-5:].index.tolist():\n",
    "    sns.kdeplot(df[df['emotion']==emotion]['char_length'],ax=ax, label=emotion)\n",
    "ax.legend()\n",
    "ax.set_title(\"Distribution of top 5 emotions character-length wise\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ac2be",
   "metadata": {},
   "source": [
    "#### Distribution of top 5 emotions token-length wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0700f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Method\n",
    "df2 = df.groupby('sentiment')['token_length'].count().sort_values(ascending=False).head(5).reset_index()\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c77ce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "for sentiment in df['sentiment'].value_counts().sort_values()[-5:].index.tolist():\n",
    "    #print(sentiment)\n",
    "    sns.kdeplot(df[df['sentiment']==sentiment]['token_length'],ax=ax, label=sentiment)\n",
    "ax.legend()\n",
    "ax.set_title(\"Distribution of top 5 emotions token-length wise\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cf3377",
   "metadata": {},
   "source": [
    "#### Average Length of each Tweet characters and Tokens wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf93a617",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_df = df.groupby('sentiment').agg({'char_length':'mean', 'token_length':'mean'}).reset_index()\n",
    "avg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31337d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.yticks(size=12)\n",
    "plt.xticks(size=13)\n",
    "sns.barplot(y=avg_df.sentiment, x=avg_df.char_length)\n",
    "plt.title(\"Average Length of each Tweet characters wise\", fontdict={'fontsize':20})\n",
    "plt.xlabel(\"Char Length\", fontdict={'fontsize':15})\n",
    "plt.ylabel(\"Emotion-Type\", fontdict={'fontsize':15})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba7a4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.yticks(size=12)\n",
    "plt.xticks(size=13)\n",
    "sns.barplot(y=avg_df.sentiment, x=avg_df.token_length)\n",
    "plt.title(\"Average Length of each Tweet token wise\", fontdict={'fontsize':20})\n",
    "plt.xlabel(\"Token Length\", fontdict={'fontsize':15})\n",
    "plt.ylabel(\"Emotion-Type\", fontdict={'fontsize':15})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f43486",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0320944",
   "metadata": {},
   "source": [
    "#### Case Folding and Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69065c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "#     removing the @mentions\n",
    "    text = re.sub(r\"@\\w+|#\\w+\", \"\", text)\n",
    "    \n",
    "#     removing # hashtages from text\n",
    "    text = re.sub(r\"#\",\"\", text)\n",
    "    \n",
    "#     removing RT from text\n",
    "    text = re.sub(r\"RT[\\s]+\",\"\", text)\n",
    "    \n",
    "#     removing hyperlinks from text\n",
    "    text = re.sub(r\"\\w+:\\/\\/\\S+\",\"\", text)\n",
    "    \n",
    "#     removing punctuation from the text\n",
    "    text = re.sub(r\"[^a-zA-Z]\",\" \", text)\n",
    "    \n",
    "#     convert text into lowercase\n",
    "    text.lower()\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da417d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.content.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a427bcae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29acc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# case folding\n",
    "temp = df['content'].str.lower()\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91dbdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove hashtag and mention using regex\n",
    "import re\n",
    "temp = temp.apply(lambda x: re.sub(r'@\\w+|#\\w+', '', x))\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e094eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove url using regex\n",
    "temp = temp.apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d6386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "import string\n",
    "temp = temp.apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd97db58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove number\n",
    "temp = temp.apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b6107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove whitespace\n",
    "temp = temp.apply(lambda x: x.strip())\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8385893e",
   "metadata": {},
   "source": [
    "#### Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40d96e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "temp = temp.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12efbbce",
   "metadata": {},
   "source": [
    "#### lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e108e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "temp = temp.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d44fa5",
   "metadata": {},
   "source": [
    "#### tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32a0a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp.apply(lambda x: word_tokenize(x))\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37e94bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp to df content_token\n",
    "df['content_token'] = temp\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d2a40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52319a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove NaN data in content_token\n",
    "df = df.dropna(subset=['content_token'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dff3dc",
   "metadata": {},
   "source": [
    "#### Finding & Removing Duplicate Synonim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e56d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find synonym of each token.\n",
    "from nltk.corpus import wordnet\n",
    "def find_synonym(word):\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            synonyms.append(l.name())\n",
    "    return synonyms\n",
    "\n",
    "df['synonym'] = df['content_token'].apply(lambda x: [find_synonym(word) for word in x])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5ab12b",
   "metadata": {},
   "source": [
    "#### Dictionary of word index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b146dc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_word = {}\n",
    "for i, word in enumerate(df['content_token'].sum()):\n",
    "    if word not in index_word:\n",
    "        index_word[i] = word\n",
    "words = [value for key, value in index_word.items()]\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ee7266",
   "metadata": {},
   "source": [
    "#### set synonym dictionary using find_synonym function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47ddd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym_dict = {}\n",
    "for word in words:\n",
    "    synonym_dict.update({word : tuple([w.lower() for w in find_synonym(word)])})\n",
    "\n",
    "synonym_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f9f040",
   "metadata": {},
   "source": [
    "#### remove duplicate synonym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef404265",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in synonym_dict.items():\n",
    "    synonym_dict[key] = tuple(set(value))\n",
    "\n",
    "synonym_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272ab71d",
   "metadata": {},
   "source": [
    "#### remove null value in synonym_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e5ca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym_dict = {k: v for k, v in synonym_dict.items() if v}\n",
    "\n",
    "synonym_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcc91b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "value_occurrences = collections.Counter(synonym_dict.values())\n",
    "\n",
    "filtered_synonym = {key: value for key, value in synonym_dict.items() if value_occurrences[value] == 1}\n",
    "\n",
    "filtered_synonym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2ad4c3",
   "metadata": {},
   "source": [
    "#### Data Augmention by replacing words with synonyms using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd5481f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for augmenting data by replacing words with synonym using spaCy\n",
    "import re\n",
    "import random\n",
    "sr = random.SystemRandom()\n",
    "split_pattern = re.compile(r'\\s+')\n",
    "def data_augmentation(message, aug_range=1) :\n",
    "    augmented_messages = []\n",
    "    for j in range(0,aug_range) :\n",
    "        new_message = \"\"\n",
    "        for i in filter(None, split_pattern.split(message)) :\n",
    "            new_message = new_message + \" \" + sr.choice(filtered_synonym.get(i,[i]))\n",
    "        augmented_messages.append(new_message)\n",
    "    return augmented_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc706df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_count = df.sentiment.value_counts().to_dict()\n",
    "tweet_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5ad55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f85b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get max intent count to match other minority classes through data augmentation\n",
    "import operator\n",
    "max_intent_count = max(tweet_count.items(), key=operator.itemgetter(1))[1]\n",
    "max_intent_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a50bfbd",
   "metadata": {},
   "source": [
    "#### Balance Data\n",
    "Because sentiment data is very far apart, such as neutral containing 8638 data, while anger as much as 110 data. We decided to balance the data to make the data fairer in terms of accuracy learning later. We use the Oversampling method, which means adding synthetic data that refers to the largest amount of data in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90ee171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tqdm\n",
    "# tqdm is a library in Python which is used for creating Progress Meters or Progress Bars.\n",
    "newdf = pd.DataFrame()\n",
    "for intent, count in tweet_count.items() :\n",
    "    count_diff = max_intent_count - count    ## Difference to fill\n",
    "    multiplication_count = math.ceil((count_diff)/count)  ## Multiplying a minority classes for multiplication_count times\n",
    "    if (multiplication_count) :\n",
    "        old_message_df = pd.DataFrame()\n",
    "        new_message_df = pd.DataFrame()\n",
    "        for message in tqdm.tqdm(df[df[\"sentiment\"] == intent]['content'].values) :\n",
    "            ## Extracting existing minority class batch\n",
    "            dummy1 = pd.DataFrame([message], columns=['content'])\n",
    "            dummy1[\"sentiment\"] = intent\n",
    "            # concat existing minority class batch\n",
    "            old_message_df = pd.concat([old_message_df, dummy1])\n",
    "\n",
    "            ## Creating new augmented batch from existing minority class\n",
    "            new_messages = data_augmentation(message,  multiplication_count)\n",
    "            dummy2 = pd.DataFrame(new_messages, columns=['content'])\n",
    "            dummy2[\"sentiment\"] = intent\n",
    "            # concat new augmented batch\n",
    "            new_message_df = pd.concat([new_message_df, dummy2])\n",
    "\n",
    "        ## Select random data points from augmented data\n",
    "        new_message_df=new_message_df.take(np.random.permutation(len(new_message_df))[:count_diff])\n",
    "        \n",
    "        ## Merge existing and augmented data points using concat\n",
    "        newdf = pd.concat([newdf, old_message_df, new_message_df])\n",
    "        # newdf = newdf.append([old_message_df,new_message_df])\n",
    "    else :\n",
    "        newdf = pd.concat([newdf, df[df[\"sentiment\"] == intent]])\n",
    "        # newdf = newdf.append(df[df[\"Intent\"] == intent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d8a05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa87a202",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff84c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save newdf to csv file\n",
    "newdf.to_csv('dataset/augmented_data.csv', index=False)\n",
    "clean_df = pd.read_csv('dataset/augmented_data.csv')\n",
    "clean_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c5f011",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['sentiment'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8c796b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning the tweets using clean_tweet function\n",
    "clean_df['clean_tweet'] = clean_df['content'].apply(lambda x: clean_text(x))\n",
    "\n",
    "# lower casing clean_tweet column\n",
    "clean_df['clean_tweet'] = clean_df['clean_tweet'].apply(lambda x: x.lower())\n",
    "\n",
    "# function to remove stop words from clean_tweet column\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text.split() if word not in stop]\n",
    "    return \" \".join(text)\n",
    "\n",
    "# stopword removal\n",
    "clean_df['clean_tweet'] = clean_df['clean_tweet'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "clean_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a0bee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to lemmitize clean_tweet column\n",
    "def lemmatization(text):\n",
    "    text = [lemmatizer.lemmatize(word) for word in text.split()]\n",
    "    return \" \".join(text)\n",
    "clean_df['clean_tweet'] = clean_df['clean_tweet'].apply(lambda x: lemmatization(x))\n",
    "\n",
    "# tokenization using word_tokenize\n",
    "clean_df['clean_tweet_token'] = clean_df['clean_tweet'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "clean_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4448c31",
   "metadata": {},
   "source": [
    "### Multi-Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b685ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(clean_df['clean_tweet_token'], clean_df['sentiment'], test_size=0.2, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e504c00",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ffe442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train.astype('U'))\n",
    "X_test = vectorizer.transform(X_test.astype('U'))\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b5726e",
   "metadata": {},
   "source": [
    "### Model Making"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619de339",
   "metadata": {},
   "source": [
    "#### Model Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40a6dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "# Fit model\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = mnb.predict(X_train)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "y_pred_test = mnb.predict(X_test)\n",
    "\n",
    "acc_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f'The Results of the calculation of the accuracy of the Train Data : {acc_train}')\n",
    "print(f'The Results of the calculation of the accuracy of the Test Data : {acc_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7303ab",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4b09e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(y_test, y_pred_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05ebea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "print(confusion_matrix(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cfb13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13,13))\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=mnb.classes_)\n",
    "# ax.tick_params()\n",
    "disp.plot(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9bb181",
   "metadata": {},
   "source": [
    "#### Model Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f92697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model using Linear SVC\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Inisiasi LinearSVC\n",
    "\n",
    "lsvc = LinearSVC()\n",
    "\n",
    "# Fit model\n",
    "lsvc.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = lsvc.predict(X_train)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "y_pred_test = lsvc.predict(X_test)\n",
    "\n",
    "acc_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "\n",
    "print(f'The Results of the calculation of the accuracy of the Train Data : {acc_train}')\n",
    "print(f'The Results of the calculation of the accuracy of the Test Data : {acc_test}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11cab1b",
   "metadata": {},
   "source": [
    "#### Model Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a38afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e5295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "print(confusion_matrix(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c597fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13,13))\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=lsvc.classes_)\n",
    "# ax.tick_params()\n",
    "disp.plot(ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039dd212",
   "metadata": {},
   "source": [
    "#### Model using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f739f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model using Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = lr.predict(X_train)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "y_pred_test = lr.predict(X_test)\n",
    "\n",
    "acc_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f'The Results of the calculation of the accuracy of the Train Data : {acc_train}')\n",
    "print(f'The Results of the calculation of the accuracy of the Test Data : {acc_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c83aafb",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9639a195",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4508ebe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "print(confusion_matrix(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883d9de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13,13))\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=lr.classes_)\n",
    "# ax.tick_params()\n",
    "disp.plot(ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178e6777",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tweet = \"It is very important for us to work hard to achieve goals in life\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5d23b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63147354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
